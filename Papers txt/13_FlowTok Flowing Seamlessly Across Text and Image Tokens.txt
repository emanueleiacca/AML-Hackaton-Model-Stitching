FlowTok: Flowing Seamlessly Across Text and Image Tokens

arXiv:2503.10772v2 [cs.CV] 19 Mar 2025

Ju He1 Qihang Yu1 Qihao Liu2 Liang-Chieh Chen1
1
2
ByteDance Seed
Johns Hopkins University
https://tacju.github.io/projects/flowtok.html

Figure 1. Text-to-Image Generation Results by FlowTok. FlowTok projects both text and images into a unified, compact 1D latent space,
enabling direct flow matching between 1D tokens and facilitating the efficient generation of diverse, high-fidelity images.

Abstract

2D latent embeddings. To address this, we introduce FlowTok, a minimal framework that seamlessly flows across text
and images by encoding images into a compact 1D token representation. Compared to prior methods, this design reduces
the latent space size by 3.3× at an image resolution of 256,
eliminating the need for complex conditioning mechanisms
or noise scheduling. Moreover, FlowTok naturally extends to
image-to-text generation under the same formulation. With
its streamlined architecture centered around compact 1D
tokens, FlowTok is highly memory-efficient, requires significantly fewer training resources, and achieves much faster
sampling speeds—all while delivering performance comparable to state-of-the-art models. Code will be available at
https://github.com/bytedance/1d-tokenizer.

Bridging different modalities lies at the heart of crossmodality generation. While conventional approaches treat
the text modality as a conditioning signal that gradually guides the denoising process from Gaussian noise
to the target image modality, we explore a much simpler
paradigm—directly evolving between text and image modalities through flow matching. This requires projecting both
modalities into a shared latent space, which poses a significant challenge due to their inherently different representations: text is highly semantic and encoded as 1D tokens,
whereas images are spatially redundant and represented as
1

Text

Conditioning Signal

“a cutesy
plushy cat”

CLIP Text Embedding

legendText Token

Image Token

NxC

Flow Matching

Noise

Image
HxWxD

HxWxD

Text as Conditions
Direct Flow between Modalities
Text-to-Image Generation
Text

“a cutesy
plushy cat”

Flow Matching
NxD

Image
NxD

Image-to-Text Generation

Figure 2. Text as Conditions vs. Direct Flow between Modalities. Top: Conventional text-to-image generation relies on the diffusion
process, where text serves as a conditioning signal to guide the denoising process. Bottom: The proposed FlowTok enables direct flow
between text and image modalities by projecting both into a shared, compact 1D latent space, facilitating seamless generation of both.

1. Introduction

source distribution; instead, it only requires the source and
target distributions to share the same shape. Pioneering
works [7, 51, 72, 80, 94] have demonstrated its effectiveness
in learning direct mappings within the same modality (e.g.,
image-to-image generation). Meanwhile, CrossFlow [52] extends flow matching to cross-modal learning by mapping text
into a 2D latent space to match the shape of image embeddings, paving the way for new possibilities. However, while
this approach simplifies the overall pipeline, it still operates
on 2D latent representations. As a result, the additional
computational overhead introduced by the text variational
autoencoder [41] in CrossFlow makes it slower than modern
text-to-image diffusion models like SD1.5 and SD2.1 [69],
ultimately contradicting its original goal of efficiency.

Bridging different modalities is essential for comprehending
the diverse forms of data that represent our world, encompassing both understanding and generation. In multimodal
understanding, extensive research has focused on designing
architectures that project different modalities into a shared
latent space [6, 20, 24, 37, 38, 46, 47, 55, 65, 83]. These approaches have significantly advanced cross-modal representation learning and real-world understanding by leveraging a
common latent space between modalities.
In contrast, multimodal generation (e.g., text-to-image
generation) follows a different paradigm, primarily relying on the diffusion process [36, 53, 61, 73, 74], where the
source modality (e.g., text) serves as a conditioning signal
to guide the denoising process. Various conditioning mechanisms have been explored, including concatenation [11],
cross-attention [16, 69], conditioning embeddings [61], and
hybrid strategies [26, 40]. While effective, these approaches
introduce substantial complexity, requiring intricate conditioning mechanisms and noise scheduling. This naturally
raises an important question: Can we unify multimodal understanding and generation by enabling direct transitions
within a shared latent space?

To this end, we introduce FlowTok, a minimal framework
that enables seamless Flowing of Tokens across text and
image—the two most prevalent modalities (Fig. 2). At the
core of FlowTok, both text and images are encoded into compact 1D latent tokens within a unified space, enabling direct
flow matching between them. On the text side, FlowTok
employs a pre-trained text encoder [65] to extract initial 1D
text embeddings. Since these embeddings typically reside
in a higher-dimensional space than image latents, FlowTok
introduces a lightweight text projector to map text embeddings into a low-dimensional variational latent space. On
the image side, FlowTok builds on recent advancements in
image tokenization [40, 91] to encode images into compact
1D latent tokens. Specifically, we enhance TA-TiTok [40] by
integrating RoPE [76] and SwiGLU FFN [71], improving
positional information handling and reconstruction quality.

To address this question, we revisit flow matching [7,
50, 54]—a modern generative framework that learns a direct path from noise to data, enabling faster convergence
and accelerated sampling, leading to state-of-the-art multimodal generation results [26, 42]. Unlike diffusion models, flow matching is not constrained to using noise as the
2

To enable direct flow matching, we align the number of image latent tokens K in TA-TiTok to match the text encoder’s
output sequence length (K = 77 for CLIP text encoder).
By integrating these simple yet effective designs across
text and image modalities, FlowTok represents both in the
same 1D low-dimensional space with shape 77 × 16 (77 tokens, each with 16 dimensions). This compact representation
is 3.3× smaller than typical 2D flow matching shapes [75]
of 32 × 32 × 4 for image resolutions of 256. This alignment
enables fast, direct flow matching and seamless evolution
between the two modalities.
Unlike standard flow matching models [50, 54, 57], FlowTok eliminates the need for intricate conditioning mechanisms, offering a fully self-attention-based generative model.
This allows for direct flow across modalities without additional complexity. Unlike CrossFlow [52], which converts
text into 2D embeddings, FlowTok retains the 1D structure
of text embeddings, avoiding the need for flattening and
transformation into 2D. This simplifies the framework while
eliminating reliance on heavy parametric contrastive losses
for semantic preservation.
As a result, FlowTok offers a streamlined and resourceefficient training process. Its largest variant, FlowTok-H
(1.1B), supports a batch size of 8K on 8 A100 GPUs without
requiring gradient checkpointing or gradient accumulation.
In contrast, recent text-to-image models of similar scale
typically require 32 to 64 A100 GPUs to train with a batch
size of only 2K [16, 69]. Moreover, FlowTok converges
significantly faster, as shown in Fig. 3a, with FlowTok-H
completing training in just 26.1 8-A100 days—far less than
SD 2.1 [69], which requires 1041.6 8-A100 GPU days.
Inference is also highly efficient, with FlowTok achieving over 10× the throughput of Show-o [85] and CrossFlow [52], as shown in Fig. 3b. This dramatically reduces
computational costs, making text-to-image research far more
accessible. To ensure full reproducibility, we train FlowTok
exclusively on publicly available datasets, avoiding reliance
on high-quality proprietary data. Remarkably, despite its
minimalist design and reduced data requirements, FlowTok
achieves state-of-the-art text-to-image performance.
Beyond text-to-image generation, FlowTok seamlessly
extends to image-to-text generation, maintaining strong performance under the same minimalist framework. We believe
our work establishes a strong foundation for future research
in generalized cross-modality generation. To support further
advancements, all code will be released.

15.0

15.0

12.0
FlowTok-XL
10.0
9.0

FlowTok-H CrossFlow

SD-1.5

8.0

COCO FID-30K

COCO FID-30K

SD-2.1
12.0
FlowTok-XL
10.0 CrossFlow
FlowTok-H

9.0 Show-o
8.0

20

50

100

1000

1

5

10

20

30

Training Costs (8-A100 days)

Inference Speed (imgs / second)

(a) FID vs. Training Costs.

(b) FID vs. Inference Speed.

Figure 3. COCO Results. FlowTok presents comparable performance to previous methods on COCO while significantly reducing
training resource requirements (Fig. 3a) and achieving much faster
sampling speed (Fig. 3b). This efficiency stems from its minimalist
design centered around 1D tokens, which facilitates direct transformation between text and image modalities, leading to superior
performance with enhanced computational efficiency. We note that
the compared CrossFlow [52] uses high-quality proprietary data.

els [18, 26, 63, 67], offering faster training and sampling
compared to conventional diffusion methods [36, 53, 61].
Several works further optimize flow trajectories by minimizing curvature [44, 64, 79]. Despite its theoretical flexibility
in handling arbitrary distributions, recent approaches primarily evolve noise into target distributions, often relying on
complex control signal conditioning, which complicates the
pipeline and overlooks the potential of directly transforming
control signals into target distributions. In contrast, only a
few works [7, 51, 54, 72, 80, 94] explore direct transport
within the same modality (e.g., image-to-image [28, 54, 94]),
leaving cross-modal transport (e.g., text-to-image) underexplored. In this work, we introduce FlowTok, a minimal yet effective framework that enables seamless flow across text and
image modalities using 1D tokens. Unlike CrossFlow [52],
which follows a similar paradigm but relies on 2D latent
representations and incurs additional computational costs
due to the text variational encoder, FlowTok operates within
a unified, compact 1D token space. This design achieves
a 3.3× compression rate in latent size, significantly reducing training costs and accelerating the sampling process, all
while maintaining state-of-the-art performance.
Text-to-Image Generation. Text-to-image generation has
advanced rapidly in recent years, driven by various generative paradigms, including diffusion models [16, 62, 69, 70],
flow matching models [18, 26, 86, 92], sequence models [27, 30, 68, 89, 90], and masked generative models [9, 13, 40, 84]. While early works in each category
establish the foundation for their respective approaches, subsequent advancements across different model types have
primarily emerged from three key areas: careful data collection and advanced image recaptioning for improved data
quality [12, 22, 40], architectural and conditioning improve-

2. Related Work
Flow Matching. Flow matching [7, 50, 54] models generative processes by constructing a transport map between two
distributions via an ordinary differential equation (ODE).
It has recently gained traction as the foundation for stateof-the-art text-to-image and text-to-video synthesis mod3

to the target distribution. Taking the derivative of Xt with
respect to t, we have:

ments for faster convergence and better text-image alignment [16, 18, 26, 27], and micro-conditioning for finer control over generated samples [9, 40, 62]. By contrast, this
work introduces a minimalist framework FlowTok that directly maps text tokens to image tokens, eliminating the
need for noise scheduling and complex conditioning mechanisms. This streamlined design enhances both efficiency and
simplicity while maintaining competitive performance.

Vt =

dXt
= N − X,
dt

where Vt indicates the direction from the source to the target
distribution such that the induced flow accurately transports
the source distribution to the target distribution.
Notably, while the source distribution is typically modeled as Gaussian noise in generative frameworks [26], the
flow matching formulation generalizes to arbitrary source
distributions, provided that the source and target distributions share the same shape. In FlowTok, we directly define a
unified latent space for image and text modalities, treating
them as both source and target distributions. This design
enables seamless generation across different modalities.

3. Preliminary
1D Visual Tokenization [40, 91] deviates from traditional
2D grid-based latent tokenization by adopting a compact
1D representation, eliminating the need to preserve the 2D
spatial structure. This work focuses on continuous 1D visual tokens for flow matching. During tokenization, given an
input image I ∈ RH×W ×3 , the image is downscaled by a facH
W
tor of f , resulting in patches P ∈ R f × f ×D . These patches
are concatenated with a set of latent tokens L ∈ RK×D to
form a sequence that is passed through a Vision Transformer
(ViT) [23] encoder, Enc, to generate embeddings. Only
the embeddings corresponding to the latent tokens are retained, forming a compact 1D latent representation. This
representation is modeled as a Gaussian distribution with
KL divergence regularization, resulting in a compact 1D
VAE representation, ZI ∈ RK×D . In the de-tokenization
phase, text guidance is applied by incorporating text embeddings generated by a pre-trained text encoder [65]. These
text embeddings are projected through a linear layer to align
with the channel dimensions of ViT decoder, resulting in
T ∈ RN ×D , where N is the number of context tokens predefined by the text encoder. The text embedding T is then
concatenated with the latent tokens ZI and a set of mask
H
W
tokens M ∈ R f × f ×D . The combined sequence is passed
through the decoder Dec, yielding the reconstructed image Î.
Formally, with ⊕ denoting concatenation, the tokenization
and de-tokenization can be represented as:

4. Method
In this section, we focus on text-to-image generation as the
primary task to illustrate FlowTok. We first detail how images and text are projected into a unified, compact latent
space as 1D tokens while preserving semantic information
(Sec. 4.1). Next, we introduce FlowTok as a general framework for seamless flow between text and image tokens and
discuss its extension to image-to-text generation under the
same formulation (Sec. 4.2).

4.1. Unifying Latent Space of Image and Text
The structural discrepancy between text and images presents
a significant challenge in unifying them within the same
latent space for flow matching. Text is inherently semantic,
encoded as a 1D latent sequence with high-dimensional channels to preserve meaning, whereas images contain spatially
redundant information and are typically represented as 2D
feature maps with lower channel dimensions to retain spatial
priors. To bridge this gap, we propose encoding images into
compact 1D tokens by leveraging recent advancements in
image tokenization. This formulation helps preserve the 1D
structure of text embeddings, requiring only their projection
into a more compressed set of tokens while ensuring that
semantic information is retained. Below, we detail how both
images and text are encoded.
Encoding Images into Compact Tokens. We build upon
the core idea of TA-TiTok [40] with several enhancements
to improve our image tokenizer. Specifically, we replace the
original learnable 1D positional embedding with RoPE [76]
to enhance TA-TiTok performance. Additionally, we substitute the MLP blocks in the Vision Transformer (ViT) [23]
with SwiGLU FFN [71], which helps learn a more effective
latent space [17, 88]. To align with the number of context
tokens N of the text encoder, we set the number of latent
tokens K in TA-TiTok accordingly (K = N = 77 for

ZI = Enc(P ⊕ L),
Î = Dec(ZI ⊕ T ⊕ M).
Flow matching [50, 54] is a framework that learns a continuous transformation between a source distribution and a
target distribution. The source distribution is not necessarily
required to be Gaussian noise, though we use Gaussian noise
as a concrete example below.
During training, given a sample X from the target distribution, a sampled time step t ∈ [0, 1], and a noise sample
N ∼ N (0, I) from the source distribution. An intermediate
representation Xt is obtained by:
Xt = (1 − t) · X + t · N.
The flow matching model is trained to estimate the velocity field Vt , which describes the direction from the source
4

Tinit

Image VAE
Decoder

NxD

NxC

ZI

DiT Block

Text Projector

DiT Block

CLIP Text
Encoder

DiT Block

“dog meme king”

xL

ZT

NxD

Text-to-Image Generation

Image-to-Text Generation

CLIP Text Embedding
ZI

xL
DiT Block

DiT Block

DiT Block

Image VAE
Encoder

Image Token

legendText Token

ZT

Text Decoder

CLIP Text
Tokenizer

“dog meme king”

NxD

NxD

Figure 4. Overview of FlowTok. FlowTok is a minimal framework that facilitates seamless flow between 1D text tokens and image
tokens for both text-to-image and image-to-text generation. Top: For text-to-image generation, the input text is encoded by the CLIP text
encoder into Tinit ∈ RN ×C , projected into a low-dimensional latent space as text tokens ZT ∈ RN ×D , then transformed into image tokens
ZI ∈ RN ×D of the same shape through flow matching and decoded by a 1D Image VAE Decoder to generate the final image. Bottom: For
image-to-text generation, an input image is encoded by a 1D Image VAE Encoder into ZI , mapped to ZT through flow matching and decoded
into text via a text decoder. Unlike conventional approaches that rely on 2D noise and image latents (e.g., 32 × 32 × 4 for 256-resolution
images) with text as conditions, our direct 1D transformation (i.e., 77 × 16) achieves a 3.3× compression rate, significantly reducing
memory costs, accelerating training, and enabling faster inference.

CLIP [65]). As a result, the encoder of TA-TiTok encodes
each image into a compact 1D token sequence ZI ∈ RK×D .

and compute a contrastive loss between them, inspired by
CLIP [65]. Concretely, we calculate the scaled pairwise
cosine similarities using a learnable temperature parameter
τ , followed by a symmetric cross-entropy loss:

Transforming Texts into Compact Tokens. We use a pretrained text encoder [65] to extract the initial text embedding
Tinit ∈ RN ×C , where C denotes the number of channels.
Notably, C is typically much larger than the image latent
size D, as it carries richer semantic information. Since our
goal is to directly flow the text embedding into the image
latent space, we need to ensure both embeddings have the
same shape. While we already align K with N through
careful tuning of the image tokenizer, ensuring the image is
encoded to match the length of the text tokens. The remaining challenge lies in aligning the number of channels (i.e., C
and D), which we resolve using a text projector. Since only
the number of channels in Tinit needs adjustment while preserving its 1D shape, we employ a few simple Transformer
blocks as the projector. To introduce variability in image
generation from the same text, we model the projected text
latents ZT ∈ RN ×D as a Gaussian distribution by applying
KL divergence regularization Lkld .

logitsTZ = exp(τ ) × (TP × ZT
T ),
logitsZT = exp(τ ) × (ZT × TT
P ),
Lalign = (CE(logitsTZ , labels) + CE(logitsZT , labels))/2,
where T denotes the transpose operation, CE represents the
cross-entropy loss, and labels are assigned based on their
batch indices, ensuring that each text token is explicitly
trained to align with its corresponding CLIP text embedding within the same batch. We also explore alternative approaches to preserving semantic information, such as aligning with the average-pooled text embedding or using a cosine similarity loss with a margin. However, we find that the
CLIP-style loss achieves the best performance. More details
are provided in Sec. 5.3.
Through the aforementioned designs, FlowTok efficiently
tokenizes text into the same low-dimensional latent space
while preserving semantic information. This alignment with
the tokenized image latent space establishes a foundation for
direct flow between compressed text tokens ZT and image
tokens ZI . Notably, when using CLIP as the text encoder,
FlowTok effectively reduces the latent size compared to traditional 2D flow matching methods. At an image resolution
of 256, the latent size is reduced from 32 × 32 × 4 to 77 × 16,
achieving a 3.3× compression. This reduction significantly
lowers memory requirements and accelerates training, en-

A crucial aspect of text-to-image generation is ensuring
that the generated image accurately reflects the input text
description. Since reducing the channel dimensions of text
embeddings via a learnable projector may result in semantic
information loss, we introduce an auxiliary text alignment
loss Lalign to preserve semantic consistency. Specifically,
we employ a lightweight MLP to project Tinit into a new
space TP ∈ RN ×D for alignment. We then flatten and
normalize both TP and ZT along the channel dimension
5

model
depth width mlp heads #params
FlowTok-B
12
768 3072 12
153M
FlowTok-XL 28 1152 4608 16
698M
FlowTok-H
36 1280 5120 20
1.1B

on the ImageNet validation set, matching the performance
of the original TA-TiTok with 128 tokens.
Text Projector. We train a text projector for text-to-image
generation, which transforms CLIP text embeddings into
a latent representation ZT of shape 77 × 16, aligning with
the image latent space ZI encoded by our image tokenizer.
The text projector consists of six Transformer [81] blocks,
each comprising a multi-head self-attention mechanism and
a multi-layer perceptron (MLP), both enhanced with skip
connections [34] to ensure stable training.
Text Decoder. We train a text decoder for image-to-text
generation, composed of six Transformer [81] blocks, similar to the text projector. The decoder takes the text latent
representation ZT as input and outputs the corresponding
CLIP text tokenizer indices, which can be further converted
into text using the CLIP text tokenizer.
FlowTok. We adopt DiT [61] blocks as the fundamental
building units of our FlowTok to model token interactions.
Specifically, we follow the DiT architecture to implement
FlowTok-B for efficient ablation studies and FlowTok-XL
for enhanced performance. To further push the performance,
we scale up the depth, width, and number of attention heads,
constructing FlowTok-H with 1.1B parameters. The detailed
model configurations are provided in Tab. 1.
Dataset. We employ open-source datasets [40] to facilitate
the reproducibility of FlowTok’s simple framework. Specifically, our image tokenizer is trained on DataComp-1B [29],
and our text tokenizer is trained on COCO [49]. For text-toimage generation, inspired by recent works [15, 16, 21, 93],
we adopt a two-stage training strategy: pre-training and finetuning. The pre-training stage leverages a combination of
DataComp-1B [29], CC12M [14], and LAION-aesthetic [1],
while the fine-tuning stage incorporates additional highquality datasets, including LAION-art [3], LAION-pop [4],
JourneyDB [77], and DALLE3-1M [25]. For image-to-text
generation, we follow the Karpathy split [39] of COCO [19]
to divide the training and validation sets. Detailed dataset
information is provided in the Appendix.
Training. The training objectives of FlowTok primarily focus on predicting velocity in flow matching, denoted as Lfm .
For text-to-image generation, we introduce two additional
losses: KL-divergence loss (Lkld ) to enforce a Gaussian
distribution on text tokens and text alignment loss (Lalign )
to preserve semantic information as discussed in Sec. 4.1.
Formally, the overall training objective is:

Table 1. Architecture Configuration of FlowTok. Following prior
work, we scale up DiT blocks across three configurations.

hancing the framework’s efficiency and scalability.

4.2. FlowTok: A General Framework for Seamless
Flow Across Text and Image Tokens
Text-to-Image Generation. As shown in Fig. 4 (top), with
both image and text mapped into the same latent space, FlowTok leverages vanilla flow matching [50] by stacking DiT
blocks [61]. Notably, source modality (i.e., text) is directly
treated as the source distribution for flow matching, removing the need for concatenation or cross-attention within the
DiT blocks. This design choice further simplifies the overall
framework and streamlines text-to-image generation. Combined with the compact 1D tokens introduced in Sec. 4.1,
FlowTok achieves high memory efficiency, supporting a
batch size of 8K on 8 A100 GPUs. Additionally, it enables
fast sampling, running over 10× faster than modern textto-image diffusion models [69], significantly lowering the
computational barrier for training large-scale text-to-image
generative models.
Image-to-Text Generation. FlowTok can also be seamlessly extended to image-to-text generation using compact
1D image and text tokens under the same formulation, as
shown in Fig. 4 (bottom). Specifically, the image tokens ZI
flow to text tokens ZT , where a trained text decoder takes
ZT as input and outputs tokenizer indices, which can then be
decoded back into the corresponding caption.

5. Experimental Results
In this section, we first provide the implementation details of
FlowTok (Sec. 5.1), followed by the main results on text-toimage and image-to-text generation (Sec. 5.2). Finally, we
present the ablation studies to better understand the design
choices of FlowTok in text-to-image generation (Sec. 5.3).

5.1. Implementation Details
Image Tokenizer. We build our image tokenizer upon the
official TA-TiTok [40] codebase with minimal modifications.
The encoder uses ViT-B [23], while the decoder uses ViT-L,
both operating with a patch size of f = 16. To align with
the output sequence length of CLIP’s text encoder, we set
the number of 1D latent tokens K to 77 and the token dimension to 16. Additionally, we enhance the tokenizer with
RoPE [76] and SwiGLU FFN [71]. Notably, our enhanced
tokenizer achieves a FID of 1.02 in a zero-shot evaluation

L = Lfm + γ1 ∗ Lkld + γ2 ∗ Lalign ,
where γ1 and γ2 control the weighting of losses. By default,
we set γ1 to 1×10−4 and γ2 to 1 for text-to-image generation,
while both are set to 0 for image-to-text generation.
Evaluation. We follow standard evaluation practices to
report relevant metrics for both text-to-image and imageto-text generation. Specifically, for text-to-image genera6

method

params open-data T↓
I↑ COCO FID-30K↓ MJHQ-30K FID↓
text as conditions
GLIDE [58]
5.0B
✗
12.24
✗
10.39
Dalle·2 [66]
6.5B
LlamaGen [78]
775M
✗
25.59
PixArt-α [16]
630M
✗
94.1 7.9
7.32
9.85
✗
8.76
SDXL [62]
2.6B
LDM [69]
1.4B
✓
12.63
Stable-Diffusion-1.5 [69] 860M
✓
781.2 9.62
Stable-Diffusion-2.1 [69] 860M
✓
1041.6 13.45
26.96
Show-o [85]
1.3B
✓
1.0
9.24
14.99
text as source distributions
CrossFlow [52]
950M
✗
78.8 1.1
9.63
FlowTok-XL
698M
✓
20.4 22.7
10.06
7.68
FlowTok-H
1.1B
✓
26.1 18.2
9.67
7.15
Table 2. Zero-Shot Text-to-Image Generation Results on COCO and MJHQ-30K. We compare FlowTok with state-of-the-art methods,
categorized into two approaches: (1) text as conditions, where text tokens are used as conditions to guide the generation process, and (2)
text as source distributions, where the model directly learns the alignment between text and image distributions. “open-data”: Models are
trained exclusively with publicly available datasets. “T”: Model training cost, measured in 8 A100 days using float16 precision. “I”: Model
inference throughput, measured at 256px resolution in samples per second on a single A100 with batch size 64 using float16 precision.

method
B@4↑ M↑ R↑ C↑ S↑
direct flow from image to text distributions
CrossFlow [52]
36.4 27.8 57.1 116.2 20.4
FlowTok-XL
37.1 27.8 57.6 117.0 20.5
other methods
MNIC [31]
30.9 27.5 55.6 108.1 21.0
MIR [43]
32.5 27.2 - 109.5 20.6
NAIC-CMAL [33] 35.3 27.3 56.9 115.5 20.8
SATIC [95]
32.9 27.0 - 111.0 20.5
SCD-Net [56]
37.3 28.1 58.0 118.0 21.6

30K [45] in Tab. 2. The compared methods are categorized
into two groups: text as conditions, where text serves as a
guiding signal for image generation, and text as source distributions, where text is directly modeled as a distribution in
the generative process. As observed, FlowTok achieves comparable performance to prior methods in both categories on
COCO FID-30K. Specifically, compared to CrossFlow [52],
which also uses text as the source distribution, FlowTok-H
attains a FID-30K of 9.67—roughly on par with CrossFlow.
When further evaluating FlowTok on MJHQ-30K to assess
the aesthetic quality of generated images, we find that, despite being trained solely on publicly available datasets without access to high-quality proprietary data, FlowTok-XL already surpasses other state-of-the-art models, demonstrating
its ability to generate diverse, high-quality images. Furthermore, FlowTok-H further improves the FID score to 7.15,
underscoring its superior image generation capabilities.
Beyond performance, FlowTok requires significantly
fewer training resources compared to existing state-of-theart models. Specifically, FlowTok-XL completes training
in just 20.4 8-A100 days, while FlowTok-H increases the
budget slightly to 26.1 8-A100 days. In contrast, the most efficient text-as-condition model, PixArt-α [16], still demands
94.1 8-A100 days. Compared to CrossFlow [52], which also
treats text as source distributions and requires 78.8 8-A100
days, FlowTok is much more efficient.
Additionally, FlowTok demonstrates significantly faster
inference speeds. At 256px resolution, FlowTok-XL generates 22.7 images per second, while FlowTok-H achieves
18.2 images per second. In contrast, PixArt-α runs at 7.9
images per second, and Show-o at just 1.0 images per second. More notably, within the text as source distributions

Table 3. Image-to-Text Generation Results on COCO. FlowTok achieves performance comparable to state-of-the-art methods
on image-to-text generation, evaluated on the COCO Karpathy
split. For a fair comparison, we restrict our evaluation to nonautoregressive methods trained without CIDEr optimization.

tion, we report FID-30K on the COCO [49], and FID on
MJHQ-30K [45]. For image-to-text generation, we report
BLEU-4 [60], METEOR [10], ROUGE [48], CIDEr [82],
and SPICE [8] on the COCO Karpathy Split [39]. To incorporate classifier-free guidance (CFG) [35] within FlowTok,
we follow CrossFlow[52] and utilize a CFG indicator. Unless otherwise stated, we find that using only 20 steps for
sampling is sufficient due to the small 1D latent shape of
FlowTok. This significantly speeds up the inference process,
enabling faster generation without compromising performance.

5.2. Main Results
Text-to-Image Generation. We report zero-shot textto-image generation results on COCO [49] and MJHQ7

target
COCO FID-30K↓
Ave Pool
36.02
MLP
29.14

loss type
COCO FID-30K↓
Cosine
31.80
Contrastive
29.14

γ2 COCO FID-30K↓
1.0
29.14
2.0
30.59

(a) Text Alignment Target

(b) Text Alignment Loss Function

(c) Text Alignment Loss Weight

Table 4. Ablation Studies on Text Alignment Loss. We conduct comprehensive ablation studies on three key aspects of the text alignment
loss Lalign : the alignment target (Tab. 4a), the choice of loss function (Tab. 4b), and the loss weight γ2 (Tab. 4c), aiming to identify the most
effective strategy for preserving semantic information within FlowTok during text-to-image generation. For efficient verification, we report
FID-30K on COCO using FlowTok-B, without applying the CFG indicator.

category, FlowTok achieves a 20× speedup in sampling time
compared to CrossFlow, which runs at only 1.1 images per
second. This efficiency stems from FlowTok ’s streamlined
framework and its effective use of 1D tokens, significantly
reducing computational overhead.
Image-to-Text Generation. We evaluate image-to-text generation on COCO [49] using the Karpathy split [39], with
results summarized in Tab. 3. To ensure a fair comparison,
we categorize methods into two groups: direct flow from
image to text distributions, which represents a new paradigm
leveraging flow matching for direct image-to-text transformation, and other methods, considering only those trained
without CIDEr optimization. Within the direct flow category, FlowTok-XL consistently outperforms its counterpart,
CrossFlow [52], across most metrics. Specifically, FlowTokXL achieves a BLEU-4 (B@4) score of 37.1, surpassing
CrossFlow by 0.7, and a CIDEr score of 117.0, exceeding
CrossFlow by 0.8. Moreover, compared to state-of-the-art
methods from other paradigms, FlowTok-XL demonstrates
competitive performance, highlighting direct flow matching
as a promising approach for image-to-text generation. Notably, FlowTok performs image-to-text generation under the
same formulation using compact 1D tokens, theoretically
requiring fewer training resources and enabling faster sampling compared to paradigms that operate on 2D latents, as
adopted by CrossFlow. However, a direct quantitative comparison is not possible, as CrossFlow has not released the
corresponding checkpoint for evaluation.

significantly worse compared to using a simple MLP to
learn the alignment target (row 2), as inspired by prior
works [32, 59, 92]. We attribute this performance gap to
the fact that adjacent channels in the CLIP text embedding
are not necessarily correlated, and simple average pooling
discards too much semantic information. In contrast, a learnable MLP mitigates this information loss, making it a more
effective choice for defining the text alignment target.
Text Alignment Loss Function. Next, we examine the text
alignment loss function in Tab. 4b. Besides the contrastive
loss adopted from [65], we explore using a cosine similarity
loss, similar to [87]. Specifically, we compute the cosine
similarity between the text tokens and the alignment target,
applying a penalty to pairs with similarity below a threshold.
Our experiments show that while both loss functions are
effective, the contrastive loss achieves better performance.
Text Alignment Loss Weight. Finally, we investigate the
impact of the text alignment loss weight, γ2 in Tab. 4c. Our
results indicate that setting γ2 to 1.0, equal to the weight of
the flow matching loss, is sufficient to preserve semantic information while maintaining high-quality image generation.
Increasing γ2 further can cause the text alignment loss to
dominate the overall objective during early training stages,
potentially hindering final performance.

6. Conclusion
In this paper, we introduce FlowTok, a minimal yet powerful
framework that enables seamless direct flow between 1D
text and image tokens. Through carefully designed key modules and loss functions, FlowTok projects both modalities
into a unified 1D latent space while preserving semantic
information, enabling both text-to-image and image-to-text
generation under the same formulation. This design makes
FlowTok highly memory-efficient, supporting an 8K batch
size on just 8 A100 GPUs during training. Additionally, its
simplicity accelerates convergence—within approximately
20 days on 8 A100 GPUs, FlowTok achieves performance
comparable to state-of-the-art models that require significantly longer training times. The streamlined design also
enables over 10× faster sampling than modern text-to-image
generative models. By releasing our code, we aim to further
advance research in text-image cross-modal generation.

5.3. Ablation Studies
We conduct ablation studies on text-to-image generation using FlowTok-B and evaluate them on COCO for efficiency.
Our ablations focus on the design of the text alignment loss,
as it plays a critical role in preserving semantic information.
Specifically, we investigate three key aspects: the text alignment target (Tab. 4a), the choice of loss function (Tab. 4b),
and the loss weight (Tab. 4c). Details are provided below.
Text Alignment Target. We first investigate the choice of
alignment target for the projected text tokens ZT in Tab. 4a.
A straightforward baseline is to directly apply average pooling (row 1) to the original CLIP text embedding Tinit along
the channel dimension, reducing the dimensionality from
768 to 16 to match ZT . However, this approach performs
8

hyper-parmeters pre-training fine-tuning
optimizer
AdamW
AdamW
0.9
0.9
optimizer-β1
0.95
0.95
optimizer-β2
weight decay
0.03
0.03
0.0004
0.0002
lr
lr scheduling
constant
constant
lr warmup steps
10K
0
4096
4096
batch size
training steps
250K
150k

Appendix
The supplementary material provides additional information:
• Sec. A: More implementation details, including dataset
filtering, and FlowTok training hyperparameters.
• Sec. B: Additional qualitative text-to-image and image-totext generation samples produced by FlowTok.
• Sec. C: Discussions on limitations and future work of
FlowTok.

A. More Implementation Details
model
Text Decoder
Image Tokenizer

dataset

COCO [49]
DataComp [29]
DataComp [29]
FlowTok: pre-training
CC12M [14]
LAION-aesthetic [1]
DataComp† [29]
LAION-art† [3]
FlowTok: fine-tuning LAION-pop† [4]
DALLE3-1M [25]
JourneyDB [77]

Table 6. Training Hyper-parameters for FlowTok.

filtering
resolution aesthetic watermark
✓
✓
✓
✓
✓
✓
✓

✓ (5.0)
✓ (5.0)
✓ (6.0)

B. Qualitative Examples of FlowTok
Additional Generation Results. Fig. 5, Fig. 6, and Fig. 7
present additional text-to-image generation results produced
by FlowTok, demonstrating its ability to generate diverse,
high-fidelity images. Meanwhile, Fig. 8 displays the imageto-text generation results, showcasing FlowTok’s capability
to produce accurate and descriptive captions.

✓
✓
✓
✓
✓
✓

C. Limitations and Future Work

Table 5. Training Data Details. The filtering criteria applied include resolution (aspect ratio < 2 and longer side ≥ 256), aesthetic
score (predicted score exceeding the specified value in parentheses), and watermark detection (removal of images predicted to
contain watermarks). †: We use the re-captioned version released
by MaskGen [40], which contains improved captions.

The primary limitation of FlowTok arises during text-toimage generation. To match the compact dimensionality of
image latents (e.g., 16), FlowTok projects CLIP text embeddings into the same low-dimensional latent space. While
the text alignment loss helps preserve semantic information,
some degree of information loss is inevitable during this projection. Consequently, the alignment between text and generated images may be weaker compared to state-of-the-art
models employing cross-attention mechanisms. To address
this, one potential solution is to introduce a stronger alignment loss that better retains textual semantics. A more fundamental approach, however, involves increasing the channel
dimensionality of image latents by aligning them with vision
foundation models [59] during image tokenizer training, as
suggested by VA-VAE [88]. This strategy aims to identify an
optimal channel dimension for both image and text tokens,
achieving a balance between preserving semantic information and maintaining efficiency in training and inference.
Additionally, FlowTok currently utilizes only the vanilla
flow matching technique to validate the framework’s effectiveness. However, many recent advancements in flow matching, such as logit-normal sampling [26], have not yet been
explored in our model. Incorporating these techniques could
accelerate convergence and enhance performance.
Finally, FlowTok serves as a starting point for exploring
efficient direct evolution between text and image modalities.
In the future, we aim to extend to a more general framework
that can accommodate a broader range of modalities, supporting additional tasks under the same unified formulation.

Dataset Filtering. In line with previous works [40], we
apply three filtering criteria to curate open data for training
the image tokenizer and FlowTok: resolution, aesthetic quality, and watermark filtering. The COCO [19, 49] dataset is
used directly to train the text decoder without any filtering.
Details of the applied filtering criteria are shown in Tab. 5.
Specifically, resolution filtering is applied during the training of the image tokenizer and for text-to-image generation.
This ensures that the longer side of each image is at least 256
pixels and the aspect ratio is below 2. For text-to-image training, we further apply aesthetic filtering using the LAIONaesthetic-v2 predictor [2] to retain only high-quality images.
Images with aesthetic scores above 5.0 are retained during
the pre-training stage, while a stricter threshold of 6.0 is used
during fine-tuning to ensure even higher image quality.
Additionally, watermark filtering is implemented for
FlowTok’s text-to-image generation by using the LAIONWatermarkDetector [5], removing images with watermark
probabilities exceeding 0.5. Synthetic datasets such as JourneyDB [77] and DALLE3-1M [25] are exempt from these
filtering steps, as they inherently meet our high resolution
and quality standards.
Training Hyper-parameters. Tab. 6 provides the complete
list of hyper-parameters used for training FlowTok.
9

Figure 5. Text-to-Image Generation Results by FlowTok. FlowTok generates diverse, high-fidelity images.

Figure 6. Text-to-Image Generation Results by FlowTok. FlowTok generates diverse, high-fidelity images.

10

Figure 7. Text-to-Image Generation Results by FlowTok. FlowTok generates diverse, high-fidelity images.

“A girl is blowing
out a candle”

“A wooden table
next to window”

“Two men are riding
motorcycles on road”

“Two giraffes are eating
leaves and grass”

“A cat eating a banana”

“A zebra standing
in a snow field”

“A man is running
on the field with a soccer”

“A little girl is eating pizza”

Figure 8. Image-to-Text Generation Results by FlowTok. FlowTok generates precise captions.

11

References

and Zhenguo Li. PixArt-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In ICLR,
2024. 2, 3, 4, 6, 7
[17] Jieneng Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, and
Liang-Chieh Chen. Vitamin: Designing scalable vision models in the vision-language era. In CVPR, 2024. 4
[18] Shoufa Chen, Chongjian Ge, Yuqi Zhang, Yida Zhang,
Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao
Lai, Yifei Hu, et al. Goku: Flow based video generative
foundation models. arXiv preprint arXiv:2502.04896, 2025.
3, 4
[19] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick.
Microsoft coco captions: Data collection and evaluation
server. arXiv preprint arXiv:1504.00325, 2015. 6, 9
[20] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying
vision-and-language tasks via text generation. In ICML, 2021.
2
[21] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang
Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, Matthew Yu, Abhishek
Kadian, Filip Radenovic, Dhruv Mahajan, Kunpeng Li, Yue
Zhao, Vladan Petrovic, Mitesh Kumar Singh, Simran Motwani, Yi Wen, Yiwen Song, Roshan Sumbaly, Vignesh Ramanathan, Zijian He, Peter Vajda, and Devi Parikh. Emu:
Enhancing image generation models using photogenic needles in a haystack. arXiv preprint arXiv:2309.15807, 2023.
6
[22] Xueqing Deng, Qihang Yu, Ali Athar, Chenglin Yang, Linjie Yang, Xiaojie Jin, Xiaohui Shen, and Liang-Chieh Chen.
Coconut-pancap: Joint panoptic segmentation and grounded
captions for fine-grained understanding and generation. arXiv
preprint arXiv:2502.02589, 2025. 3
[23] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In ICLR, 2021. 4, 6
[24] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson,
Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e:
An embodied multimodal language model. arXiv preprint
arXiv:2303.03378, 2023. 2
[25] Ben Egan, Alex Redden, XWAVE, and SilentAntagonist.
Dalle3 1 Million+ High Quality Captions.
https : / / huggingface . co / datasets /
ProGamerGov/synthetic-dataset-1m-dalle3high-quality-captions, 2024. 6, 9
[26] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz,
Axel Sauer, Frederic Boesel, et al. Scaling rectified flow
transformers for high-resolution image synthesis. In ICML,
2024. 2, 3, 4, 9
[27] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen
Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and
Yonglong Tian. Fluid: Scaling autoregressive text-to-image

[1] LAION2B-en-aesthetic. https://huggingface.co/
datasets/laion/laion2B-en-aesthetic, . 6, 9
[2] LAION-aesthetics predictor V2. https://github.com/
christophschuhmann / improved - aesthetic predictor, . 9
[3] LAION-art. https://huggingface.co/datasets/
laion/laion-art, . 6, 9
[4] LAION-pop.
https : / / huggingface . co /
datasets/laion/laion-pop, . 6, 9
[5] LAION-5B-WatermarkDetection.
https :
/ / github . com / LAION - AI / LAION - 5B WatermarkDetection, . 9
[6] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katherine Millican, Malcolm Reynolds, et al. Flamingo: a
visual language model for few-shot learning. NeurIPS, 2022.
2
[7] Michael S Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint
arXiv:2209.15571, 2022. 2, 3
[8] Peter Anderson, Basura Fernando, Mark Johnson, and
Stephen Gould. Spice: Semantic propositional image caption
evaluation. In ECCV, 2016. 7
[9] Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Qing-Guo Chen,
Xiangtai Li, Zhen Dong, Lei Zhu, and Shuicheng YAN. Meissonic: Revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis. In ICLR, 2025.
3, 4
[10] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic
metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation
and/or summarization, 2005. 7
[11] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li,
Hang Su, and Jun Zhu. All are worth words: A vit backbone
for diffusion models. In CVPR, 2023. 2
[12] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng
Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee,
Yufei Guo, et al. Improving image generation with better
captions. Computer Science, 2(3):8, 2023. 3
[13] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,
José Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy,
William T Freeman, Michael Rubinstein, et al. Muse: Textto-image generation via masked generative transformers. In
ICML, 2023. 3
[14] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
Soricut. Conceptual 12m: Pushing web-scale image-text
pre-training to recognize long-tail visual concepts. In CVPR,
2021. 6, 9
[15] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao,
Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and
Zhenguo Li. PixArt-Σ: Weak-to-strong training of diffusion
transformer for 4k text-to-image generation. In ECCV, 2024.
6
[16] Junsong Chen, Jincheng YU, Chongjian GE, Lewei Yao, Enze
Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu,

12

generative models with continuous tokens. arXiv preprint
arXiv:2410.13863, 2024. 3, 4
[28] Johannes S Fischer, Ming Gui, Pingchuan Ma, Nick Stracke,
Stefan A Baumann, and Björn Ommer. Boosting latent diffusion with flow matching. arXiv preprint arXiv:2312.07360,
2023. 3
[29] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan
Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,
Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets.
NeurIPS, 2023. 6, 9
[30] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,
Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based
text-to-image generation with human priors. In ECCV, 2022.
3
[31] Junlong Gao, Xi Meng, Shiqi Wang, Xia Li, Shanshe Wang,
Siwei Ma, and Wen Gao. Masked non-autoregressive image
captioning. arXiv preprint arXiv:1906.00717, 2019. 7
[32] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin
Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach
to self-supervised learning. NeurIPS, 2020. 8
[33] Longteng Guo, Jing Liu, Xinxin Zhu, Xingjian He, Jie Jiang,
and Hanqing Lu. Non-autoregressive image captioning with
counterfactuals-critical multi-agent learning. arXiv preprint
arXiv:2005.04690, 2020. 7
[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR, 2016.
6
[35] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. arXiv preprint arXiv:2207.12598, 2022. 7
[36] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. 2, 3
[37] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,
Andrew Zisserman, and Joao Carreira. Perceiver: General
perception with iterative attention. In ICML, 2021. 2
[38] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representation
learning with noisy text supervision. In ICML, 2021. 2
[39] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2015. 6,
7, 8
[40] Dongwon Kim, Ju He, Qihang Yu, Chenglin Yang, Xiaohui Shen, Suha Kwak, and Liang-Chieh Chen. Democratizing text-to-image masked generative models with compact text-aware one-dimensional tokens. arXiv preprint
arXiv:2501.07730, 2025. 2, 3, 4, 6, 9
[41] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. 2
[42] Black Forest Labs. Flux. https://github.com/
black-forest-labs/flux, 2024. 2
[43] Jason Lee, Elman Mansimov, and Kyunghyun Cho. Deterministic non-autoregressive neural sequence modeling by iterative
refinement. arXiv preprint arXiv:1802.06901, 2018. 7

[44] Sangyun Lee, Beomsu Kim, and Jong Chul Ye. Minimizing
trajectory curvature of ode-based generative models. In ICML,
2023. 3
[45] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao
Xu, and Suhail Doshi. Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation.
arXiv preprint arXiv:2402.17245, 2024. 7
[46] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip:
Bootstrapping language-image pre-training for unified visionlanguage understanding and generation. In ICML, 2022. 2
[47] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,
and Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language. arXiv preprint
arXiv:1908.03557, 2019. 2
[48] Chin-Yew Lin. Rouge: A package for automatic evaluation
of summaries. In Text summarization branches out, 2004. 7
[49] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
ECCV, 2014. 6, 7, 8, 9
[50] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative
modeling. In ICLR, 2023. 2, 3, 4, 6
[51] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A. Theodorou, Weili Nie, and Anima Anandkumar.
I2 sb: Image-to-image schrödinger bridge. arXiv preprint
arXiv:2302.05872, 2023. 2, 3
[52] Qihao Liu, Xi Yin, Alan Yuille, Andrew Brown, and Mannat
Singh. Flowing from words to pixels: A framework for crossmodality evolution. arXiv preprint arXiv:2412.15213, 2024.
2, 3, 7, 8
[53] Qihao Liu, Zhanpeng Zeng, Ju He, Qihang Yu, Xiaohui Shen,
and Liang-Chieh Chen. Alleviating distortion in image generation via multi-resolution diffusion models. NeurIPS, 2024.
2, 3
[54] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight
and fast: Learning to generate and transfer data with rectified
flow. In ICLR, 2023. 2, 3, 4
[55] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:
Pretraining task-agnostic visiolinguistic representations for
vision-and-language tasks. NeurIPS, 2019. 2
[56] Jianjie Luo, Yehao Li, Yingwei Pan, Ting Yao, Jianlin Feng,
Hongyang Chao, and Tao Mei. Semantic-conditional diffusion networks for image captioning. In CVPR, 2023. 7
[57] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M
Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring
flow and diffusion-based generative models with scalable
interpolant transformers. In ECCV, 2024. 3
[58] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741, 2021. 7
[59] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo,
Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel
Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2:
Learning robust visual features without supervision. arXiv
preprint arXiv:2304.07193, 2023. 8, 9

13

[60] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. Bleu: a method for automatic evaluation of machine
translation. In ACL, 2002. 7
[61] William Peebles and Saining Xie. Scalable diffusion models
with transformers. In ICCV, 2023. 2, 3, 6
[62] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann,
Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach.
Sdxl: Improving latent diffusion models for high-resolution
image synthesis. arXiv preprint arXiv:2307.01952, 2023. 3,
4, 7
[63] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra,
Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary,
Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan
Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng
Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt
Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly,
Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak
Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly
Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu,
Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang
Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao
Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya,
Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce
Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang,
John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar,
Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone
Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and
Yuming Du. Movie gen: A cast of media foundation models.
arXiv preprint arXiv:2410.13720, 2024. 3
[64] Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles
Domingo-Enrich, Brandon Amos, Yaron Lipman, and
Ricky TQ Chen. Multisample flow matching: Straightening flows with minibatch couplings. arXiv preprint
arXiv:2304.14772, 2023. 3
[65] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervision.
In ICML, 2021. 2, 4, 5, 8
[66] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):
3, 2022. 7
[67] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille,
and Liang-Chieh Chen. Flowar: Scale-wise autoregressive image generation meets flow matching. arXiv preprint
arXiv:2412.15205, 2024. 3
[68] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille,
and Liang-Chieh Chen. Beyond next-token: Next-x prediction for autoregressive visual generation. arXiv preprint
arXiv:2502.20388, 2025. 3
[69] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Björn Ommer. High-resolution image

synthesis with latent diffusion models. In CVPR, 2022. 2, 3,
6, 7
[70] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,
Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael
Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language
understanding. NeurIPS, 2022. 3
[71] Noam Shazeer. Glu variants improve transformer. arXiv
preprint arXiv:2002.05202, 2020. 2, 4, 6
[72] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and
Arnaud Doucet. Diffusion schrödinger bridge matching.
NeurIPS, 2023. 2, 3
[73] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In ICML, 2015. 2
[74] Yang Song and Stefano Ermon. Generative modeling by
estimating gradients of the data distribution. NeurIPS, 2019.
2
[75] stabilityai, 2023. 3
[76] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen,
and Yunfeng Liu. Roformer: Enhanced transformer with
rotary position embedding. arXiv preprint arXiv:2104.09864,
2021. 2, 4, 6
[77] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong
Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin,
Yi Wang, et al. Journeydb: A benchmark for generative image
understanding. NeurIPS, 2023. 6, 9
[78] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue
Peng, Ping Luo, and Zehuan Yuan. Autoregressive model
beats diffusion: Llama for scalable image generation. arXiv
preprint arXiv:2406.06525, 2024. 7
[79] Alexander Tong, Kilian Fatras, Nikolay Malkin, Guillaume
Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Guy Wolf, and
Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. arXiv preprint
arXiv:2302.00482, 2023. 3
[80] Alexander Tong, Nikolay Malkin, Kilian Fatras, Lazar
Atanackovic, Yanlei Zhang, Guillaume Huguet, Guy Wolf,
and Yoshua Bengio.
Simulation-free schr\" odinger
bridges via score and flow matching. arXiv preprint
arXiv:2307.03672, 2023. 2, 3
[81] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. NeurIPS, 2017. 6
[82] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. Cider: Consensus-based image description evaluation. In CVPR, 2015. 7
[83] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia
Tsvetkov, and Yuan Cao. Simvlm: Simple visual language
model pretraining with weak supervision. arXiv preprint
arXiv:2108.10904, 2021. 2
[84] Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui
Shen, Daniel Cremers, and Liang-Chieh Chen. Maskbit:
Embedding-free image generation via bit tokens. arXiv
preprint arXiv:2409.16211, 2024. 3
[85] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang,
Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen,

14

Zhenheng Yang, and Mike Zheng Shou. Show-o: One single
transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 3, 7
[86] Chenglin Yang, Celong Liu, Xueqing Deng, Dongwon Kim,
Xing Mei, Xiaohui Shen, and Liang-Chieh Chen. 1.58-bit
flux. arXiv preprint arXiv:2412.18653, 2024. 3
[87] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi
Feng, and Hengshuang Zhao. Depth anything: Unleashing
the power of large-scale unlabeled data. In CVPR, 2024. 8
[88] Jingfeng Yao and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models.
arXiv preprint arXiv:2501.01423, 2025. 4, 9
[89] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan
Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei
Yang, Burcu Karagol Ayan, et al. Scaling autoregressive
models for content-rich text-to-image generation. TMLR,
2022. 3
[90] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and LiangChieh Chen. Randomized autoregressive visual generation.
arXiv preprint arXiv:2411.00776, 2024. 3
[91] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen,
Daniel Cremers, and Liang-Chieh Chen. An image is worth
32 tokens for reconstruction and generation. NeurIPS, 2024.
2, 4
[92] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong,
Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers
is easier than you think. In ICLR, 2025. 3, 8
[93] Wendi Zheng, Jiayan Teng, Zhuoyi Yang, Weihan Wang,
Jidong Chen, Xiaotao Gu, Yuxiao Dong, Ming Ding, and Jie
Tang. Cogview3: Finer and faster text-to-image generation
via relay diffusion. arXiv preprint arXiv: 2403.05121, 2024.
6
[94] Linqi Zhou, Aaron Lou, Samar Khanna, and Stefano Ermon. Denoising diffusion bridge models. arXiv preprint
arXiv:2309.16948, 2023. 2, 3
[95] Yuanen Zhou, Yong Zhang, Zhenzhen Hu, and Meng Wang.
Semi-autoregressive transformer for image captioning. In
ICCV, 2021. 7

15

