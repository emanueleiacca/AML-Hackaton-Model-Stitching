LABridge: Text–Image Latent Alignment Framework
via Mean-Conditioned OU Process
Huiyang Shao1,2

Xin Xia2,*
1

Yuxi Ren2

Xing Wang2

2

ByteDance Seed

Tsinghua University

Xuefeng Xiao2,†

Abstract
Diffusion models have emerged as state-of-the-art in image synthesis.However, it
often suffer from semantic instability and slow iterative denoising. We introduce
Latent Alignment Framework (LABridge), a novel Text–Image Latent Alignment
Framework via an Ornstein–Uhlenbeck (OU) Process, which explicitly preserves
and aligns textual and visual semantics in an aligned latent space. LABridge
employs a Text-Image Alignment Encoder (TIAE) to encode text prompts into
structured priors that are directly aligned with image latents. Instead of a homogeneous Gaussian, Mean-Conditioned OU process smoothly interpolates between
these text-conditioned priors and image latents, improving stability and reducing
the number of denoising steps. Extensive experiments on standard text-to-image
benchmarks show that LABridge achieves better text–image alignment metric and
competitive FID scores compared to leading diffusion baselines. By unifying text
and image representations through principled latent alignment, LABridge paves
the way for more efficient, semantically consistent, and high-fidelity text to image
generation.

1

Introduction

Diffusion models [Sohl-Dickstein et al., 2015, Song and Ermon, 2019, Ho et al., 2020, Song et al.,
2021] represent a significant advancement in generative modeling, demonstrating state-of-the-art
performance in diverse tasks such as text generation [Li et al., 2022, Wu et al., 2023], high-fidelity
image synthesis [Rombach et al., 2022, Ramesh et al., 2022, Ho et al., 2022, Shao et al., 2023,
Lin et al., 2025], image restoration [Blattmann et al., 2023, Brooks et al., 2024], and 3D content
creation [Liu et al., 2023, Evans et al., 2024]. These models typically operate by defining a forward
diffusion process that gradually adds noise to data, transforming it into a simple prior distribution
(often Gaussian), and then learning a reverse process to generate data by iteratively denoising
samples drawn from this prior. The mathematical foundation often relies on stochastic differential
equations (SDEs) [Song et al., 2021] or discrete-time Markov chains, enabling powerful sampling
and manipulation strategies like accelerated generation [Mei et al., 2024], timestep analysis, and
model distillation [Shao et al., 2025, Xie et al., 2024, Nguyen et al., 2024, Kang et al., 2024].
Existing Challenges
Despite their success, standard diffusion models face limitations, particularly in text-to-image generation, stemming largely from their reliance on a fixed, often unstructured prior:
1. Instability and Ambiguity from Homogeneous Priors: Conventional methods map the entire
diverse data distribution qdata (x) to a single, fixed prior, typically N (0, I). This forces distinct
semantic concepts (e.g., "cat," "dog," "landscape") onto the same simple latent structure. This
collapsing of priors can lead to instability, as the reverse process must disentangle these varied
* Corresponding Author

†

Project Leader

39th Conference on Neural Information Processing Systems (NeurIPS 2025).

Cat Region

Cat Region
Overlap

Ca

No Overlap
G
Pr aus
io sia
r
n

pt

om

Pr

A
:“

t”

ca

e
ag
Im n
et tio
rg bu
Ta istri
D

e
ag
Im n
et tio
rg bu
Ta istri
D

Dog Region

(a) Diffusion Sampling Process

tP

rio

r

“A

io

“A

t”

ca

Dog Region
Do

g

(b) LABridge Sampling Process

Pr

r

g”

do

Figure 1: Comparision of sampling process between traditional diffusion and LABridge
semantics from a homogeneous starting point. Furthermore, it can create ambiguity in score
estimation (∇xt log p(xt )), as paths originating from different initial data points might overlap
significantly in the latent space near t = T , making the learned score an average that lacks precision
for any specific semantic direction. We provide analysis in Appendix E.3
2. Inefficient Sampling: As shown in Fig. 1 (a), the diffusion sampling process follows a curved
path. Without strong guidance, especially in early steps, the process can be slow, requiring many
iterations (NFE - Number of Function Evaluations) to converge to a high-fidelity image that
accurately reflects the conditioning. We provide analysis in Appendix D.3
3. Weak Text-Vision Alignment: While conditioning mechanisms inject textual information, the
fundamental diffusion process still operates between the image manifold and Gaussian prior. This
indirect connection can limit the precise alignment between the generated image and complex or
nuanced text prompts, especially for out-of-distribution concepts (analysis in Appendix D.2).
Our Innovations: LABridge
To overcome these challenges, we introduce LABridge, a framework designed to enhance text-vision
alignment and accelerate sampling in diffusion models. LABridge leverages two core ideas: a
dedicated encoder TIAE to create structured, text-conditioned priors, and an OU diffusion process to
connect image latents directly to these priors.
1. Text Encoder for Structured Priors: We employ a encoder to process text prompts (y) and
generate corresponding latent representations µT (y). The latent µT (y) acts as a structured, textspecific prior mean in the aligned latent space of an image autoencoder. The TIAE ensures that
semantically similar texts map to nearby priors, preserving semantic structure.
2. OU Diffusion Process for Alignment and Stability: We adopt OU process explicitly models
the stochastic path between the image latent x0 (obtained from a VAE encoder) and a distribution
centered around the text-specific prior µT (y), i.e., N (µT (y), σT2 I). The OU process, known for
its mean-reverting property, naturally pulls the state towards the target mean µT (y), inherently
promoting stability and alignment.
LABridge offers a new perspective: it frames text-to-image generation as learning a stochastic process
between the image latent manifold and a manifold of text-conditioned priors. This explicit alignment
via the process mechanism enhances semantic consistency and, by providing a better starting point
and direction, significantly speeds up the sampling process.
In summary, our contributions are threefold:
• We propose LABridge, a novel framework utilizing an encoder TIAE to generate structured
text-conditioned priors µT (y) and an OU diffusion process to align them with image latents x0 .
• We demonstrate that the OU process mechanism, combined with structured priors, improves
text-vision alignment and inherently accelerates sampling by providing a more certain trajectory,
supported by theoretical analysis.
• We validate LABridge experimentally, showing significant improvements in sampling and textimage consistency metrics while maintaining competitive image fidelity compared to baselines.

2

Preliminaries

In this section, we introduce the essential background concepts. Additional technical details, and
extended definitions are deferred to Appendix A.
2

2.1

Diffusion Processes and Score Matching

Diffusion Process. Let x ∈ Rd follow the data distribution qdata (x). A forward diffusion process
defines a sequence of latent variables {xt }t∈[0,T ] starting from x0 ∼ qdata (x) and evolving towards a
simple prior distribution pT (xT ) = N (0, I) as t goes from 0 to T . This evolution is often described
by a stochastic differential equation (SDE) [Song et al., 2021]:
dxt = f (xt , t) dt + g(t) dwt ,

(1)

where f (·, t) is the drift function, g(t) is the diffusion coefficient, and wt is a standard Wiener
process. Generating new data involves reversing this process. The corresponding reverse-time SDE is
given by [Anderson, 1982]:


dxt = f (xt , t) − g(t)2 ∇xt log pt (xt ) dt + g(t) dw̄t ,
(2)
where w̄t is a Wiener process running backward in time, and pt (xt ) is the marginal probability
density of xt . The crucial term is the score function ∇xt log pt (xt ).
Score Matching. In practice, the true score ∇xt log pt (xt ) is unknown and is approximated by a
time-dependent neural network sθ (xt , t), often conditioned on additional information y (like text
embeddings), denoted sθ (xt , t, y). This network is trained by minimizing a score matching objective
[Hyvärinen and Dayan, 2005, Vincent, 2011]. For many diffusion processes (like VP and VE), the
conditional score ∇xt log pt (xt | x0 ) is tractable. A common training objective is:
h
i
2
LSM (θ) = Et∼U (0,T ),x0 ∼qdata ,ϵ∼N (0,I) λ(t) sθ (αt x0 + σt ϵ, t, y) − ∇xt log pt (xt | x0 ) , (3)
where pt (xt | x0 ) = N (xt ; αt x0 , σt2 I) defines the transition kernel, and λ(t) is a weighting function.
For this Gaussian kernel, ∇xt log pt (xt | x0 ) = −(xt − αt x0 )/σt2 = −ϵ/σt . This leads to the
widely used noise prediction objective:
h
i
2
Ldenoise (θ) = Et,x0 ,ϵ λ′ (t) ϵθ (αt x0 + σt ϵ, t, y) − ϵ ,
(4)
where ϵθ is the network predicting the noise ϵ, related to the score network by sθ (xt , t, y) =
−ϵθ (xt , t, y)/σt .
2.2

Diffusion Bridge Models

While standard diffusion maps data to a fixed prior, a diffusion bridge connects two specified endpoint
distributions, p0 (x0 ) and pT (xT ), which can both be complex. This is particularly relevant for tasks
involving paired data (x0 , xT ) ∼ qdata (x0 , xT ), such as image translation or, in our case, aligning
image latents x0 with text-derived priors µT .
Stochastic Bridges via h-Transform. Given a forward SDE Eq. (1), Doob’s h-transform provides
a way to condition the process to start at x0 at t = 0 and end exactly at xT = y at t = T . The
resulting bridge SDE is:


dxt = f (xt , t) + g(t)2 ∇xt log pT |t (y | xt ) dt + g(t) dwt ,
(5)
where pT |t (xT | xt ) is the transition probability density of the original SDE from time t to T . The
term h(xt , t, y, T ) = ∇xt log pT |t (y | xt ) is the "guidance" term ensuring the endpoint constraint.
For linear SDEs with Gaussian transitions (like VP, VE, OU), this term is often tractable.
Denoising Diffusion Bridge Models [Zhou et al., 2023]. Instead of exact endpoints, we often want
to sample from a conditional distribution q(x0 | xT ) given paired data (x0 , xT ) ∼ qdata . This can
be achieved by reversing a bridge process designed such that its marginals q(x0 , xT ) approximate
qdata (x0 , xT ). Zhou et al. [2023] show that the reverse SDE for sampling xt given xT = y is:
h
i
dxt = f (xt , t) − g 2 (t) ∇xt log qt (xt | xT = y) − ∇xt log pT |t (y | xt ) dt + g(t) dw̄t ,
(6)
where ∇xt log qt (xt | xT = y) is the score of the conditional bridge distribution.
3

2.3

Mean-Conditioned Ornstein-Uhlenbeck Process

Ornstein-Uhlenbeck (OU) Process. The OU process is a mean-reverting stochastic process often
used to model systems returning to equilibrium. Its SDE is:
dxt = θ (µ − xt ) dt + σ dwt ,

(7)

where θ > 0 is the rate of mean reversion, µ is the equilibrium mean, and σ is the volatility. The drift
term θ(µ − xt ) pulls the state xt towards µ.
Ornstein-Uhlenbeck Bridge (OUB). An OU process is a conditioned OU process that starts at
x0 at t = 0 and ends at xT = y at t = T . Its SDE can be derived using Doob’s h-transform. For
the standard OU process with σ = 1 starting from x0 at t = 0, the transition density to time T is
2
Gaussian: pT |0 (xT | x0 ) = N (xT ; µ + (x0 − µ)e−θT , σ2θ (1 − e−2θT )I). The OUB derived from
this inherits the mean-reverting property but ensures the endpoint constraint. In our work, we use a
specific form of OU process connecting x0 to a distribution around µT (y), defined by the transition:


2
−θt
−θt σ
−2θt
q(xt | x0 , µT ) = N xt ; x0 e
+ µT (1 − e ), (1 − e
)I .
(8)
2θ
This corresponds to an OU process SDE dxt = θ(µT − xt )dt + σdwt . This structure is key to
LABridge. The parameter θ controls the strength of reversion towards the text-conditioned prior
mean µT . OUB processes can encompass VP and VE under specific parameter choices.

3

Proposed Method: LABridge

3.1

Motivation: Structured Priors and Directed Diffusion

A primary challenge lies in directly using text as latent representations. Text data is fundamentally
different from typical latent variables. It is inherently infinite and unstructured; the space of
possible sentences and meanings is vast and does not easily map to predefined, discrete categories
or a simple, low-dimensional manifold. Converting raw text directly into a structured latent code
suitable for generative models is non-trivial.
Instead, we must first represent text in a suitable format, typically through powerful pretrained
embedding models (like CLIP, T5) that capture semantic meaning in high-dimensional vectors (Ey ).
However, simply using these embeddings as conditioning signals for a standard diffusion process
(mapping image I to noise ϵ) still relies on the model implicitly learning the complex relationship
between the text embedding space and the image manifold during the denoising process initiated
from a generic prior.
This motivates the need to represent text in
a continuous latent space that is explicitly
aligned with the latent space of images. Rather
than mapping images to generic noise, we propose mapping images x0 to text-conditioned priors µT (y) that live in the similar latent space.
This requires:
1. A mechanism to map text embeddings Ey to
target latent priors µT (y).
2. Ensuring these priors µT (y) are semantically
consistent (similar texts map to nearby priors)
and aligned with corresponding image latents.
3. A generative process that efficiently connects
these aligned endpoint distributions.

Figure 2: Conceptual illustration: Alignment and
Continuity of Text-Image Latent Spaces

4

Raw Image

Diffusion Model
Text Latent

Image Encoder

TIAE

Text Embedding

Image Latent

Text

Text Embedding
Model
pretrained & frozen

Figure 3: Overview of the LABridge. An image I is encoded to x0 by EVAE . The corresponding
text y is embedded to Ey by EEmb and then mapped to a target prior mean µT (y) by the Text-Vision
Alignment Encoder ETE (TIAE). An OU diffusion process learns the stochastic path between x0 and
the distribution N (µT (y), σT2 I). During inference, sampling starts from xT and follows the learned
reverse process dynamics towards x0 , which is then decoded to an image Iˆ by DVAE .
3.2 LABridge Framework Overview
As depicted in Fig. 3, LABridge integrates three key components:
1. Pretrained Image Autoencoder: We utilize a frozen Variational Autoencoder (VAE) with encoder
EVAE and decoder DVAE . The encoder maps an input image I to a latent representation x0 =
EVAE (I) in Rd , effectively moving the diffusion process to a compressed latent space. The decoder
reconstructs the image Iˆ = DVAE (x0 ).
2. Text-Vision Alignment Encoder (TIAE): This novel component, denoted ETE , is responsible for
bridging the semantic gap between text and vision. It takes text embeddings Ey (obtained from a
frozen pretrained text model EEmb , e.g., CLIP [Radford et al., 2021]) as input and outputs a target
prior mean µT (y) = ETE (Ey ) ∈ Rd . The TIAE is specifically trained to ensure that µT (y) is both
semantically meaningful (reflecting the content of y) and aligned with the corresponding image
latents x0 in the VAE’s latent space.
3. Mean-Conditioned OU Diffusion Process: Instead of a standard diffusion process mapping to
N (0, I), we employ an OU diffusion process [Zhou et al., 2023] to model the stochastic transition
between the image latent x0 and the text-conditioned prior distribution N (µT (y), σT2 I). The
process is parameterized by a score network sθ (xt , t, y) or, equivalently, a noise prediction network
ϵθ (xt , t, y). The OU process inherently incorporates mean reversion towards µT (y), promoting
stability and directed sampling.
3.3

TIAE Architecture and Training Objective

Architecture. To ensure seamless integration and potentially leverage existing performant architectures, we adopt a structure similar to modern diffusion models for the TIAE ETE , specifically using
blocks inspired by DiT [Peebles and Xie, 2023] which handle conditional inputs effectively. It takes
the text embedding Ey as input and outputs the prior mean µT (y).
Training Objective. The TIAE is trained to produce priors µT (y) that are: (a) aligned with
corresponding image latents x0 , and (b) preserve the semantic structure of the text embeddings Ey .
We use a composite loss:
(a) Latent Alignment Loss (Lalign ): Encourages the TIAE output µT (y) to be close to the VAE latent
x0 for corresponding image-text pairs (I, y).


Lalign = E(x0 ,y)∼qdata ∥µT (y) − x0 ∥22 ,
(9)
where x0 = EVAE (I) and µT (y) = ETE (Ey ). This loss is visualized in Fig. 4.
(b) Semantic Consistency Loss (Lsem ): Ensures that the distances between latent priors reflect the
distances between text embeddings (using cosine similarity).
h
2 i
Lsem = Eyi ,yj simcos (µT (yi ), µT (yj )) − simcos (Eyi , Eyj )
.
(10)
This encourages the structure of the text embedding space to be preserved in the prior space, as
illustrated conceptually in Fig. 5.
5

Alignment
Reconstruct Image

Raw Image

Text Embedding
Model

Text

pretrained & frozen

Text Embedding

Text Latent

TIAE

Image Latent
Image Decoder

Image Encoder

Alignment

Figure 4: An overview of the TIAE training process. The input text is embedded using a pretrained
model and then processed by our TIAE to produce a latent representation µT . The alignment loss
encourages this µT to be close to the image latent x0 obtained from the corresponding raw image I
via the Image Encoder. An Image Decoder can reconstruct Iˆ from µT (for Lrec ) or from x0 .

Mean
TIAE
Std dev

...

pretrained & frozen

...

Distribution Divergence

Text

Text Embedding
Model

Text Embedding

Generated Latent Distribution

Alignment

Text1 Embedding

Text Embedding
Model

Text2

Text2 Embedding

pretrained & frozen

Text3

Text3 Embedding

Text4

Text4 Embedding

Text2

Text3
Text4

TIAE

(a) Latent Representation

All Latent Distribution

Embedding Similarity

Text1
Text1

(b) Similarity Alignment

Figure 5: Illustration of the latent space alignment and distribution analysis for TIAE. Text embeddings (left) are encoded into latent distributions centered at µT (center). The semantic consistency loss Lsem aims to ensure that similar text inputs (e.g., Text1, Text2) produce close latent
means/distributions, preserving semantic proximity, as shown by the embedding similarity matrix
(right) compared to the latent distribution similarity.
(c) Reconstruction Loss (Lrec ): To ensure µT (y) can be decoded into meaningful images, we can
add a reconstruction term comparing the decoded prior to the original image.


Lrec = E(x0 ,y)∼qdata ∥DVAE (µT (y)) − I∥pp ,
(11)
The total TIAE loss is LTIAE = wa Lalign + ws Lsem + wr Lrec , with weights wa , ws , wr .
3.4

Integration with OU Diffusion Process

Once the TIAE ETE is trained and frozen, we train the diffusion model component. LABridge employs
an OU diffusion process defined by the forward stochastic differential equation (SDE):
dxt = θ(µT (y) − xt )dt + σdwt ,

t ∈ [0, T ],

(12)

where µT (y) = ETE (EEmb (y)) is the target mean obtained from the frozen TIAE for a given text
prompt y. This SDE defines a process that starts near x0 at t = 0 and is drawn towards µT (y) as
t → ∞. The transition kernel q(xt |x0 , y) corresponding to this SDE, assuming xt depends on x0
and the target mean µT (y).
The goal is to learn the reverse process to sample x0 ∼ q(x0 |y) starting from the prior p(xT |y) =
N (xT ; µT (y), σT2 I). The reverse process is governed by the score function ∇xt log qt (xt |y). We
follow the standard practice in diffusion models and train a neural network ϵθ (xt , t, y) to predict the
noise ϵ that generated xt from x0 and µT (y) via Eq. 8. The noise prediction objective for the OU
process is:
h
i
2
LBridge = Et∼U (0,T ),(x0 ,y)∼qdata ,ϵ∼N (0,I) w′ (t) ∥ϵθ (xt , t, y) − ϵ∥2 ,
(13)
6

where w′ (t) is a time-dependent weighting function. The reverse probability flow ODE used for
sampling is:
σ2
dxt = [θ(µT (y) − xt ) +
ϵθ (xt , t, y)]dt.
(14)
2σt
3.5

Training and Inference Algorithms

The overall training procedure for LABridge consists of two sequential stages (details of training
procedure are provided in Algo. 1 in Appendix B. ):
• Stage 1: TIAE Training. Train the Text-Vision Alignment Encoder ETE using the composite loss
LTIAE on paired image-text data, with the VAE and text embedder frozen.
• Stage 2: OU Process Training. Freeze the trained ETE . Train the noise prediction network ϵθ for
the OU process using the denoising objective LBridge (Eq. 13).
The overall inference procedure for LABridge mainly based on Eq. 14 (details of inference procedure
are provided in Algo. 2 in Appendix B.).

4

Theoretical Guarantees

We provide theoretical justification for the advantages of LABridge, highlighting improvements in
text-vision alignment, sampling efficiency and stability standard diffusion models using fixed priors.
Detailed statements and proofs are provided in Sec. D and E.
Theorem 4.1 (Enhanced Text-Vision Alignment). The TIAE training objective, particularly Lalign
and Lsem , explicitly optimizes the prior mean µT (y) to be (a) close to the corresponding image latent
mean E[x0 |y] and (b) preserve the semantic structure of the text embeddings Ey . The OU process
formulation reinforces this alignment during diffusion training and generation. (Ref: Thm. D.3, Prop.
D.4 in the appendix).
Theorem 4.2 (Sampling Acceleration via Informed Initialization and Dynamics). LABridge accelerates sampling due to two factors:
(i) Reduced Initial Error: Starting the reverse process from xT ∼ N (µT (y), σT2 I) provides an
initial state closer (in expectation) to the target conditional mean E[x0 |y] compared to starting
from N (0, σT2 I). (Ref: Thm. D.5 in appendix).
(ii) Directed Drift: The OU reverse dynamics (Eq. 14) include an explicit mean reversion term
θ(µT (y) − xt ) which provides additional drift towards the text-aligned prior mean µT (y), supplementing the learned score/noise term and offering stronger guidance than standard diffusion
drifts . (Ref: Thm. D.6 in appendix).
Theorem 4.3 (Improved Sampling Stability). The inherent mean-reverting property of the OU
process drift term θ(µT (y) − xt ) enhances the stability of the reverse sampling process, making it
less prone to divergence compared to processes with zero or origin-centric drift, especially when
score estimates may be imperfect. (Ref: Thm. D.8 in appendix).
Theorem 4.4 (Tighter Evidence Lower Bound). Using the text-conditioned prior p(xT |y) =
N (µT (y), σT2 I) results in a smaller expected KL divergence between the forward process endpoint distribution q(xT |x0 , y) and the prior, compared to using a fixed prior p(xT ) = N (0, σT2 I),
provided E[x0 |y] varies significantly with y and µT (y) approximates it well. This leads to a tighter
ELBO. (Ref: Thm. D.9).

5

Experiment

We conducted a series of experiments to verify the effectiveness of LABridge under various settings.
Experiment Setup All code was performed on 8 A100 GPUs machine. For the first part, we employ
DiT-XL/2 model to learn from scratch on benchmark dataset. We adopt NV-Embed-v2 1 as pretrained
1

https://huggingface.co/nvidia/NV-Embed-v2

7

text embedding model, which output 4096 (1 × 64 × 64) vector. The training utilized AdamW optimizer
with a learning rate of 1e − 5, β1 = β2 = 0.9, weight decay of 0.03, batch size of 16, and run for
200 epochs. All images are preprocessed with center crop and resized (1024 × 1024). We tune the
weighting wa , ws and wr in the range of [0, 1]. After a brief sweep, we used wa = 1.0, ws = 0.5,
and wr = 0.2.
Dataset. The training data comprises a selection from COYO [Byeon et al., 2022] datasets, following
selection criteria in [Lin et al., 2024, Ren et al., 2024]. We evaluated performance on standard
benchmarks: COCO [Lin et al., 2014], ImageNet [Deng et al., 2009], MJHQ-30K [Li et al., 2024a].
Evaluation Metrics. We evaluate all models on the COCO validation set [Lin et al., 2014], using two
primary metrics: FID [Heusel et al., 2017] and CLIP score [Radford et al., 2021, Hessel et al., 2021].
Specifically, we report FID-10K, where prompts are randomly sampled from the validation set. The
generated images for these prompts are then compared to reference images from validation set. In
addition to these, we evaluate our models on the GenEval [Ghosh et al., 2024] and DPG-Bench [Hu
et al., 2024] benchmarks, both of which are designed to measure text-image alignment. Finally, we
report Inception Score [Salimans et al., 2016] and Precision/Recall [Kynkäänniemi et al., 2019] as
secondary metrics to provide a more comprehensive evaluation.
Base Models. We implement our framework on five existing popular base models: stable-diffusionv1-5 (SD15) [Rombach et al., 2022], stable-diffusion-xl-v1.0-base (SDXL) [Rombach et al., 2022]
with UNet architecture, and SD-3.5 [Esser et al., 2024] with MM-DiT architecture, PixArt-α [Chen
et al., 2023] with DiT architecture.
w/o CFG

w CFG

Model

FID ↓

IS ↑

Pre. ↑

Rec. ↑

FID ↓

IS ↑

Pre. ↑

Rec. ↑

GIVT [Tschannen et al., 2025]
MAR-B [Li et al., 2024b]
LDM-4 [Rombach et al., 2022]
CausalFusion-L [Deng et al., 2024]

5.67
3.48
10.56
10.56

192.4
103.5
103.5

0.75
0.78
0.71
0.71

0.59
0.58
0.62
0.62

3.35
2.31
3.60
1.94

281.7
247.7
264.4

0.84
0.82
0.87
0.82

0.53
0.57
0.48
0.59

ADM [Dhariwal and Nichol, 2021]
DiT-XL [Peebles and Xie, 2023]
SiT-XL [Ma et al., 2024]
ViT-XL [Hang et al., 2023]
U-ViT-H/2 [Bao et al., 2023]
MaskDiT [Zheng et al., 2023]
RDM [Teng et al., 2023]
CausalFusion-XL [Deng et al., 2024]
DiT-XL/2 + LABridge (VE)
DiT-XL/2 + LABridge (VP)
DiT-XL/2 + LABridge (OU)
CausalFusion-XL + LABridge (VP)

10.94
9.62
8.3
8.10
6.58
5.69
5.27
3.61
5.74
4.95
3.83
3.49

121.5
178.0
153.4
180.9
152.1
164.4
179.2
182.4

0.69
0.67
0.74
0.75
0.75
0.70
0.76
0.76
0.77

0.63
0.67
0.60
0.62
0.66
0.64
0.67
0.65
0.68

4.59
2.27
2.06
2.06
2.29
2.28
1.99
1.77
2.13
1.95
1.84
1.69

186.7
278.2
270.3
263.9
276.6
260.4
282.3
279.9
285.4
289.3
291.3

0.82
0.83
0.82
0.82
0.80
0.81
0.82
0.84
0.86
0.87
0.86

0.52
0.57
0.59
0.57
0.61
0.58
0.61
0.57
0.60
0.62
0.63

Table 1: Benchmarking class-conditional image generation on ImageNet 256×256. The left half
is without CFG, the right half is with CFG. Best results are in bold, second best are underlined.
Learning from Scratch Experiments. We evaluated our method (three bridges) on the ImageNet
256×256 dataset with and without classifier-free guidance (CFG) and compared it with various stateof-the-art approaches (ImageNet 512x512 dataset results in Appendix C). The results are summarized
in Tab. 2. Without CFG: Our method, CausalFusion-XL + LABridge, achieves the best performance
among all evaluated models. Compared to the DiT-XL method, DiT-XL/2 + LABridge achieves
significant performance improvements across all metrics. This demonstrates the effectiveness of
our proposed LABridge in improving the quality and diversity of generated images. With CFG:
When CFG is applied, our method again outperforms others, achieving the best FID and the highest
IS, further highlighting its ability to generate high-quality and consistent text-conditioned images.
These results validate that our approach is generalizable across different backbones and significantly
enhances both image quality and text-image alignment.
Fine-Tuning Existing Pretrained Models. We conducted extensive experiments with various
pretrained models with similar baseline Liu et al. [2024]. The experimental results, summarized in
Tab. 2 and Fig. 6 (Appendix C.3), demonstrate that our method consistently outperforms baseline
approaches across multiple evaluation metrics and datasets. Notably, our approach excels on the
GenEval and DPG benchmarks, which are specifically designed to measure text-image consistency.
8

COCO-10K
MJHQ-30K
FID ↓
CLIP ↑
FID ↓
CLIP ↑
Stable Diffusion V1.5 Comparision
SD15-Base [Rombach et al., 2022]
15.81±.04 28.03±.08 13.54±.03 28.40±.01
SD15 + CF Liu et al. [2024]
14.83±.05 28.52±.07 12.62±.02 28.90±.03
SD15 + LABridge (VP, UNet)
13.82±.04 29.01±.06 11.63±.03 29.42±.02
Stable Diffusion XL Comparision
SDXL-Base [Rombach et al., 2022] 11.68±.04 28.83±.04 10.55±.01 29.63±.01
SDXL + CF (Liu et al. [2024])
12.69±.04 29.33±.03 9.59±.02 30.15±.03
SDXL + LABridge(VP, UNet)
12.72±.02 29.82±.01 8.55±.03 30.63±.01
Stable Diffusion 3.5 Medium DiT Comparision
9.88±.03 29.91±.04 8.45±.01 30.71±.03
SD3.5-M [Esser et al., 2024]
SD3.5-M + CF Liu et al. [2024]
8.89±.04 30.43±.02 7.28±.02 31.23±.03
SD3.5-M + LABridge(VP, DiT)
7.86±.05 30.92±.01 7.44±.03 31.72±.02
Stable Diffusion 3.5 Large DiT Comparision
SD3.5-L [Esser et al., 2024]
7.33±.03 30.88±.03 5.84±.02 31.41±.02
6.33±.04 31.36±.04 4.84±.03 31.89±.02
SD3.5-L + CF Liu et al. [2024]
SD3.5-L + LABridge(VP, DiT)
5.34±.03 31.87±.02 3.82±.02 32.39±.01
PixArt DiT Comparision
PixArt-α [Chen et al., 2023]
11.24 ±.02 29.52±.03 9.65±.02 30.01±.04
10.23±.03 30.02±.04 8.62±.03 30.53±.03
PixArt-α + CF Liu et al. [2024]
PixArt-α + LABridge (VP, DiT)
9.23±.02 30.51±.03 7.63±.04 31.01±.02
Method

Text-Alignment
GenEval ↑ DPG ↑
0.48±.04
0.54±.04
0.57±.04

70.64±.01
71.35±.01
72.42±.02

0.56±.03
0.62±.03
0.65±.03

75.52±.02
76.53±.02
77.21±.01

0.62±.03
0.65±.03
0.67±.03

83.31±.02
84.31±.02
85.31±.01

0.66±.03
0.68±.03
0.69±.03

84.52±.02
85.52±.02
85.28±.01

0.49±.03
0.51±.03
0.54±.03

75.42±.04
76.65±.04
77.76±.03

Table 2: Quantitative comparison of state-of-the-art models across various architectures and benchmarks for different metrics.

SDXL(
50NFEs
)
bl
ondewoma
ni
nka
r
a
t
euni
f
or
m
wi
t
hr
e
dt
oor
a
ngegr
a
di
e
ntc
i
r
c
l
e

SDXL+Cr
os
s
Fl
ow

5M(
50NFES) SD3.
5M+Cr
os
s
Fl
ow SD3.
SDXL+TI
AE(
UNe
t
) SD3.
5M +TI
AE(
Di
T)

Gr
oundTr
ut
h

Ahi
ghf
a
s
hi
onc
l
oa
ki
ns
of
twhi
t
e
,
i
t
se
dge
sc
ur
l
i
ngupwa
r
dsa
si
f
move
dbya
nuns
e
e
nf
or
c
e
.

Ac
l
os
e
upoff
r
e
s
hs
t
r
a
wbe
r
r
ya
nd
l
e
mons
l
i
c
e
sf
l
oa
t
i
ngi
ns
pa
r
kl
i
ng
wa
t
e
r
,s
ur
r
ounde
dbybubbl
e
s

ar
e
a
l
i
s
t
i
cphot
oofagr
e
e
kc
e
na
r
y
ofni
ght

Figure 6: Qualitative comparison of LABridge incroprate pretrained models against raw models.
Please zoom in to check details, lighting, and aesthetic performances. All methods that do not have
an NFE in place default to 50.
The superior performance on these benchmarks underscores the strong competitiveness of our method
in improving alignment between text and generated images. Moreover, to verify the generalizability
of TIAE, we tested TIAE in combination with various other community text-to-image plugins. The
results are shown in Fig. 7, demonstrating the robustness.
Ablation Studies. We conducted extensive ablation studies to evaluate the effectiveness of the
different loss functions used in the LABridge training method, aiming to validate its correctness. The
CLIP and FID metrics were tested on COCO-10k and MJHQ-30K, with the results summarized in Tab.
3. These results show that the training process combining all three loss functions consistently delivers
the best performance across most backbones and benchmark datasets. In contrast, using a single loss
9

Module
Lrec Lsem Lalign
✓
✓
✓
✓

✓

✓
✓

✓
✓
✓

✓
✓
✓
✓

✓

✓
✓

✓
✓
✓

COCO-10K
MJHQ-30K
FID ↓
GenEval ↑ DPG ↑
CLIP ↑
FID ↓
GenEval ↑ DPG ↑
Stable Diffusion V1.5 Ablation
29.01 ± .02 13.82 ± .04 0.57 ± .02 72.42 ± .03 29.42 ± .01 11.63 ± .05 0.57 ± .01 72.42 ± .02
28.90 ± .03 14.01 ± .04 0.55 ± .03 71.85 ± .04 29.25 ± .02 11.90 ± .04 0.55 ± .03 71.54 ± .04
28.56 ± .03 14.32 ± .05 0.54 ± .04 71.35 ± .05 28.97 ± .02 12.04 ± .04 0.54 ± .03 71.35 ± .04
28.12 ± .04 14.85 ± .06 0.53 ± .05 70.65 ± .06 28.45 ± .03 12.38 ± .06 0.53 ± .04 70.65 ± .05
27.89 ± .05 15.12 ± .07 0.52 ± .06 69.80 ± .07 27.83 ± .04 12.67 ± .05 0.52 ± .05 69.80 ± .06
27.45 ± .06 15.54 ± .08 0.51 ± .07 69.32 ± .08 27.32 ± .05 13.01 ± .07 0.51 ± .06 69.32 ± .07
Stable Diffusion XL Ablation
29.82 ± .03 12.82 ± .02 0.65 ± .03 77.21 ± .02 30.63 ± .03 8.55 ± .02 0.65 ± .02 77.21 ± .01
29.61 ± .04 13.09 ± .03 0.63 ± .04 76.75 ± .03 30.65 ± .04 8.85 ± .03 0.63 ± .04 76.70 ± .03
29.45 ± .04 12.79 ± .03 0.62 ± .05 76.53 ± .04 30.12 ± .04 8.98 ± .03 0.62 ± .04 76.53 ± .03
29.12 ± .05 13.42 ± .04 0.60 ± .06 75.90 ± .05 29.78 ± .05 9.27 ± .04 0.60 ± .05 75.90 ± .04
28.78 ± .06 13.83 ± .05 0.58 ± .07 75.30 ± .06 29.32 ± .06 9.64 ± .05 0.58 ± .06 75.30 ± .05
28.45 ± .07 14.21 ± .06 0.57 ± .08 74.74 ± .07 28.95 ± .07 10.02 ± .06 0.57 ± .07 74.74 ± .06
CLIP ↑

Table 3: A Evaluation of Performance Following the Ablation Studies of Lrec , Lsem , and Lalign
Modules in Stable Diffusion V1.5 and Stable Diffusion XL Models across COCO-10K and MJHQ30K Datasets.
or a combination of two losses yields slightly inferior results. This highlights the effectiveness of our
proposed LABridge approach.

6

Conclusion

We presented LABridge, a novel text-to-image generation framework designed to overcome semantic
instability and slow sampling inherent in diffusion models. By employing a TIAE to create structured,
text-conditioned prior aligned with image latents, and utilizing an OU diffusion bridge to connect
these representations, LABridge establishes explicit text-vision consistency. LABridge offers a robust
pathway towards efficient, high-quality conditional image synthesis.

References
Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their
Applications, 12(3):313–326, 1982.
Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth
words: A vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, pages 22669–22679, 2023.
Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik
Lorenz, Yam Levi, Zion English, Vikram Voleti, and Adam et al. Letts. Stable video diffusion:
Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023.
Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr,
Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh.
Video generation models as world simulators, 2024. URL https://openai.com/research/
video-generation-models-as-world-simulators.
Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim.
Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dataset,
2022.
Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James
Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for
photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023.
Chaorui Deng, Deyao Zh, Kunchang Li, Shi Guan, and Haoqi Fan. Causal diffusion transformers for
generative modeling. arXiv preprint arXiv:2412.12095, 2024.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A largescale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern
Recognition, pages 248–255. IEEE, 2009.
10

Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances
in neural information processing systems, 34:8780–8794, 2021.
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam
Levi, Dominik Lorenz, Axel Sauer, and Frederic et al. Boesel. Scaling rectified flow transformers
for high-resolution image synthesis. In Forty-first International Conference on Machine Learning,
2024.
Zach Evans, CJ Carr, Josiah Taylor, Scott H Hawley, and Jordi Pons. Fast timing-conditioned latent
audio diffusion. arXiv preprint arXiv:2402.04825, 2024.
Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework
for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36,
2024.
Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, and Baining
Guo. Efficient diffusion training via min-snr weighting strategy. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 7441–7451, 2023.
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A referencefree evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Isabelle
Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan,
and Roman Garnett, editors, Advances in Neural Information Processing Systems, volume 30,
pages 6626–6637, 2017.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in
Neural Information Processing Systems, volume 33, pages 6840–6851, 2020.
Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P
Kingma, Ben Poole, Mohammad Norouzi, and David J et al. Fleet. Imagen video: High definition
video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.
Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu Ella. Equip diffusion models
with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024.
Aapo Hyvärinen and Peter Dayan. Estimation of non-normalized statistical models by score matching.
Journal of Machine Learning Research, 6(4), 2005.
S Kang et al. Distilling diffusion models into conditional gans. ECCV, 2024.
Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved
precision and recall metric for assessing generative models. Advances in neural information
processing systems, 32, 2019.
Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.
5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint
arXiv:2402.17245, 2024a.
Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image
generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024b.
Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-lm
improves controllable text generation. Advances in Neural Information Processing Systems, 35:
4328–4343, 2022.
Shanchuan Lin, Anran Wang, and Xiao Yang. Sdxl-lightning: Progressive adversarial diffusion
distillation. arXiv preprint arXiv:2402.13929, 2024.
Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang. Diffusion adversarial
post-training for one-step video generation. arXiv preprint arXiv:2501.08316, 2025.
11

Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–
ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13, pages 740–755. Springer, 2014.
Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and
Mark D Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv
preprint arXiv:2301.12503, 2023.
Qihao Liu, Xi Yin, Alan Yuille, Andrew Brown, and Mannat Singh. Flowing from words to pixels: A
framework for cross-modality evolution. arXiv preprint arXiv:2412.15213, 2024.
Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and
Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant
transformers. arXiv preprint arXiv:2401.08740, 2024.
S Mei et al. Codi: Conditional diffusion distillation for higher-fidelity and faster image generation.
CVPR, 2024.
T Nguyen et al. Swiftbrush: One-step text-to-image diffusion model with variational score distillation.
CVPR, 2024.
William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of
the IEEE/CVF International Conference on Computer Vision, pages 4195–4205, 2023.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International conference on machine learning, pages
8748–8763. PMLR, 2021.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with CLIP latents. arXiv preprint arXiv:2204.06125, 2022.
Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao.
Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. arXiv preprint
arXiv:2404.13686, 2024.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. Advances in neural information processing systems, 29,
2016.
Huiyang Shao, Qianqian Xu, Peisong Wen, Peifeng Gao, Zhiyong Yang, and Qingming Huang.
Building bridge across the time: Disruption and restoration of murals in the wild. In Proceedings
of the IEEE/CVF International Conference on Computer Vision, pages 20259–20269, 2023.
Huiyang Shao, Xin Xia, Yuhong Yang, Yuxi Ren, Xing Wang, and Xuefeng Xiao. Rayflow: Instanceaware diffusion acceleration via adaptive flow trajectories. arXiv preprint arXiv:2503.07699,
2025.
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In International Conference on Machine Learning,
pages 2256–2265. PMLR, 2015.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
In Advances in Neural Information Processing Systems, volume 32, pages 11895–11907, 2019.
Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. In International
Conference on Learning Representations, 2021.
12

Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang.
Relay diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint
arXiv:2309.03350, 2023.
Michael Tschannen, Cian Eastwood, and Fabian Mentzer. Givt: Generative infinite-vocabulary
transformers. In European Conference on Computer Vision, pages 292–309. Springer, 2025.
Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661–1674, 2011.
Tong Wu, Zhihao Fan, Xiao Liu, Hai-Tao Zheng, Yeyun Gong, Jian Jiao, Juntao Li, Jian Guo, Nan
Duan, Weizhu Chen, et al. Ar-diffusion: Auto-regressive diffusion model for text generation.
Advances in Neural Information Processing Systems, 36:39957–39974, 2023.
Sirui Xie, Zhisheng Xiao, Diederik P Kingma, Tingbo Hou, Ying Nian Wu, Kevin Murphy, Tim
Salimans, Ben Poole, and Ruiqi Gao. Em distillation for one-step diffusion models. arXiv preprint
arXiv:2405.16852, 2024.
Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models
with masked transformers. arXiv preprint arXiv:2306.09305, 2023.
Linqi Zhou, Aaron Lou, Samar Khanna, and Stefano Ermon. Denoising diffusion bridge models.
arXiv preprint arXiv:2309.16948, 2023.

13

NeurIPS Paper Checklist
1. Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope?
Answer: [Yes]
Justification: The abstract and introduction accurately summarize the proposed LABridge framework, its components (TIAE, OU Bridge), the claimed benefits (improved alignment, accelerated
sampling), and the intention to provide theoretical backing, which are reflected in the subsequent
sections (Method, Theoretical Guarantees, Proof Details).
2. Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We provide limitation in Sec. F.
3. Theory assumptions and proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and a
complete (and correct) proof?
Answer: [Yes]
Justification: The paper includes dedicated sections for Theoretical Analysis (Sec.D) and Proof
Details (Sec. E). Assumptions are stated (Assum. D.1), and detailed proofs are provided for the
theorems and propositions presented (e.g., Thm. D.3, D.5, D.6, D.9, Propositions D.4, D.7, D.8).
4. Experimental result reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the
paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We describe the algorithms (Algo. 1, 2) and specific details (Sec. C) about the
experimental setup, such as datasets used, specific hyperparameters (learning rates, batch sizes,
parameters), training duration, evaluation metrics implementation, or baseline implementation
details necessary to fully reproduce the claimed experimental results.
5. Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instructions to
faithfully reproduce the main experimental results, as described in supplemental material?
Answer: [No]
Justification: We will provide the source code and data after the draft is completed.
6. Experimental setting/details
Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters,
how they were chosen, type of optimizer, etc.) necessary to understand the results?
Answer: [Yes]
Justification: Key details about the experimental setup are provided, including specific hyperparameter values (e.g., for TIAE loss weights wa , ws , wr , OU parameters θ, σ, learning rates, optimizer
types, batch sizes), data splits for training/testing on the mentioned benchmarks, and specific
schedules used.
7. Experiment statistical significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
Justification: We conducted extensive experiments to demonstrate the effectiveness of our method.
8. Experiments compute resources
14

Question: For each experiment, does the paper provide sufficient information on the computer
resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
Answer: [Yes]
Justification: We provide information about the computational resources used for the experiments.
9. Code of ethics
Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS
Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: This paper appears to be focused on algorithmic contributions and does not inherently
conflict with the NeurIPS Code of Ethics.
10. Broader impacts
Question: Does the paper discuss both potential positive societal impacts and negative societal
impacts of the work performed?
Answer: [No]
Justification: Our work primarily focuses on algorithm design and technical aspects, with limited
direct societal impact. As such, the paper does not extensively discuss broader societal implications,
positive or negative.
11. Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible release of
data or models that have a high risk for misuse (e.g., pretrained language models, image generators,
or scraped datasets)?
Answer: [NA]
Justification: This question is not applicable.
12. Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper,
properly credited and are the license and terms of use explicitly mentioned and properly respected?
Answer: [Yes]
Justification: We cites prior work for components like pretrained VAEs (Stable Diffusion), text
encoders (CLIP), and architectural inspiration (MMDiT).
13. New assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: Documentation requirements for new assets are not applicable.
14. Crowdsourcing and research with human subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as well as
details about compensation (if any)?
Answer: [NA]
Justification: The research presented in the paper does not involve crowdsourcing experiments or
research with human subjects.
15. Institutional review board (IRB) approvals or equivalent for research with human subjects
Question: Does the paper describe potential risks incurred by study participants, whether such
risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals
(or an equivalent approval/review based on the requirements of your country or institution) were
obtained?
Answer: [NA]
15

Justification: The research presented does not involve human subjects, so IRB approval is not
applicable.
16. Declaration of LLM usage
Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard
component of the core methods in this research? Note that if the LLM is used only for writing,
editing, or formatting purposes and does not impact the core methodology, scientific rigorousness,
or originality of the research, declaration is not required.
Answer: [NA]
Justification: Large language models (or derived text encoders like CLIP/T5) are used to obtain
text embeddings for conditioning, which is standard practice in text-to-image generation. LLMs
are not an important, original, or non-standard component of the core novel methodology presented
in this paper.

16

