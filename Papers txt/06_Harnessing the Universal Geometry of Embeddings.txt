Harnessing the Universal Geometry of Embeddings

arXiv:2505.12540v3 [cs.LG] 25 Jun 2025

Rishi Jha

Collin Zhang Vitaly Shmatikov John X. Morris
Department of Computer Science
Cornell University

Abstract
We introduce the first method for translating text embeddings from one vector
space to another without any paired data, encoders, or predefined sets of matches.
Our unsupervised approach translates any embedding to and from a universal latent
representation (i.e., a universal semantic structure conjectured by the Platonic
Representation Hypothesis). Our translations achieve high cosine similarity across
model pairs with different architectures, parameter counts, and training datasets.
The ability to translate unknown embeddings into a different space while preserving
their geometry has serious implications for security. An adversary with access
to a database of only embedding vectors can extract sensitive information about
underlying documents, sufficient for classification and attribute inference.

Figure 1: Left: input embeddings from different model families (T5-based GTR [46] and BERT-based
GTE [32]) are fundamentally incomparable. Right: given unpaired embedding samples from different
models on different texts, our model learns a latent representation where they are closely aligned.

1

Introduction

Text embeddings are the backbone of modern NLP, powering tasks like retrieval, RAG, classification,
and clustering. There are many embedding models trained on different datasets, data shufflings, and
initializations. An embedding of a text encodes its semantics: a good model maps texts with similar
semantics to vectors close to each other in the embedding space. Since semantics is a property of
text, different embeddings of the same text should encode the same semantics. In practice, however,
different models encode texts into completely different and incompatible vector spaces.
The Platonic Representation Hypothesis [18] conjectures that all image models of sufficient size
converge to the same latent representation. We propose a stronger, constructive version of this
hypothesis for text models: the universal latent structure of text representations can be learned and,
furthermore, harnessed to translate representations from one space to another without any paired data
or encoders.
Preprint. Under review.

Documents (D)
Subject: Enron
Bashing on Frontline
Body:
Karen, please call
me when you
receive this email.
Thank you. Rick

Embeddings of D
(Encoder A)

Embeddings of W
(Encoder B)

Encoder A

Secret

Encoder B

Wool
Global wool production
is about 2 million
tonnes per year, of
which 60% goes into
apparel. Wool
comprises ca

Public

Some emails
discussing NROn
Employee/s
Complaint To
thePublic (RS-ES
statement of
late/commissary
speaker ﬁrm (...)
Documents (D)
(Recovered)

Documents (W)

B1

A2
T

A1

B2

Embeddings of D
(Encoder B)

Figure 2: Given only a vector database from an unknown model, vec2vec translates the database
into the space of a known model using latent structure alone. Converted embeddings reveal sensitive
information about the original documents, such as the topic of an email (pictured, real example).
In this work, we show that the Strong Platonic Representation Hypothesis holds in practice. Given
unpaired examples of embeddings from two models with different architectures and training data, our
method learns a latent representation in which the embeddings are almost identical (Figure 1).
We draw inspiration from research on aligning word embeddings across languages [61, 10, 15, 9]
and unsupervised image translation [35, 68]. Our vec2vec method uses adversarial losses and cycle
consistency to learn to encode embeddings into a shared latent space and decode with minimal loss.
This makes unsupervised translation possible. We use a basic adversarial approach with vector space
preservation [45] to learn a mapping from an unknown embedding distribution to a known one.
vec2vec is the first method to successfully translate embeddings from the space of one model to
another without paired data.1 vec2vec translations achieve cosine similarity as high as 0.92 to the
ground-truth vectors in their target embedding spaces and perfect matching on over 8000 shuffled
embeddings (without access to the set of possible matches in advance).
To show that our translations preserve not only the relative geometry of embeddings but also the
semantics of underlying inputs, we extract information from them using zero-shot attribute inference
and inversion, without any knowledge of the model that produced the original embeddings.2

2

Problem formulation: unsupervised embedding translation

Consider a collection of embedding vectors {u1 , . . . un }, for example, a dump of a compromised
vector database, where each ui = M1 (di ) is generated by an unknown encoder M1 : Vs → RdM1
from an unknown document di . We cannot make queries to M1 and do not know its training data,
nor architectural details. Our goal is to extract any information about the documents di .
We do assume access to a different encoder M2 that we can query at will to generate new embeddings
in some other space. We also assume high-level distributional knowledge about the hidden documents: their modality (text) and language (e.g., English). To extract information, we may translate
{u1 , . . . un } into the output space of M2 and apply techniques like inversion that take advantage of
the encoder.
Limitations of correspondence methods. There is significant prior research on the problem of
matching or correspondence between sets of embedding vectors [1, 48, 8, 53]. These methods
typically assume that the two (or more) sets of embeddings are generated by different encoders on
the same or highly-overlapping inputs. In other words, for each unknown vector, there must already
exist a set of candidate vectors in a different embedding. In practice, it is unrealistic to expect that
such a database be available, so these methods are not directly applicable. Some matching methods,
1

Prior work has successfully translated word embeddings between languages, typically relying on overlapping
vocabularies across languages. In contrast, we translate embeddings of entire sequences between model spaces.
2
Our code is available on GitHub.

2

Known Embedding Space

Unknown Embedding Space

Document di
Subject: Enron Bashing
on Frontline
Body:
Karen, please call me
when you receive this
email. Thank you. Rick

F(ui)
M1

ui

ve
vec2

c

vi

Visible to vec2vec

F(ui) ≈ vi

Invisible to vec2vec

Figure 3: Unsupervised embedding translation. With access to only ui = M1 (di ), vec2vec
seeks to generate a translation F (ui ) that is close in M2 ’s embedding space to the ideal embedding
vi = M2 (di ) without access to di , vi , or M1 .
however, support translation between embedding spaces without overlapping inputs. Our experiments
demonstrate that these methods struggle significantly, even when correspondence exists.
Our task is inherently more challenging than matching, because we do not assume access to encoder
M1 , nor do we have additional representations of documents d1 , . . . , dn beyond their embeddings
ui = M1 (di ). Therefore, we rely solely on unsupervised translation from M1 to M2 . The effectiveness of such unsupervised translation approaches thus critically depends on identifying and leveraging
shared geometric structures within the embedding spaces produced by M1 and M2 .
The Strong Platonic Representation Hypothesis. Our hope that unsupervised embedding translation
is possible at all rests on the stronger version of the Platonic Representation Hypothesis [18]. Our
conjecture is as follows: neural networks trained with the same objective and modality, but with
different data and model architectures, converge to a universal latent space such that a translation
between their respective representations can learned without any pairwise correspondence.
Translation enables information extraction. Solving unsupervised translation will allow us to
use information extraction tools designed to operate on vectors produced by known encoders. For
example, we could apply inversion models [42, 66] to recover unknown documents {di }.

3

Our method: vec2vec

Unsupervised translation has been successful in computer vision, using a combination of cycle
consistency and adversarial regularization [35, 68]. Our design of vec2vec is inspired in part by
these methods. We aim to learn embedding-space translations that are cycle-consistent (mapping to
and from an embedding space should end in the same place) and indistinguishable (embeddings for
the same text from either space should have identical latents).
3.1

Architecture

We propose a modular architecture, where embeddings are encoded and decoded using space-specific
adapter modules and passed through a shared backbone network. Figure 2 shows these components.
Input adapters A1 : Rd → RZ and A2 : Rd → RZ transform embeddings from each encoder-specific
space into a universal latent representation of dimension Z. The shared backbone T : RZ → RZ
extracts a common latent embedding from adapted inputs. Output adapters B1 : RZ → Rd and
B2 : RZ → Rd translate these common latent embeddings back into the encoder-specific spaces.
Thus, translation functions F1 , F2 and additional reconstruction mappings R1 , R2 are defined as:
F1 = B 2 ◦ T ◦ A1 ,

F2 = B1 ◦ T ◦ A2

R 1 = B 1 ◦ T ◦ A1

R2 = B2 ◦ T ◦ A2

Parameters of all components are collectively denoted θ = {A1 , A2 , T, B1 , B2 }.
Unlike images, embeddings do not have any spatial bias. Instead of CNNs, we use multilayer perceptrons (MLP) with residual connections, layer normalization, and SiLU nonlinearities. Discriminators
mirror this structure but omit residual connections to simplify adversarial learning.
3

3.2

Optimization

In addition to the ‘generator’ networks F and R, we introduce discriminators operating on both the
latent representations of F (D1ℓ , D2ℓ ) and the output embeddings (D1 , D2 ).
Our goal is to train the parameters of θ by solving:
θ∗ = arg min
θ

max

D1 ,D2 ,D1ℓ ,D2ℓ

Ladv (F1 , F2 , D1 , D2 , D1ℓ , D2ℓ ) + λgen Lgen (θ),

(1)

where Ladv and Lgen represent adversarial and generator-specific constraints respectively and hyperparameter λgen controls their tradeoff.
Adversarial loss. The adversarial loss encourages generated embeddings to match the empirical
distributions of original embeddings both at the embedding and latent levels. Specifically, applying
the standard GAN loss formulation [13] to both levels yields:
Ladv (F1 , F2 , D1 , D2 , D1ℓ , D2ℓ ) = LGAN (D1 , F1 ) + LGAN (D2 , F2 )
+ LGAN (D1ℓ , T ◦ A1 ) + LGAN (D2ℓ , T ◦ A2 ).
Generator. Because adversarial losses alone do not guarantee that translated embeddings preserve
semantics [68], we introduce three additional constraints to help the generator learn a useful mapping:
Reconstruction enforces that an embedding, when mapped into the latent space and back into its
original embedding space, closely matches its initial representation:
Lrec (R1 , R2 ) = Ex∼p ∥R1 (x) − x∥22 + Ey∼q ∥R2 (y) − y∥22 .
where p and q are distributions of embeddings sampled from M1 and M2 , respectively.
Cycle-consistency acts as an unsupervised proxy for supervised pair alignment, ensuring that F and
G can translate an embedding to the other embedding space and back again with minimal corruption:
LCC (F1 , F2 ) = Ex∼p ∥F2 (F1 (x)) − x∥22 + Ey∼q ∥F1 (F2 (y)) − y∥22 .
Vector space preservation (VSP) ensures that pairwise relationships between generated embeddings
remain consistent under translation [45, 64]. Given a batch of B embeddings x1 , ..., xB and y1 , ..., yB ,
we sum their average pairwise distances after translation by both F1 and F2 :
B B 
1 XX
LVSP (F1 , F2 ) =
∥M1 (xi ) · M1 (xj ) − F1 (M1 (xi )) · F1 (M1 (xj ))∥22
B i=1 j=1

2
+ ∥M2 (yi ) · M2 (yj ) − F2 (M2 (yi )) · F2 (M2 (yj ))∥2
Combining these losses yields: Lgen (θ) = λrec Lrec (R1 , R2 ) + λCC LCC (F1 , F2 ) + λVSP LVSP (F1 , F2 ),
where hyperparameters λCC , λrec , and λVSP control relative importance.

4
4.1

Experimental setup
Preliminaries

Datasets. We use the Natural Questions (NQ) [25] dataset of user queries and Wikipedia-sourced
answers for training (a 2-million subset) and evaluation (a 65536 subset). To evaluate information
extraction, we use TweetTopic [2], a dataset of tweets multi-labeled by 19 topics; a random 8192record subset of Pseudo Re-identified MIMIC-III (MIMIC) [28], a pseudo re-identified version of the
MIMIC dataset [19] of patient records multi-labeled by 2673 MedCAT [24] disease descriptions; and
a random 50-email subset of the Enron Email Corpus (Enron) [21], an unlabeled, public dataset of
internal emails of a defunct energy company.
Models. Table 1 lists the embedding models representing three size categories, four transformer
backbones, and two output dimensionalities. Granite is multilingual, CLIP is multimodal.
4

Model
[46]
[49]
[57]
[32]
[67]
[14]

gtr
clip
e5
gte
stella
granite

Params (M)

Backbone

Year

Dims

Max Seq.

110
151
109
109
109
278

T5
CLIP
BERT
BERT
BERT
RoBERTa

2021
2021
2022
2023
2023
2024

768
512
768
768
768
768

512
77
512
512
512
512

Table 1: Embedding models used in our experiments.

Training. Unless otherwise specified, each vec2vec is trained on two sets of embeddings generated
from disjoint sets of 1 million 64-token sequences sampled from NQ (see Appendix D for experiments
with fewer embeddings). Due to GAN instability [52], we select the best of multiple initializations
and leave more robust training to future work.
4.2

Evaluating translation

Let ui = M1 (di ) and vi = M2 (di ) denote the source and target embeddings of the same input di .
The goal of translation is to generate a vector that is as close to vi as possible. We say that (ui , vj ) are

“aligned” by the translator F if vj is the closest embedding to F
(ui ): j = arg mink cos F (ui ), vk .

A perfect translator F ∗ satisfies i = arg mink cos F ∗ (ui ), vk for all i.
Given (unknown) embeddings {M2 (dj )}nj=0 ordered by decreasing cosine similarity to F (ui ), let ri
be the rank of the correct embedding vi = M2 (di ). To measure quality of F , we use three metrics.
Mean Cosine Similarity measures how close translations are, on average, to their targets. Top-1
Accuracy is the fraction of translations whose target is closer than any other embedding. Mean
Rank is the average rank of targets with respect to translations. The ideal translator F ∗ achieves
mean similarity of 1.0, top-1 accuracy of 1.0, and mean rank of 1.0. Recall that a random alignment
corresponds to a mean rank of n2 . Formally,
n

cos(ui , vi ) =


1 X
1 − cos F (ui ), vi
n i=1

n

Top-1(r) =

1X
1{ri = 1}
n i=1

n

Rank(r) =

1X
ri
n i=1

vec2vec is the first unsupervised embedding translator, thus there is no direct baseline. As our Naïve
baseline, we simply use F (x) = x to measure geometric similarity between embedding spaces. The
second (pseudo)baseline is Oracle-aided optimal transport. It assumes that candidate targets are
known and is thus
easier than vec2vec and the Naïve baseline. We solve optimal assignment,
Pstrictly
n
π ∗ = arg minπ i=1 cos(ui , vπ(i) ), via either the Hungarian, Earth Mover’s Distance, Sinkhorn, or
Gromov-Wasserstein algorithms, choosing the solver with the lowest rank for each experiment.
4.3

Evaluating information extraction

We measure whether translation preserves semantics via attribute inference: for each translated
embedding F (M1 (di )), our goal is to infer attributes ci ⊆ C of di .
The first method we use is zero-shot embedding attribute inference: calculate pairwise cosine
similarities between F (M1 (di )) and the embeddings of all attributes P
in C, identify
top k closest

n
attributes, and measure whether they are correct via top-k accuracy: n1 i=0 1 |cki ∩ ci | ≥ 1 .
The second method is embedding inversion that recovers text inputs from embeddings. Since [42]
requires a pre-trained inversion model for each embedding space, we use [66] instead to generate an
approximation d′i of di from F (M1 (di )) in a zero-shot manner. We measure the extracted information
using LLM judge accuracy: the fraction of translated embeddings for which GPT-4o determines that
d′ reveals information in d. See Appendix E for our prompt.
In addition to the Naïve baseline, we also consider an Oracle attribute inference: zero-shot
classification with the correct embedding M2 (d) and class labels M2 (C).
5

0.03 0.56

stella gtr

0.03 -0.02 0.03

0.00

0.38 0.01 0.56 0.00
e5 granite gte
M2

gtr stella

0.88

0.90 0.75 0.92

0.85 0.90

0.91 0.96

0.75 0.75 0.91

0.90

0.82 0.92 0.96 0.90
e5 granite gte
M2

gte granite e5

0.68 0.01

Similarity of Latents
0.88 0.85 0.75 0.82

stella gtr

0.01 -0.02 0.01

gte granite e5

0.02

stella gtr

M1
gte granite e5

Similarity of Inputs
0.02 0.68 0.03 0.38

gtr stella

Difference in Similarities
0.86 0.17 0.72 0.45
0.86

0.89 0.77 0.91

0.17 0.89

0.87 0.40

0.72 0.77 0.87

0.90

0.45 0.91 0.40 0.90
e5 granite gte
M2

gtr stella

0.8
0.6
0.4
0.2
0.0

Figure 4: Pairwise cosine similarities of input embeddings (left) and their vec2vec latents (middle)
across different embedding pairs. The absolute difference between the heatmaps plots is on the right.
All numbers are computed on the same batch of 1024 NQ texts.

5

vec2vec learns to translate embeddings without any paired data

We first show that vec2vec learns a universal latent space, then demonstrate that this space preserves
the geometry of all embeddings. Therefore, we can use it like a universal language of text encoders
to translate their representations without any paired data.
vec2vec learns a universal latent space. vec2vec projects embeddings M1,2,... into a shared
latent space via compositions of input adapters (A1,2,... ) and a shared translator T . Figure 4 shows
that even when the embeddings ui = M1 (di ) and vi = M2 (di ) are far apart (i.e., have low cosine
similarity), their representations in vec2vec’s latent space are incredibly close: T (A1 (ui )) ≈
T (A2 (vi )). Figure 1 visualizes this (via two-dimensional projections) for vec2vec trained on gte
and gtr embeddings: the embeddings are far apart, but their latents are nearly overlapping.

Naïve Baseline

vec2vec

OT Baseline

M1

M2

cos(·) ↑

T-1 ↑

Rank ↓

cos(·) ↑

T-1 ↑

Rank ↓

cos(·) ↑

T-1 ↑

Rank ↓

gra.

gtr
gte
ste.
e5

0.80 (0.0)
0.87 (0.0)
0.79 (0.0)
0.85 (0.0)

0.99
0.95
0.98
0.98

1.19 (0.1)
1.18 (0.0)
1.05 (0.0)
1.11 (0.0)

-0.03 (0.0)
0.01 (0.0)
0.01 (0.0)
0.02 (0.0)

0.00
0.00
0.00
0.00

4168.73 (9.2)
4088.58 (9.2)
4208.26 (9.2)
4111.60 (9.2)

0.50 (0.0)
0.85 (0.0)
0.45 (0.0)
0.68 (0.0)

0.00
0.00
0.00
0.00

4094.22 (9.2)‡
4069.91 (9.2)†
4096.35 (9.2)‡
4096.17 (9.2)‡

gtr

gra.
gte
ste.
e5

0.81 (0.0)
0.87 (0.0)
0.80 (0.0)
0.83 (0.0)

0.99
0.93
0.99
0.84

1.02 (0.0)
2.31 (0.1)
1.03 (0.0)
2.88 (0.2)

-0.03 (0.0)
0.04 (0.0)
0.00 (0.0)
0.03 (0.0)

0.00
0.00
0.00
0.00

4169.76 (9.2)
4080.92 (9.2)
4198.78 (9.2)
4082.84 (9.2)

0.50 (0.0)
0.85 (0.0)
0.46 (0.0)
0.83 (0.0)

0.00
0.00
0.00
0.00

4093.55 (9.2)‡
4079.92 (9.2)†
4093.85 (9.2)‡
4066.42 (9.2)†

gte

gra.
gtr
ste.
e5

0.75 (0.0)
0.75 (0.0)
0.89 (0.0)
0.87 (0.0)

0.95
0.91
1.00
0.99

1.22 (0.0)
2.64 (0.1)
1.00 (0.0)
5.19 (0.5)

0.01 (0.0)
0.04 (0.0)
0.56 (0.0)
0.68 (0.0)

0.00
0.00
1.00
1.00

4079.81 (9.3)
4084.15 (9.2)
1.00 (0.0)
1.00 (0.0)

0.69 (0.0)
0.70 (0.0)
0.69 (0.0)
0.83 (0.0)

0.00
0.00
1.00
1.00

4069.23 (9.2)†
4078.45 (9.2)†
1.00 (0.0)†
1.00 (0.0)†

ste.

gra.
gtr
gte
e5

0.80 (0.0)
0.82 (0.0)
0.92 (0.0)
0.86 (0.0)

0.98
1.00
1.00
1.00

1.08 (0.0)
1.10 (0.0)
1.00 (0.0)
1.00 (0.0)

0.01 (0.0)
0.00 (0.0)
0.56 (0.0)
0.38 (0.0)

0.00
0.00
1.00
0.99

4209.08 (9.3)
4192.31 (9.2)
1.00 (0.0)
1.03 (0.0)

0.48 (0.0)
0.50 (0.0)
0.86 (0.0)
0.83 (0.0)

0.00
0.00
1.00
1.00

4096.38 (9.2)‡
4095.46 (9.2)‡
1.00 (0.0)†
1.00 (0.0)†

e5

gra.
gtr
gte
ste.

0.81 (0.0)
0.74 (0.0)
0.90 (0.0)
0.78 (0.0)

0.99
0.82
1.00
1.00

2.20 (0.2)
2.56 (0.0)
1.01 (0.0)
1.00 (0.0)

0.02 (0.0)
0.03 (0.0)
0.68 (0.0)
0.38 (0.0)

0.00
0.00
1.00
1.00

4120.60 (9.3)
4080.76 (9.3)
1.00 (0.0)
1.00 (0.0)

0.69 (0.0)
0.70 (0.0)
0.86 (0.0)
0.68 (0.0)

0.00
0.00
1.00
1.00

4096.12 (9.2)‡
4065.74 (9.2)†
1.00 (0.0)†
1.00 (0.0)†

Table 2: In-distribution translations: vec2vecs trained on NQ and evaluated on a 65536 text subset of
NQ (chunked in batches of size 8192). The rank metric varies from 1 to 8192, thus 4096 corresponds
to a random ordering. Standard errors are shown in parentheses. Bold denotes best value. Symbols
denote the lowest-rank solver for specific experiments: Sinkhorn† and Gromov-Wasserstein‡ .

6

TweetTopic

MIMIC

M1

M2

cos(·) ↑

Top-1 ↑

Rank ↓

cos(·) ↑

Top-1 ↑

Rank ↓

gran.

gtr
gte
stel.
e5

0.74 (0.0)
0.85 (0.0)
0.77 (0.0)
0.83 (0.0)

0.99
0.95
0.96
0.87

1.09 (0.1)
1.26 (0.1)
1.11 (0.0)
3.10 (0.7)

0.74 (0.0)
0.85 (0.0)
0.72 (0.0)
0.84 (0.0)

0.60
0.08
0.13
0.12

23.38 (1.6)
346.21 (7.8)
242.23 (6.1)
361.06 (8.7)

gtr

gran.
gte
stel.
e5

0.79 (0.0)
0.85 (0.0)
0.77 (0.0)
0.80 (0.0)

0.98
0.96
0.96
0.53

2.41 (0.6)
1.29 (0.2)
1.10 (0.0)
13.38 (1.2)

0.78 (0.0)
0.84 (0.0)
0.72 (0.0)
0.82 (0.0)

0.51
0.12
0.27
0.01

35.27 (1.9)
279.56 (6.9)
127.92 (4.4)
1413.80 (18.3)

gte

gran.
gtr
stel.
e5

0.73 (0.0)
0.71 (0.0)
0.86 (0.0)
0.83 (0.0)

0.94
0.95
1.00
0.91

1.33 (0.1)
1.29 (0.1)
1.00 (0.0)
1.57 (0.2)

0.73 (0.0)
0.69 (0.0)
0.85 (0.0)
0.86 (0.0)

0.09
0.12
1.00
0.54

342.15 (7.8)
256.63 (6.4)
1.00 (0.0)
17.71 (0.9)

stel.

gran.
gtr
gte
e5

0.79 (0.0)
0.77 (0.0)
0.90 (0.0)
0.85 (0.0)

0.99
1.00
1.00
0.98

1.09 (0.1)
1.00 (0.0)
1.00 (0.0)
1.05 (0.0)

0.77 (0.0)
0.75 (0.0)
0.91 (0.0)
0.85 (0.0)

0.14
0.56
1.00
0.51

221.95 (5.9)
17.70 (1.0)
1.00 (0.0)
26.33 (1.2)

e5

gran.
gtr
gte
stel.

0.79 (0.0)
0.67 (0.0)
0.87 (0.0)
0.75 (0.0)

0.98
0.80
0.99
0.98

1.08 (0.0)
3.10 (0.6)
1.02 (0.0)
1.06 (0.0)

0.78 (0.0)
0.66 (0.0)
0.87 (0.0)
0.75 (0.0)

0.21
0.01
0.60
0.46

151.09 (4.6)
1029.64 (14.9)
32.59 (2.6)
32.12 (1.4)

Table 3: Out-of-distribution translations: vec2vecs trained on NQ and evaluated on the entire
TweetTopic test set (800 tweets) and an 8192-record subset of MIMIC. The rank metric varies from 1
to 800 (for TweetTopic) and 8192 (for MIMIC), thus 400 and, respectively, 4096 correspond to a
random ordering. Standard errors are shown in parentheses.

vec2vec translations mirror target geometry. Table 2 shows that vec2vec generates embeddings
with near-optimal assignment across model pairs, achieving cosine similarity scores up to 0.92, top-1
accuracies up to 100%, and ranks as low as 1. In same-backbone pairings (e.g., (gte, e5)), vec2vec’s
top-1 accuracy and rank are comparable to both the naïve baseline and (surprisingly) the oracle-aided
optimal transport. Although the embeddings generated by vec2vec are significantly closer to the
ground truth than the naïve baseline, in same-backbone pairings the embeddings are close enough to
be compatible. In cross-backbone pairings, vec2vec is far superior on all metrics, while baseline
methods perform similarly to random guessing.
Table 3 shows that this performance extends to out-of-distribution data. Our vec2vec translators
were trained on NQ (drawn from Wikipedia), yet exhibit high cosine similarity, high accuracy, and
low rank when evaluated on tweets (which are far more colloquial and use emojis) and medical
records (which contain domain-specific jargon unlikely to appear in NQ). In Appendix C, we show
that baseline methods fail on cross-backbone embedding pairs.

OT Baseline

vec2vec
M2

cos(·) ↑

T-1 ↑

Rank ↓

cos(·) ↑

T-1 ↑

Rank ↓

gra.
gtr
gte
ste.
e5

clip

0.78 (0.0)
0.73 (0.0)
0.62 (0.0)
0.77 (0.0)
0.64 (0.0)

0.35
0.13
0.00
0.31
0.01

226.62 (3.2)
711.23 (5.9)
3233.41 (9.8)
286.69 (3.6)
2568.21 (9.4)

0.60 (0.0)
0.59 (0.0)
0.59 (0.0)
0.59 (0.0)
0.60 (0.0)

0.00
0.00
0.00
0.00
0.00

4096.00 (9.2)
4096.78 (9.2)
4096.16 (9.2)
4096.76 (9.2)
4096.41 (9.2)

clip

gra.
gtr
gte
ste.
e5

0.74 (0.0)
0.67 (0.0)
0.75 (0.0)
0.72 (0.0)
0.73 (0.0)

0.72
0.27
0.00
0.61
0.01

4.46 (0.1)
155.11 (2.1)
2678.90 (8.9)
22.50 (0.5)
1692.28 (8.2)

0.48 (0.0)
0.49 (0.0)
0.72 (0.0)
0.45 (0.0)
0.68 (0.0)

0.00
0.00
0.00
0.00
0.00

4096.86 (9.2)
4096.35 (9.2)
4096.44 (9.2)
4096.27 (9.2)
4096.42 (9.2)

M1

Table 4: Translations between unimodal and multimodal (CLIP) embeddings: vec2vecs trained on
NQ and evaluated on a 65536 text subset of NQ (chunked in batches of size 8192). Rank varies from
1 to 8192, thus 4096 corresponds to a random ordering. Since the embedding dimensionalities are
different, only the Gromov-Wasserstein OT baseline is run and the naive baseline does not apply.
Bold denotes best value.

7

TweetTopic (k = 1)

MIMIC (k = 10)

M1

M2

vec2vec

Naïve

M1

M2

vec2vec

Naïve

M1

M2

gran.

gtr
gte
stel.
e5

0.25
0.32
0.24
0.31

0.10
0.09
0.10
0.18

0.30
0.30
0.30
0.30

0.24
0.34
0.28
0.31

0.19
0.36
0.27
0.19

0.11
0.13
0.04
0.20

0.76
0.76
0.76
0.76

0.88
1.00
0.96
0.97

gtr

gran.
gte
stel.
e5

0.34
0.33
0.30
0.30

0.08
0.13
0.10
0.04

0.24
0.24
0.24
0.24

0.30
0.34
0.28
0.31

0.16
0.28
0.25
0.09

0.12
0.05
0.07
0.09

0.88
0.88
0.88
0.88

0.76
1.00
0.96
0.97

gte

gran.
gtr
stel.
e5

0.37
0.24
0.31
0.37

0.04
0.13
0.20
0.30

0.34
0.34
0.34
0.34

0.30
0.24
0.28
0.31

0.18
0.10
0.68
0.37

0.11
0.03
0.83
0.63

1.00
1.00
1.00
1.00

0.76
0.88
0.96
0.97

stel.

gran.
gtr
gte
e5

0.35
0.26
0.38
0.35

0.07
0.13
0.36
0.34

0.28
0.28
0.28
0.28

0.30
0.24
0.34
0.31

0.23
0.22
0.90
0.38

0.09
0.09
0.98
0.46

0.96
0.96
0.96
0.96

0.76
0.88
1.00
0.97

e5

gran.
gtr
gte
stel.

0.33
0.26
0.34
0.26

0.15
0.22
0.28
0.16

0.31
0.31
0.31
0.31

0.30
0.24
0.34
0.28

0.14
0.11
0.47
0.36

0.07
0.04
0.66
0.40

0.97
0.97
0.97
0.97

0.76
0.88
1.00
0.96

Table 5: Information leakage via top-k zero-shot attribute inference: vec2vecs trained on NQ and
evaluated on the TweetTopic test set (800 tweets) and an 8192-record subset of MIMIC. M1 and M2
represent ideal zero-shot inference: attributes and embeddings are encoded using the same model.

Finally, Table 4 shows that vec2vec can even translate to and from the space of CLIP, a multimodal
embedding model which was trained in part on image data. While the translations are not as strong as
in Table 2, vec2vec consistently outperforms the optimal transport baseline. These results show the
promise of our method at adapting to new modalities: in particular, the embedding space of CLIP has
been successfully connected to other modalities such as heatmaps, audio, and depth charts [12].

6

Using vec2vec translations to extract information

In this section, we show that vec2vec translations not only preserve the geometric structure of
embeddings but also retain sufficient semantics to enable attribute inference.

vec2vec translations even work for embeddings of medical records, which are much further from the training
distribution than tweets. The attributes in this case are
MedCAT disease descriptions, very few of which occur
in the training data. Attribute inference on translated
embeddings is comparable to the naïve baseline in samebackbone pairings and outperforms it (often greatly) in
cross-backbone pairings. The fact that vec2vec preserves the semantics of concepts like "alveolar periostitis" (which never appears in its training data) is evidence
that its latent space is indeed a universal representation.

M1
gte granite e5

Percentage of Emails With Leakage 80
49 52 28 62

stella gtr

Zero-shot attribute inference. Table 5 shows that
attribute inference on vec2vec translations consistently
outperforms the naïve baseline and often does better than
the ideal zero-shot baseline which performs inference
on ground-truth document and attribute embeddings in
the same space (this baseline is imaginary since these
embeddings are not available in our setting).

70

55

56

58

40

20

60

46

56

52

66

e5 granite gte
M2

56

56

60

34

80

50

55

40

40
gtr stella

30
20

Figure 5: Leakage of information via inversion. Trained on NQ and evaluated on
a 50-email subset of the Enron Email Corpus. Cells denote judge accuracy.

Zero-shot inversion. Inversion, i.e., reconstruction of
text inputs, is more ambitious than attribute inference. vec2vec translations retain enough semantic
information that off-the-shelf, zero-shot inversion methods like [66], developed for embeddings
8

computed by standard encoders, extract information for as many as 80% of documents given only
their translated embeddings, for some model pairs (Figure 5). These inversions are imperfect and we
leave development of specialized inverters for translated embeddings to future work. Nevertheless, as
exemplified in Figure 6, they still extract information such as individual and company names, dates,
promotions, financial information, outages, and even lunch orders. In Appendix E, we show the
prompt we use to measure extraction.
Ground Truth: “Subject: Enron Bashing on Frontline \n Body:..."
Generation: “Some emails discussing NROn Employee/s Complaint To thePublic ..."
Ground Truth: “Subject: Trades for 3/1/02 \n Body: \n John , \n The following trades..."
Generation: “... future transactions may await John G..."
Ground Truth: “ The following expense report is ready for approval..."
Generation: “ The upcoming expense statement from YYYY MM Dec..."
Figure 6: Examples of inversions that infer entities and content .

7

Related work

Representation alignment. Similarities between representations of different neural networks are
investigated in [26, 31, 58, 5, 18, 60, 30]. Methods based on CCA [41], SVCCA, [50], CKA [23, 37],
ICA [62], time-series [38], and GUIs [16] have been used to compare embeddings from different
subspaces. [36, 44, 39, 56, 47] harness representation similarity for zero-shot stitching, substitution,
domain transfer, and multi-modal adaptation. All rely on some amount of paired data, which is
difficult to reduce [6]. Our method does not just measure similarity, we learn how to translate
representations across spaces without any paired data.
Optimal transport. The problem of unsupervised optimal transport has been studied for image style
transfer [17, 35, 68], word translation [61, 10, 15, 9, 20], and natural language sequence translation
[51, 27, 1, 4, 63, 3]. Our method builds on these works, which often employ a combination of
cycle-consistency and adversarial loss. Importantly, unlike prior word and sequence translation
methods, multiple representations of the same input (e.g., heavily overlapping word vocabularies) are
unavailable in our setting. [53] proposes a solver for matching small sets of embeddings between
different vision-language models. Our method goes well beyond matching by taking unknown
embeddings and generating matching embeddings in the space of another model.
Embedding inversion. An emerging line of research investigates decoding text from language model
embeddings [54, 29, 42] and outputs [43, 7, 65]. vec2vec helps apply these to unknown embeddings,
without an encoder or paired data, by translating them to the space of a known model.
Bridging modality gaps. Previous work has noted an inherent “gap” between image- and text-based
models [33] and proposed various ways to unify the modalities [55]. Some approaches feed image
embeddings directly into language models [22, 59, 11, 34], while others generate captions from
image embeddings [40] or even from text embeddings themselves [42]. [12] introduces a shared
embedding space that integrates inputs from multiple modalities, including text, audio, and vision. In
contrast, our post-hoc approach directly translates between representations and complements these
systems by enabling inputs from a wide variety of embedding models.

8

Discussion and Future Work

The Platonic Representation Hypothesis conjectures that the representation spaces of modern neural
networks are converging. We assert the Strong Platonic Representation Hypothesis: the latent
universal representation can be learned and harnessed to translate between representation spaces
without any encoders or paired data.
9

In Section 5, we demonstrated that our vec2vec method successfully translates embeddings generated
from unseen documents by unseen encoders, and the translator is robust to (sometimes very) outof-distribution inputs. This suggests that vec2vec learns domain-agnostic translations based on the
universal geometric relationships which encode the same semantics in multiple embedding spaces.
In Section 6, we showed that vec2vec translations preserve sufficient input semantics to enable
attribute inference. We extracted sensitive disease information from patient records and partial content
from corporate emails, with access only to document embeddings and no access to the encoder that
produced them. Better translation methods will enable higher-fidelity extraction, confirming once
again that embeddings reveal (almost) as much as their inputs.
Our findings provide compelling evidence for the Strong Platonic Representation Hypothesis for textbased models. Our preliminary results on CLIP suggest that the universal geometry can be harnessed
in other modalities, too. The results in this paper are but a lower bound on inter-representation
translation. Better and more stable learning algorithms, architectures, and other methodological
improvements will support scaling to more data, more model families, and more modalities.

Acknowledgments and Disclosure of Funding
This research is supported in part by the Google Cyber NYC Institutional Research Program. JM is
supported by the National Science Foundation.

References
[1]
[2]

[3]

[4]
[5]
[6]

[7]

[8]
[9]
[10]
[11]

David Alvarez-Melis and Tommi S. Jaakkola. “Gromov-Wasserstein Alignment of Word
Embedding Spaces”. 2018. arXiv: 1809.00013 [cs.CL].
Dimosthenis Antypas, Asahi Ushio, Francesco Barbieri, and Jose Camacho-Collados. “Multilingual Topic Classification in X: Dataset and Analysis”. In: Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Ed. by Yaser Al-Onaizan,
Mohit Bansal, and Yun-Nung Chen. Miami, Florida, USA: Association for Computational
Linguistics, Nov. 2024, pp. 20136–20152.
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. “A robust self-learning method for fully
unsupervised cross-lingual mappings of word embeddings”. In: Proceedings of the 56th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Ed. by
Iryna Gurevych and Yusuke Miyao. Melbourne, Australia: Association for Computational
Linguistics, July 2018, pp. 789–798.
Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. “Unsupervised Neural
Machine Translation”. 2018. arXiv: 1710.11041 [cs.CL].
Yamini Bansal, Preetum Nakkiran, and Boaz Barak. “Revisiting Model Stitching to Compare
Neural Representations”. 2021. arXiv: 2106.07682 [cs.LG].
Irene Cannistraci, Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, and
Emanuele Rodolà. “Bootstrapping Parallel Anchors for Relative Representations”. 2023. arXiv:
2303.00721 [cs.LG].
Nicholas Carlini, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan
Hayase, A. Feder Cooper, Katherine Lee, Matthew Jagielski, Milad Nasr, Arthur Conmy,
Itay Yona, Eric Wallace, David Rolnick, and Florian Tramèr. “Stealing Part of a Production
Language Model”. 2024. arXiv: 2403.06634 [cs.CR].
Liqun Chen, Zhe Gan, Yu Cheng, Linjie Li, Lawrence Carin, and Jingjing Liu. “Graph Optimal
Transport for Cross-Domain Alignment”. 2020. arXiv: 2006.14744 [cs.CL].
Xilun Chen and Claire Cardie. “Unsupervised Multilingual Word Embeddings”. 2018. arXiv:
1808.08933 [cs.CL].
Alexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, and Hervé
Jégou. “Word Translation Without Parallel Data”. 2018. arXiv: 1710.04087 [cs.CL].
Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao,
Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma,
and Li Yi. “DreamLLM: Synergistic Multimodal Comprehension and Creation”. 2024. arXiv:
2309.11499 [cs.CV].

10

[12] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala,
Armand Joulin, and Ishan Misra. “ImageBind: One Embedding Space To Bind Them All”.
2023. arXiv: 2305.05665 [cs.CV].
[13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. “Generative adversarial networks”. In: Commun.
ACM 63.11 (Oct. 2020), pp. 139–144.
[14] IBM Granite Embedding Team. “Granite Embedding Models”. Dec. 2024.
[15] Edouard Grave, Armand Joulin, and Quentin Berthet. “Unsupervised Alignment of Embeddings with Wasserstein Procrustes”. 2018. arXiv: 1805.11222 [cs.LG].
[16] Florian Heimerl, Christoph Kralj, Torsten Moller, and Michael Gleicher. “embComp: Visual
Interactive Comparison of Vector Embeddings”. In: IEEE Transactions on Visualization and
Computer Graphics 28.8 (Aug. 2022), pp. 2953–2969.
[17] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. “Multimodal Unsupervised Imageto-Image Translation”. 2018. arXiv: 1804.04732 [cs.CV].
[18] Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. “The Platonic Representation Hypothesis”. 2024. arXiv: 2405.07987 [cs.LG].
[19] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad
Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. “MIMICIII, a freely accessible critical care database”. In: Scientific data 3.1 (2016), pp. 1–9.
[20] Armand Joulin, Piotr Bojanowski, Tomas Mikolov, Hervé Jégou, and Edouard Grave. “Loss in
Translation: Learning Bilingual Word Mapping with a Retrieval Criterion”. In: Proceedings of
the 2018 Conference on Empirical Methods in Natural Language Processing. Ed. by Ellen
Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii. Brussels, Belgium: Association
for Computational Linguistics, Oct. 2018, pp. 2979–2984.
[21] Bryan Klimt and Yiming Yang. “The enron corpus: a new dataset for email classification
research”. In: Proceedings of the 15th European Conference on Machine Learning. ECML’04.
Pisa, Italy: Springer-Verlag, 2004, pp. 217–226.
[22] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. “Grounding language models to images
for multimodal inputs and outputs”. In: International Conference on Machine Learning. PMLR.
2023, pp. 17283–17300.
[23] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. “Similarity of
Neural Network Representations Revisited”. 2019. arXiv: 1905.00414 [cs.LG].
[24] Zeljko Kraljevic, Thomas Searle, Anthony Shek, Lukasz Roguski, Kawsar Noor, Daniel
Bean, Aurelie Mascio, Leilei Zhu, Amos A Folarin, Angus Roberts, et al. “Multi-domain
clinical natural language processing with MedCAT: the medical concept annotation toolkit”.
In: Artificial intelligence in medicine 117 (2021), p. 102083.
[25] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh,
Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova,
Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le,
and Slav Petrov. “Natural Questions: A Benchmark for Question Answering Research”. In:
Transactions of the Association for Computational Linguistics 7 (2019). Ed. by Lillian Lee,
Mark Johnson, Brian Roark, and Ani Nenkova, pp. 452–466.
[26] Aarre Laakso and Garrison Cottrell. “Content and cluster analysis: Assessing representational
similarity in neural systems”. In: Philosophical Psychology 13.1 (2000), pp. 47–76.
[27] Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato. “Unsupervised Machine Translation Using Monolingual Corpora Only”. 2018. arXiv: 1711.00043
[cs.CL].
[28] Eric Lehman, Sarthak Jain, Karl Pichotta, Yoav Goldberg, and Byron Wallace. “Does BERT
Pretrained on Clinical Notes Reveal Sensitive Data?” In: Proceedings of the 2021 Conference
of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies. Ed. by Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer,
Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and
Yichao Zhou. Online: Association for Computational Linguistics, June 2021, pp. 946–959.
[29] Haoran Li, Mingshi Xu, and Yangqiu Song. “Sentence Embedding Leaks More Information
than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence”.
2023. arXiv: 2305.03010 [cs.CL].

11

[30] Jiaang Li, Yova Kementchedjhieva, Constanza Fierro, and Anders Søgaard. “Do Vision and
Language Models Share Concepts? A Vector Space Alignment Study”. In: Transactions of the
Association for Computational Linguistics 12 (2024), pp. 1232–1249.
[31] Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. “Convergent Learning:
Do different neural networks learn the same representations?” 2016. arXiv: 1511.07543
[cs.LG].
[32] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang.
“Towards General Text Embeddings with Multi-stage Contrastive Learning”. 2023. arXiv:
2308.03281 [cs.CL].
[33] Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Zou. “Mind the Gap:
Understanding the Modality Gap in Multi-modal Contrastive Representation Learning”. 2022.
arXiv: 2203.02053 [cs.CL].
[34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. “Visual Instruction Tuning”.
2023. arXiv: 2304.08485 [cs.CV].
[35] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. “Unsupervised Image-to-Image Translation
Networks”. 2018. arXiv: 1703.00848 [cs.CV].
[36] Valentino Maiorca, Luca Moschella, Antonio Norelli, Marco Fumero, Francesco Locatello,
and Emanuele Rodolà. “Latent Space Translation via Semantic Alignment”. 2024. arXiv:
2311.00664 [cs.LG].
[37] Mayug Maniparambil, Raiymbek Akshulakov, Yasser Abdelaziz Dahou Djilali, Mohamed
El Amine Seddik, Sanath Narayan, Karttikeya Mangalam, and Noel E O’Connor. “Do Vision
and Language Encoders Represent the World Similarly?” In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 2024, pp. 14334–14343.
[38] Deven M Mistry and Ali A Minai. “A Comparative Study of Sentence Embedding Models for
Assessing Semantic Variation”. In: International Conference on Artificial Neural Networks.
2023, pp. 1–12.
[39] Mazda Moayeri, Keivan Rezaei, Maziar Sanjabi, and Soheil Feizi. “Text-To-Concept (and
Back) via Cross-Model Alignment”. 2023. arXiv: 2305.06386 [cs.CV].
[40] Ron Mokady, Amir Hertz, and Amit H. Bermano. “ClipCap: CLIP Prefix for Image Captioning”. 2021. arXiv: 2111.09734 [cs.CV].
[41] Ari S. Morcos, Maithra Raghu, and Samy Bengio. “Insights on representational similarity in
neural networks with canonical correlation”. 2018. arXiv: 1806.05759 [stat.ML].
[42] John X. Morris, Volodymyr Kuleshov, Vitaly Shmatikov, and Alexander M. Rush. “Text
Embeddings Reveal (Almost) As Much As Text”. 2023. arXiv: 2310.06816 [cs.CL].
[43] John X. Morris, Wenting Zhao, Justin T. Chiu, Vitaly Shmatikov, and Alexander M. Rush.
“Language Model Inversion”. 2023. arXiv: 2311.13647 [cs.CL].
[44] Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, Francesco Locatello,
and Emanuele Rodolà. “Relative representations enable zero-shot latent space communication”.
2023. arXiv: 2209.15430 [cs.LG].
[45] Nikola Mrksic, Diarmuid Ó Séaghdha, Blaise Thomson, Milica Gasic, Lina Rojas-Barahona,
Pei-Hao Su, David Vandyke, Tsung-Hsien Wen, and Steve Young. “Counter-fitting Word
Vectors to Linguistic Constraints”. 2016. arXiv: 1603.00892 [cs.CL].
[46] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Y.
Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. “Large Dual Encoders Are
Generalizable Retrievers”. 2021. arXiv: 2112.07899 [cs.IR].
[47] Antonio Norelli, Marco Fumero, Valentino Maiorca, Luca Moschella, Emanuele Rodolà, and
Francesco Locatello. “ASIF: Coupled Data Turns Unimodal Models to Multimodal Without
Training”. 2023. arXiv: 2210.01738 [cs.LG].
[48] Gabriel Peyré, Marco Cuturi, and Justin Solomon. “Gromov-Wasserstein Averaging of Kernel
and Distance Matrices”. In: Proceedings of the 33rd International Conference on Machine
Learning (ICML). Vol. 48. JMLR: Workshop and Conference Proceedings. New York, NY,
USA: JMLR, 2016.
[49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
Sutskever. “Learning Transferable Visual Models From Natural Language Supervision”. 2021.
arXiv: 2103.00020 [cs.CV].
12

[50] Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. “SVCCA: Singular
Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability”.
2017. arXiv: 1706.05806 [stat.ML].
[51] Sujith Ravi and Kevin Knight. “Deciphering Foreign Language”. In: Proceedings of the
49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies. Ed. by Dekang Lin, Yuji Matsumoto, and Rada Mihalcea. Portland, Oregon,
USA: Association for Computational Linguistics, June 2011, pp. 12–21.
[52] Divya Saxena and Jiannong Cao. “Generative Adversarial Networks (GANs): Challenges,
Solutions, and Future Directions”. In: ACM Comput. Surv. 54.3 (May 2021).
[53] Dominik Schnaus, Nikita Araslanov, and Daniel Cremers. “It’s a (Blind) Match! Towards
Vision-Language Correspondence without Parallel Data”. 2025. arXiv: 2503.24129 [cs.CV].
[54] Congzheng Song and Ananth Raghunathan. “Information Leakage in Embedding Models”.
2020. arXiv: 2004.00053 [cs.LG].
[55] Shezheng Song, Xiaopeng Li, Shasha Li, Shan Zhao, Jie Yu, Jun Ma, Xiaoguang Mao, and
Weimin Zhang. “How to Bridge the Gap between Modalities: A Comprehensive Survey on
Multimodal Large Language Model”. 2023. arXiv: 2311.07594 [cs.CL].
[56] Yingtao Tian and Jesse Engel. “Latent translation: Crossing modalities by bridging generative
models”. In: arXiv preprint arXiv:1902.08261 (2019).
[57] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan
Majumder, and Furu Wei. “Text Embeddings by Weakly-Supervised Contrastive Pre-training”.
2024. arXiv: 2212.03533 [cs.CL].
[58] Liwei Wang, Lunjia Hu, Jiayuan Gu, Yue Wu, Zhiqiang Hu, Kun He, and John Hopcroft.
“Towards Understanding Learning Representations: To What Extent Do Different Neural
Networks Learn the Same Representation”. 2018. arXiv: 1810.11750 [cs.LG].
[59] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo,
Tong Lu, Jie Zhou, Yu Qiao, and Jifeng Dai. “VisionLLM: Large Language Model is also an
Open-Ended Decoder for Vision-Centric Tasks”. 2023. arXiv: 2305.11175 [cs.CV].
[60] Christopher Wolfram and Aaron Schein. “Layers at similar depths generate similar activations
across llm architectures”. In: arXiv preprint arXiv:2504.08775 (2025).
[61] Chao Xing, Dong Wang, Chao Liu, and Yiye Lin. “Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation”. In: Proceedings of the 2015 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Ed. by Rada Mihalcea, Joyce Chai, and Anoop Sarkar. Denver, Colorado:
Association for Computational Linguistics, May 2015, pp. 1006–1011.
[62] Hiroaki Yamagiwa, Momose Oyama, and Hidetoshi Shimodaira. “Discovering Universal
Geometry in Embeddings with ICA”. 2023. arXiv: 2305.13175 [cs.CL].
[63] Zhen Yang, Wei Chen, Feng Wang, and Bo Xu. “Unsupervised Neural Machine Translation
with Weight Sharing”. 2018. arXiv: 1804.09057 [cs.CL].
[64] Jinsung Yoon and Sercan O Arik. “Embedding-Converter: A Unified Framework for CrossModel Embedding Transformation”. 2025.
[65] Collin Zhang, John X. Morris, and Vitaly Shmatikov. “Extracting Prompts by Inverting LLM
Outputs”. 2024. arXiv: 2405.15012 [cs.CL].
[66] Collin Zhang, John X. Morris, and Vitaly Shmatikov. “Universal Zero-shot Embedding Inversion”. 2025. arXiv: 2504.00147 [cs.CL].
[67] Dun Zhang, Jiacheng Li, Ziyang Zeng, and Fulong Wang. “Jasper and Stella: distillation of
SOTA embedding models”. 2025. arXiv: 2412.19048 [cs.IR].
[68] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. “Unpaired Image-to-Image
Translation using Cycle-Consistent Adversarial Networks”. 2020. arXiv: 1703 . 10593
[cs.CV].

13

A

Compute

Our training and evaluation were conducted using diverse compute environments, including both
local and cloud GPU clusters. Experiments were done on NVIDIA 2080Ti, L4, A40, and A100
GPUs, listed in order of increasing computational capacity.
For our main experiments, we trained 36 distinct vec2vec models, with training durations ranging
from 1 to 7 days per model, depending on the specific GPU and model pair (which affected convergence rates). Additional 9 vec2vec models were trained for our ablations, bringing the total to
45 vec2vec models. Taking a conservative estimate of the average training time, this amounted to
approximately 180 GPU days (45 models × 4 days / model).
Evaluation procedures varied by model type:
• For each of our 45 vec2vec models, our full evaluation on NQ, TweetTopic, and MIMIC
datasets required approximately 1 hour per model across all GPU types.
• The 36 non-ablation GANs underwent additional analysis on the Enron email corpus,
requiring about 1.5 hours for inversion and downstream LLM evaluation.
• Naive baselines (30 models without CLIP) required approximately 30 minutes each for
evaluation, roughly half the time needed for our full models.
• Optimal transport baselines (36 models and 4 different algorithms) were run exclusively on
CPU, each requiring approximately 4 hours of computation time.
In total, our experiments consumed approximately 180 GPU days for training and an additional
78 GPU hours for evaluation and analysis. There were approximately 144 CPU hours required for
optimal transport.

B

Extended oracle-aided optimal transport baseline results

Let ui = M1 (di ) and vi = M2 (di ) denote embeddings of the same document di from two different
embedding models. In Section 5, we solve the optimal assignment problem:
n
X
π ∗ = arg min
cos(ui , vπ(i) ),
π

i=1

using four algorithms: Hungarian (linear sum assignment), Earth Mover’s Distance (EMD), Sinkhorn,
and Gromov-Wasserstein. Note that this baseline computes matchings and transports between
embeddings derived from the same underlying texts, strongly favoring optimal transport (OT) methods.
Nevertheless, OT still struggles when embeddings originate from different model backbones.
Since the Hungarian algorithm produces a discrete matching, it is evaluated only using Top-1
Accuracy, while the other algorithms are evaluated across all metrics. For each experiment, the
lowest-rank solver is reported in Table 2 and Table 4 (denoted by symbols in the final column).
Evaluation metrics are defined as follows:
1. Top-1 Accuracy: Fraction of embeddings correctly identified as closest pairs, calculated by
either selecting the maximum transported mass per embedding or applying the Hungarian
algorithm directly to the transport plan P . We report the higher accuracy between the two.
2. Mean Rank: Average rank position of the correct embedding match vi when sorted by
descending transported mass Pij from ui :
rank(vi ) = position of vi among sorted Pij .
3. Mean Cosine Similarity: Average cosine similarity between barycenters and true counterparts:
Pn
n
1X
j=1 Pij vj
′
vi = Pn
, Similarity =
cos(vi′ , vi ).
n i=1
j=1 Pij

C

Full out-of-distribution translation results

We provide baseline numbers for the experiments shown in Table 3, by dataset.
14

Naïve Baseline

vec2vec

OT Baseline

E1

E2

cos(·)

T-1

Rank

cos(·)

T-1

Rank

cos(·)

T-1

Rank

gra.

gtr
gte
ste.
e5

0.74 (0.0)
0.85 (0.0)
0.77 (0.0)
0.83 (0.0)

0.99
0.95
0.96
0.87

1.09 (0.1)
1.26 (0.1)
1.11 (0.0)
3.10 (0.7)

-0.04 (0.0)
0.00 (0.0)
0.00 (0.0)
0.02 (0.0)

0.00
0.00
0.00
0.00

415.61 (8.2)
406.73 (8.2)
417.27 (8.2)
405.53 (8.1)

0.51 (0.0)
0.74 (0.0)
0.56 (0.0)
0.75 (0.0)

0.01
0.01
0.00
0.01

398.32 (8.2)‡
398.14 (8.2)∗
399.57 (8.2)‡
398.33 (8.2)‡

gtr

gra.
gte
ste.
e5

0.79 (0.0)
0.85 (0.0)
0.77 (0.0)
0.80 (0.0)

0.98
0.96
0.96
0.53

2.41 (0.6)
1.29 (0.2)
1.10 (0.0)
13.38 (1.2)

-0.04 (0.0)
0.04 (0.0)
0.00 (0.0)
0.03 (0.0)

0.00
0.00
0.00
0.00

411.53 (8.3)
392.01 (8.2)
394.69 (8.3)
400.85 (8.2)

0.57 (0.0)
0.86 (0.0)
0.74 (0.0)
0.75 (0.0)

0.01
0.00
0.00
0.00

398.29 (8.2)‡
384.00 (8.1)†
390.67 (8.2)†
399.78 (8.2)‡

gte

gra.
gtr
ste.
e5

0.73 (0.0)
0.71 (0.0)
0.86 (0.0)
0.83 (0.0)

0.94
0.95
1.00
0.91

1.33 (0.1)
1.29 (0.1)
1.00 (0.0)
1.57 (0.2)

0.00 (0.0)
0.04 (0.0)
0.58 (0.0)
0.68 (0.0)

0.00
0.00
1.00
1.00

408.81 (8.3)
386.58 (8.3)
1.00 (0.0)
1.00 (0.0)

0.56 (0.0)
0.71 (0.0)
1.00 (0.0)
1.00 (0.0)

0.01
0.00
1.00
1.00

398.16 (8.2)∗
383.41 (8.1)†
1.00 (0.0)∗
1.00 (0.0)∗

ste.

gra.
gtr
gte
e5

0.79 (0.0)
0.77 (0.0)
0.90 (0.0)
0.85 (0.0)

0.99
1.00
1.00
0.98

1.09 (0.1)
1.00 (0.0)
1.00 (0.0)
1.05 (0.0)

0.00 (0.0)
0.00 (0.0)
0.58 (0.0)
0.37 (0.0)

0.00
0.00
1.00
0.89

418.16 (8.4)
393.07 (8.1)
1.00 (0.0)
1.55 (0.1)

0.57 (0.0)
0.71 (0.0)
1.00 (0.0)
1.00 (0.0)

0.00
0.00
1.00
1.00

399.56 (8.2)‡
390.26 (8.2)†
1.00 (0.0)∗
1.00 (0.0)∗

e5

gra.
gtr
gte
ste.

0.79 (0.0)
0.67 (0.0)
0.87 (0.0)
0.75 (0.0)

0.98
0.80
0.99
0.98

1.08 (0.0)
3.10 (0.6)
1.02 (0.0)
1.06 (0.0)

0.02 (0.0)
0.03 (0.0)
0.68 (0.0)
0.37 (0.0)

0.00
0.00
1.00
1.00

405.75 (8.3)
401.16 (8.4)
1.00 (0.0)
1.00 (0.0)

0.57 (0.0)
0.51 (0.0)
1.00 (0.0)
1.00 (0.0)

0.01
0.00
1.00
1.00

398.34 (8.2)‡
399.73 (8.2)‡
1.00 (0.0)∗
1.00 (0.0)∗

Table 6: Out-of-distribution translations on TweetTopic (with baselines): vec2vec models trained on
NQ and evaluated on the entire TweetTopic test set (800 tweets). The rank metric varies from 1 to
800, thus 400 corresponds to a random ordering. Standard errors are shown in parentheses. Symbols
denote the lowest-rank solver: Earth Mover’s Distance∗ , Sinkhorn† and Gromov-Wasserstein‡

Naïve Baseline

vec2vec

OT Baseline

E1

E2

cos(·)

T-1

Rank

cos(·)

T-1

Rank

cos(·)

T-1

Rank

gra.

gtr
gte
ste.
e5

0.74 (0.0)
0.85 (0.0)
0.72 (0.0)
0.84 (0.0)

0.60
0.08
0.13
0.12

23.38 (1.6)
346.21 (7.8)
242.23 (6.1)
361.06 (8.7)

-0.02 (0.0)
0.01 (0.0)
-0.01 (0.0)
0.02 (0.0)

0.00
0.00
0.00
0.00

4010.00 (25.8)
3978.35 (26.1)
3900.74 (26.2)
4024.92 (26.1)

0.82 (0.0)
0.92 (0.0)
0.86 (0.0)
0.93 (0.0)

0.00
0.00
0.02
0.00

3962.83 (26.1)†
3808.18 (25.9)†
3780.44 (26.0)†
3937.63 (26.2)†

gtr

gra.
gte
ste.
e5

0.78 (0.0)
0.84 (0.0)
0.72 (0.0)
0.82 (0.0)

0.51
0.12
0.27
0.01

35.27 (1.9)
279.56 (6.9)
127.92 (4.4)
1413.80 (18.3)

-0.02 (0.0)
0.08 (0.0)
0.00 (0.0)
0.09 (0.0)

0.00
0.00
0.00
0.00

4023.67 (26.1)
4180.47 (26.2)
4296.04 (26.1)
4064.47 (26.2)

0.87 (0.0)
0.87 (0.0)
0.76 (0.0)
0.93 (0.0)

0.00
0.00
0.00
0.00

3964.83 (26.1)†
4088.97 (26.2)‡
4095.11 (26.1)‡
4010.13 (26.1)†

gte

gra.
gtr
ste.
e5

0.73 (0.0)
0.69 (0.0)
0.85 (0.0)
0.86 (0.0)

0.09
0.12
1.00
0.54

342.15 (7.8)
256.63 (6.4)
1.00 (0.0)
17.71 (0.9)

0.01 (0.0)
0.08 (0.0)
0.56 (0.0)
0.69 (0.0)

0.00
0.00
1.00
0.98

3946.19 (25.8)
4229.90 (26.2)
1.00 (0.0)
1.04 (0.0)

0.87 (0.0)
0.69 (0.0)
1.00 (0.0)
1.00 (0.0)

0.00
0.00
1.00
1.00

3802.92 (25.9)†
4094.02 (26.1)‡
1.00 (0.0)∗
1.00 (0.0)∗

ste.

gra.
gtr
gte
e5

0.77 (0.0)
0.75 (0.0)
0.91 (0.0)
0.85 (0.0)

0.14
0.56
1.00
0.51

221.95 (5.9)
17.70 (1.0)
1.00 (0.0)
26.33 (1.2)

-0.01 (0.0)
0.00 (0.0)
0.56 (0.0)
0.35 (0.0)

0.00
0.00
1.00
0.59

3951.42 (25.9)
4339.83 (26.2)
1.00 (0.0)
12.68 (0.6)

0.87 (0.0)
0.70 (0.0)
1.00 (0.0)
0.93 (0.0)

0.01
0.00
1.00
1.00

3776.52 (26.0)†
4093.61 (26.1)‡
1.00 (0.0)∗
1.00 (0.0)†

e5

gra.
gtr
gte
ste.

0.78 (0.0)
0.66 (0.0)
0.87 (0.0)
0.75 (0.0)

0.21
0.01
0.60
0.46

151.09 (4.6)
1029.64 (14.9)
32.59 (2.6)
32.12 (1.4)

0.02 (0.0)
0.09 (0.0)
0.69 (0.0)
0.35 (0.0)

0.00
0.00
0.98
0.86

4008.10 (25.9)
4032.85 (26.2)
1.09 (0.0)
2.49 (0.1)

0.87 (0.0)
0.82 (0.0)
1.00 (0.0)
0.86 (0.0)

0.00
0.00
1.00
1.00

3932.58 (26.2)†
4010.06 (26.1)†
1.00 (0.0)∗
1.01 (0.0)†

Table 7: Out-of-distribution translations on MIMIC (with baselines): vec2vec models trained on
NQ and evaluated on an 8192-record subset of MIMIC. The rank metric varies from 1 to 8192, thus
4096 corresponds to a random ordering. Standard errors are shown in parentheses. Symbols denote
the lowest-rank solver: Earth Mover’s Distance∗ , Sinkhorn† and Gromov-Wasserstein‡

15

D
D.1

Ablations
Ablating components of our method

We ablate our method subtractively, measuring the key metrics after removing individual components
of our algorithm (described in Section 3). Table 8 shows that each component appears to be critical to
building good translations. While in each setting, vec2vec’s cos(·) is higher than the naïve baselines,
the Top-1 accuracies and ranks imply that ablated vec2vec translations are at best only slightly better
than the baseline and do not preserve the geometry of the vector space.
cos(·)

Top-1

Rank

vec2vec
Naïve Baseline
OT Baseline

0.75 (0.0)
0.04 (0.0)
0.70 (0.0)

0.91
0.00
0.00

2.64 (0.1)
4084.15 (9.2)
4078.45 (9.2)

– VSP loss
– CC loss
– latent GAN
– VSP and CC loss
– hyperparam. tuning

0.58 (0.0)
0.50 (0.0)
0.49 (0.0)
0.47 (0.0)
0.50 (0.0)

0.00
0.00
0.00
0.00
0.00

4196.64 (9.2)
3941.36 (9.3)
3897.09 (9.5)
3365.24 (9.3)
4011.73 (9.3)

Method

Table 8: gte → gtr translators trained without individual components of our method on NQ and
evaluated on a 65536-text subset of NQ (chunked in batches of 8192). The rank metric varies from 1
to 8192, thus 4096 corresponds to a random ordering. Standard errors are shown in parentheses.

D.2

Amount of data needed to learn translation
N

cos(·)

Top-1

Rank

1000000

0.75 (0.0)

0.92

2.73 (0.2)

10000
50000
100000
500000

0.57 (0.0)
0.74 (0.0)
0.74 (0.0)
0.75 (0.0)

0.01
0.81
0.85
0.92

1462.21 (20.)
3.91 (0.6)
4.52 (0.4)
2.73 (0.2)

Table 9: gte → gtr translators trained with different amounts of gte data: vec2vec models trained on
NQ and evaluated an 8192-record subset of NQ. The rank metric varies from 1 to 8192, thus 4096
corresponds to a random ordering. Standard errors are shown in parentheses.
In our main experiments, we use 1M-point subsets of NQ to train our vec2vec models. Now, we
train the gtr-gte vec2vec with 1M gtr embeddings but fewer gte embeddings. Table 9 shows that
translators still learn something (i.e. are better than random), and only 50K embeddings are sufficient
to learn a translation that is almost as good. Even with 10K embeddings, our method still learns
a non-trivial translation. Translation performance generally seems to improve with the amount of
training data, however.

E

Prompt for measuring information extraction

Figure 5 utilizes an LLM judge to determine whether the approximate inversion (using [66]) of a
translated email embedding leaks information about the original email. Figure 7 shows our prompt to
the GPT-4o judge:

16

Original email: {ground_truth}
Reconstructed email: {generation}.
Does the reconstructed email leak any information about the original email?
Answer with only ‘yes’ or ‘no’.
Figure 7: The prompt given to the LLM judge.

17

