Cross-Modal and Uni-Modal Soft-Label Alignment for Image-Text Retrieval
Hailang Huang1 , Zhijie Nie1,2 , Ziqiao Wang3 , Ziyu Shang4
1

Abstract

False Negative
A man riding a giant
foamy wave on a
surfboard.
ùëá#

Text-Text
Alignment
ùíî ùëªùíä , ùëªùíã = ùíì ùëªùíä , ùëªùíã

‚Ä¶

ùë∞ùíè

ùêº#

ùë∞ ùüè,

ùêº"

A man riding a wave on
top of a surfboard.
Image-Text
ùëá!
Alignment
True Negative
A bunch of carrots
sitting on a table.
ùëá"

=ùíì

Copyright ¬© 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

ùêº!
, ùëªùíè
ùíî ùë∞ùüè

Introduction
Image-Text Retrieval (ITR) retrieves relevant samples from
one modality based on a query in another modality. It involves two sub-tasks, one is image-to-text retrieval, which
requires finding the most relevant caption in the text gallery
for an input image, and the other one is text-to-image retrieval, which requires finding the most relevant image in
the image gallery for an input query text. Most existing
ITR methods (Radford et al. 2021; Diao et al. 2021; Zeng
et al. 2023) adopt contrastive learning techniques, treating
one sample as an anchor and the corresponding sample in
the other modality as a positive sample, while the uncorrelated samples are considered negatives. These methods aim
to maximize the similarity between anchor and positive samples and minimize the similarity between anchor and negative samples for cross-modal retrieval. Although these ITR
methods have achieved impressive performance, they have

Image-Image
Alignment

‚Ä¶

Current image-text retrieval methods have demonstrated impressive performance in recent years. However, they still
face two problems: the inter-modal matching missing problem and the intra-modal semantic loss problem. These problems can significantly affect the accuracy of image-text retrieval. To address these challenges, we propose a novel
method called Cross-modal and Uni-modal Soft-label Alignment (CUSA). Our method leverages the power of uni-modal
pre-trained models to provide soft-label supervision signals
for the image-text retrieval model. Additionally, we introduce
two alignment techniques, Cross-modal Soft-label Alignment
(CSA) and Uni-modal Soft-label Alignment (USA), to overcome false negatives and enhance similarity recognition between uni-modal samples. Our method is designed to be plugand-play, meaning it can be easily applied to existing imagetext retrieval models without changing their original architectures. Extensive experiments on various image-text retrieval
models and datasets, we demonstrate that our method can
consistently improve the performance of image-text retrieval
and achieve new state-of-the-art results. Furthermore, our
method can also boost the uni-modal retrieval performance
of image-text retrieval models, enabling it to achieve universal retrieval. The code and supplementary files can be found
at https://github.com/lerogo/aaai24 itr cusa.

ùíî ùë∞ùíä , ùë∞ùíã = ùíì ùë∞ùíä, ùë∞ùíã

arXiv:2403.05261v1 [cs.CV] 8 Mar 2024

SKLSDE, School of Computer Science and Engineering, Beihang University, Beijing, China
2
Shen Yuan Honors College, Beihang University, Beijing, China
3
School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Canada
4
School of Computer Science and Engineering, Southeast University, Nanjing, China
{huanghl, niezj}@act.buaa.edu.cn, zwang286@uottawa.ca, ziyus1999@seu.edu.cn

Figure 1: Illustration of our approach. We use soft-labels
r(¬∑, ¬∑) generated by uni-modal teacher models as a supervisory signal to guide cross-modal alignment and uni-modal
alignment for image-text retrieval models.

two limitations: the inter-modal matching missing problem and the intra-modal semantic loss problem.
The inter-modal matching missing problem refers to the
situation where, during model training, samples that should
be matched are mistakenly treated as unmatched due to
contrastive learning techniques and random sampling, resulting in a decrease in model performance. As shown in
Figure 1, in a batch containing n image-text pairs, if the
image I1 is considered as the anchor, the traditional contrast learning used in most ITR methods treats the text T1
as the positive sample matched with it (hard-labels), and
all other texts Tj,jÃ∏=1 as negative samples, but text Tn is
matched with image I1 . Samples like Tn are called false
negative samples, and the noisy signals caused by such false
negative samples will weaken the performance of the ITR
model. All of the image-text pairs in Figure 1 are sampled
from the widely used MSCOCO (Lin et al. 2014) dataset,
demonstrating the existence of the inter-modal matching
missing problem. This problem has also attracted the attention of other researchers (Parekh et al. 2021; Chun et al.
2021, 2022). (Parekh et al. 2021) published the first dataset
CrissCrossed Caption (CxC) with soft-labels to correct false
negative samples, but this dataset only focuses on scoring the similarity between texts, resulting in many missing

positives in the text-to-image relationship. Therefore, Chun
et al. (2022) released a machine-and-human-verified dataset
ECCV Caption, which includes manually verified corrections of false negatives, and pointed out that false negatives
hinder model evaluation. Additionally, they recommended
using the informative ranking-based metric mAP@R to evaluate model performance.
The intra-modal semantic loss problem refers to the insufficient capability of current ITR models to recognize
similar input samples. The reason for this problem is that
most ITR methods solely prioritize optimizing the similarity between two modalities, disregarding the relationships
within each modality. As illustrated in Figure 1, most ITR
methods only focus on aligning image-text pairs and overlook the operations of image-image alignment and text-text
alignment. TCL (Yang et al. 2022) tries to introduce selfsupervised contrastive learning (Gao, Yao, and Chen 2022;
Chen et al. 2020) methods on uni-modal to obtain better
joint multi-modal features, and it implicitly performs unimodal alignment at the same time, but our experiments show
that it is difficult to achieve effective uni-modal alignment
through data augmentation alone. We mathematically prove
that solely emphasizing cross-modal alignment hinders the
ability of the model to recognize similar input samples,
thereby weakening the performance of image-text retrieval
in cases where the model is confronted with unseen samples
during training but similar to certain samples in the training set. Although there have been some methods (Yang et al.
2022; Li et al. 2022c, 2023a,b) attempting to solve the above
problems and achieve certain effects, they have only separately addressed one of the two problems without considering the correlation between them.
To address the above two challenges, we propose a novel
and comprehensive framework for image-text retrieval,
called Cross-modal and Uni-modal Soft-label Alignment
(CUSA). As shown in Figure 1, our method leverages unimodal pre-training models to provide soft-label supervision
signals for the ITR model. Compared to hard-labels, softlabels can capture more fine-grained and nuanced semantic
information across and within modalities. Our method uses
two alignment techniques, Cross-modal Soft-label Alignment (CSA) and Uni-modal Soft-label Alignment (USA).
The CSA method as a regularization term to guide the crossmodal alignment of the ITR model through soft-labels. With
this approach, the model can learn not only from binary labels but also from continuous labels that reflect the semantic relatedness between images and texts. The USA method
uses soft-labels to guide the uni-modal alignment of the ITR
model. As a result, the model can better recognize similar samples within each modality and distinguish dissimilar ones. Our method is plug-and-play and can be easily applied to existing ITR models without changing their original
architectures. We conduct extensive experiments on various
ITR models and datasets and demonstrate that our method
can consistently improve the performance of image-text retrieval and achieve new state-of-the-art results. Moreover,
our method can also boost the uni-modal retrieval performance of the ITR model, enabling it to achieve universal retrieval. Our main contributions are summarized as follows:

‚Ä¢ We mathematically prove that solely emphasizing crossmodal alignment hinders the ability of the ITR model to
recognize similar input samples, thereby weakening the
performance of image-text retrieval.
‚Ä¢ We introduce two alignment techniques, CSA and USA,
that use soft-labels as supervision signals to guide the
cross-modal and uni-modal alignment of the ITR model.
‚Ä¢ We conduct extensive experiments on various ITR models and datasets, and show that our method can consistently improve the performance of image-text retrieval
and achieve new state-of-the-art results.

Related Work
Image-Text Retrieval Image-Text Retrieval (ITR) is a
typical cross-modal task, whose main challenge is to learn
a shared representation of images and texts and accurately
measure their similarity. Existing models can be classified
into three categories based on their architecture: (1) dualencoder (Radford et al. 2021; Jia et al. 2021); (2) fusionencoder (Lee et al. 2018; Diao et al. 2021; Chen et al. 2021;
Li et al. 2022b; Zhang et al. 2022), and (3) dual-encoder
+ fusion-encoder (Li et al. 2021, 2022a; Wang et al. 2022;
Zeng et al. 2023). Among them, dual-encoder models usually contain a text encoder and an image encoder, producing representations to measure the similarities between images and texts. Benefiting the simple calculation method, the
dual-encoders model usually has a fast retrieval speed. However, due to the lack of interaction between images and texts,
these models often have lower performance compared to
fusion-encoder models. The models with both dual-encoder
and fusion-encoder achieve a certain balance between performance and efficiency. Therefore, most recent works in
ITR follow the dual-encoder or the dual-encoder+fusionencoder architecture to ensure high retrieval efficiency. Recent works (Radford et al. 2021; Wang et al. 2022; Zeng
et al. 2023) introduce self-supervised contrastive learning
to align different modalities in the models with these two
architectures. However, these models use hard-labels as supervised signals for training and only align images and texts
annotated in the dataset, ignoring potential semantic similarities between different image-text pairs. Our method leverages the external knowledge provided by pre-trained unimodal models, and it can be easily applied to existing ITR
methods, thereby partially compensating for this limitation.
Alignmemt with Soft-label Soft-label usually are used to
alleviate the strict constraints imposed by noisy hard-labels
and avoid excessive confidence in incorrect predictions by
the model, which has been proven effective in various tasks.
In the methods based on knowledge distillation, the logit
produced by the teacher model can be regarded as the softlabel, which guides the learning of the student model. In the
task of image-text retrieval, Li et al. (2021) and Gao et al.
(2023) use self-distillation (He et al. 2020) to reduce the
adverse effects of noisy image-text pairs. The core idea is
to let the student model act as its teacher, and as training
progresses, the student model dynamically evolves into its
teacher. Besides, some methods (Li et al. 2022c, 2023a,b)

used external pre-trained language models to provide additional knowledge to overcome false negatives in contrastive
learning. However, existing methods only use soft-labels for
supervision in inter-modal, while our method is not only in
it to align in inter-modal but also in intra-modal.

Method
Preliminaries
Given a dataset of image-text pair {(Ii , Ti )}N
i=1 , where
(Ii , Ti ) represents the paired relationship between the image Ii and the sentence Ti . There is an image encoder that
maps each Ii to the normalized representation IÀÜi and a text
encoder that converts Ti to the normalized representation TÃÇi .
The methods based on contrastive learning use InfoNCE
loss (van den Oord, Li, and Vinyals 2019) to align these
image-text pairs. Specifically, multiple image-text pairs are
sampled from the dataset and formed into a batch. During
training, the paired image and text are ‚Äúpulled close‚Äù while
the unpaired ones in the batch are ‚Äúpushed away‚Äù in the
high-dimension space. We denote the cosine similarity of
IÀÜi and TÃÇj as si2t
ij obtained by the learnable encoders, then the
probability that Ii pairs with Tj can be calculated by

exp si2t
ij /œÑ
i2t
Qij = PN
(1)
,
i2t
k=1 exp sik /œÑ
where N is the batch size and œÑ is a learnable temperature
parameter. Similarly, we can denote the cosine similarity of
Ti and Ij as st2i
ij and calculate the probability that Ti pairs
with Ij by

exp st2i
ij /œÑ
t2i
Qij = PN
(2)
.
t2i
k=1 exp sik /œÑ
Then we can obtain the discrete probability distribution
i2t
t2i
Qi2t
= (Qi2t
=
i
i1 , ...QiN ) for each image Ii and Qi
t2i
t2i
(Qi1 , ...QiN ) for each text Ti . In the traditional setting of
contrastive learning, the ground truth is derived from the annotation of the dataset, which means that the labeled imagetext pairs in the dataset are considered to be semantically
consistent, while the arbitrary unlabeled image and text in
the dataset are considered to have no semantic associations.
Therefore, a one-hot label yi = (yi1 , ..., yiN ) is introduced,
where yii equals 1 for positive pairs and 0 for others. Then
the infoNCE loss can be expressed as
N

Li2t
itc =


1 X
H yi , Qi2t
,
i
N i=1

Lt2i
itc =


1 X
.
H yi , Qt2i
i
N i=1

(3)

Feature Extraction
We introduce two uni-modal pre-training models as teacher
models to calculate soft-labels for guiding the ITR model.
In this work, we select Unicom (An et al. 2023) for the
teacher model of images and Sentence-BERT (Reimers and
Gurevych 2019) for the texts. Unicom is currently the stateof-the-art model for image retrieval, while Sentence-BERT
is a well-known model that achieves outstanding performance in the tasks of Semantic Textual Similarity (STS).
During training, we extract image features from all images
in the datasets with Unicom-ViT-B/32 1 and text features
from all available text using MPNet (Song et al. 2020) finetuned by the authors of Sentence-BERT, i.e. all-mpnet-basev2 2 . The extraction of image and text features can be done
offline, so it does not add complexity to the ITR model during online training. It is worth noting that the choice of
teacher models for images and texts can be flexible and can
be replaced with any available models.

Cross-modal Soft-label Alignment
In practice, there may be a potential semantic association
between the unpaired image and text in the same batch, but
not labeled in the dataset. We call this situation ‚Äúthe intermodal matching missing problem‚Äù, which leads to semantically matched images and text being incorrectly pushed
away during training. To address the problem, we propose
the Cross-modal Soft-label Alignment (CSA) method (Figure 2). Specifically, we calculate the cosine similarity between IÀÜi and IÀÜj based on the features obtained from the
i2i
teacher model and denoted it as rij
. Then the similarity beÀÜ
ÀÜ
tween Ii and Ij is performed within-batch normalization to
obtain Piji2i , the probability estimate that these two images
are semantically consistent from the teacher network:

i2i
exp rij
i2i
Pij = PN
(5)
.
i2i
j=1 exp rij
i2i
i2i
Finally we denote the probability distribution (Pi1
, ..., PiN
)
i2i
as Pi . Similarly, we calculate the similarity between Ti and
t2t
Tj , denoted as rij
and obtain Pit2t . During training, we rei2i
gard Pi as the target distribution to guide the learnable distribution Qi2t
i for image-to-text alignment using KL divergence. Similarly, we use Pit2t to guide the learnable distribution Qt2i
i for text-to-image alignment using KL divergence.
Finally, the loss function for CSA is denoted as LCSA , which
can be written as

N

(4)

where H(., .) denotes the cross-entropy operation,
and the fi
t2i
nal loss is denoted as Litc = Li2t
itc + Litc /2. We use Loriginal
to represent the original loss function of the ITR model,
which is equal to Litc for most models. This is illustrated
in Figure 2 with a green circle plus sign.


t2i
LCSA = Li2t
CSA + LCSA /2

(6)

t2t
t2i
= DKL (Pii2i ‚à• Qi2t
i ) + DKL (Pi ‚à• Qi ) /2.

1
2

v2

https://github.com/deepglint/unicom
https://huggingface.co/sentence-transformers/all-mpnet-base-

Image-Text Retrieval Model

Image

I Image

ùêº"

Enc.

Online

Cross-modal
Similarity

ùêº"
ùëá$

Cross-modal logits
I2T logits

Offline

t1 t2 ‚Ä¶ tn
i1
‚Ä¶
i2
‚Ä¶
‚Ä¶ ‚Ä¶‚Ä¶‚Ä¶‚Ä¶
in
‚Ä¶

i1 i2 ‚Ä¶ in
t1
‚Ä¶
t2
‚Ä¶
‚Ä¶ ‚Ä¶‚Ä¶‚Ä¶‚Ä¶
tn
‚Ä¶

i1 i2 ‚Ä¶ in
i1 1
‚Ä¶
i2
1 ‚Ä¶
‚Ä¶ ‚Ä¶‚Ä¶‚Ä¶‚Ä¶
in
‚Ä¶ 1

Image

ùêº" Image I
Teacher
Model.

I2I soft-labels
FC

Text
A man riding a T Text
wave on top of
Enc. ùëá
$
a surfboard.

FC

Uni-modal
Similarity

i1 i2 ‚Ä¶ in
i1
‚Ä¶
i2
‚Ä¶
‚Ä¶ ‚Ä¶‚Ä¶‚Ä¶‚Ä¶
in
‚Ä¶

t1 t2 ‚Ä¶ tn
t1
‚Ä¶
t2
‚Ä¶
‚Ä¶ ‚Ä¶‚Ä¶‚Ä¶‚Ä¶
tn
‚Ä¶

I2I logits
KL-Divergence

Uni-modal Pre-training Model

T2I logits

Original loss

t1 t2 ‚Ä¶ tn
t1 1
‚Ä¶
t2
1 ‚Ä¶
‚Ä¶ ‚Ä¶‚Ä¶‚Ä¶‚Ä¶
tn
‚Ä¶ 1

T2T logits

ùëá$ Text

Teacher
Model.

T

Text
A man riding a
wave on top of
a surfboard.

T2T soft-labels
Uni-modal Similarity

Targets

Uni-modal logits

Figure 2: Illustration of our proposed CUSA. It involves an ITR model used for training and a non-training uni-modal teacher
model that provides soft-label supervision signals. The CSA method optimizes cross-modal logits, while the USA method
optimizes uni-modal logits.

Uni-modal Soft-label Alignment
Although many works in ITR have achieved impressive results, they neglect the uni-modal alignment. In this work,
we call this situation ‚Äúthe intra-modal semantic loss problem‚Äù, which may affect the model‚Äôs generalization performance on unseen data. As shown in Figure 3, we consider
the scenario where the image-text pair ‚ë† and ‚ë¢ are the samples in the training set, while the pair ‚ë° is an unseen sample
during training. For most models in ITR, it is likely to encounter the situation depicted in Figure 3(a): the image and
text in each pair can be aligned well, but two pairs may be
mapped to different regions on the hypersphere since unimodal alignment is not introduced. Assume that the encoder
that completes the training is L-Lipschitz continuity, which
means that it maps the elements that are close enough in the
sample level to near positions on the hypersphere. Consider
the case where the encoder generalizes to sample ‚ë°: image
‚ë° is closer to image ‚ë¢ at the pixel level and is therefore
mapped to its proximity, and text ‚ë° is closer to text ‚ë¢ at the
literal level and is therefore also mapped to its proximity. As
a result, it is difficult for image ‚ë° to be recalled by text ‚ë°
relying on a specific distance function of the hypersphere,
and vice versa. In addition, we mathematically prove that
the models that only focus on cross-modal alignment in ITR
are deficient in uni-modal capabilities, precisely because the
ability to recognize similar input samples of the model is not
good enough, which limits the generalization performance
of cross-modal retrieval.
Proposition 1. Cross-modal alignment alone is not sufficient for optimal recognition of similar samples.3
Motivated by Proposition 1, we propose the Uni-modal,
Soft-label Alignment (USA) method to enhance the model‚Äôs
3
Please refer to Appendix A for the proof: https://github.com/
lerogo/aaai24 itr cusa

Z

‚ìµ
‚ì∂‚ì∑
‚ì∑

‚ìµ
‚ìµ‚ì∂
O

(a)

‚ì∂

‚ì∑

‚ìµ

a person is
riding a wave on
a surfboard
‚ì∂ A man riding a
wave on top of a
surfboard
‚ì∑ A man riding a
giant foamy
wave on top of
a surfboard

Uni-modal Soft-label Alignment

Z

‚ìµ
‚ì∂‚ì∑
‚ìµ
‚ì∂‚ì∑
O

(b)

Figure 3: (a) Models ignoring intra-modal alignment tend
to obtain feature distributions on the hypersphere; (b) After
adding the USA term, the model tends to obtain feature distributions on the hypersphere.
ability to recognize the similarity between uni-modal samples, thereby improving performance in the unseen data. As
illustrated in Figure 2, we first obtain Pii2i and Pit2t from the
teacher model, respectively. followed by extracting the representation hatI for image I and TÃÇ for text T from the outputs of the ITR model. Note that each representation is then
passed through an additional projector, which is a Full Connectivity (FC) layer in practice. Similar to the Cross-modal
Soft-label Alignment method, we express the cosine simit2t
larity of IÀÜi and IÀÜj as si2i
ij , while that of TÃÇi and TÃÇj as sij .
Then the similarity between IÀÜi and IÀÜj is performed withinbatch normalization to obtain Qi2i
ij , the probability estimate
that these two images are semantically consistent from student network:

exp si2i
ij /œÑ
i2i
Qij = PN
(7)
.
i2i
j=1 exp sij /œÑ
i2i
Finally we denote the probability distribution (Qi2i
i1 , ..., QiN )

Model

MSCOCO (5K Test Set)
Image-to-Text
Text-to-Image
R@1 R@5 R@10 R@1 R@5 R@10

RSUM

Flickr30K (1K Test Set)
Image-to-Text
Text-to-Image
R@1 R@5 R@10 R@1 R@5 R@10

RSUM

Faster-RCNN, ResNet-101, without pre-training
SCAN
50.4 82.2
90.0
38.6 69.3
VSE‚àû
56.6 83.6
91.4
39.3 69.9
VSRN++
54.7 82.9
90.9
42.0 72.2
NAAF
58.9 85.2
92.0
42.5 70.9
SGR‚Ä†
57.3 83.2
90.6
40.5 69.6
+ CUSA
57.4 84.5
92.0
40.9 71.2
SAF‚Ä†
55.5 83.8
91.8
40.1 69.7
+ CUSA
55.6 84.7
92.3
40.8 71.7
SGRAF‚Ä†
58.8 84.8
92.1
41.6 70.9
+ CUSA
59.8 86.1
93.3
43.3 73.2

80.4
81.1
82.7
81.4
80.3
81.9
80.4
82.4
81.5
83.6

410.9
421.9
425.4
430.9
421.5
427.9
421.3
427.5
429.7
439.2

67.4
76.5
79.2
81.9
76.6
79.3
75.6
77.8
78.4
81.4

90.3
94.2
94.6
96.1
93.7
94.9
92.7
95.0
94.6
95.6

95.8
97.7
97.5
98.3
96.6
97.5
96.9
98.0
97.5
98.5

48.6
56.4
60.6
61.0
56.1
58.4
56.5
58.5
58.2
61.0

77.7
83.4
85.6
85.3
80.9
84.2
82.0
83.9
83.0
86.1

85.2
89.9
91.4
90.6
87.0
89.5
88.4
90.3
89.1
91.5

465.0
498.1
508.9
513.2
490.9
503.7
492.1
503.5
500.8
514.1

Dual-Encoder, pre-training
CLIPViT-B/32
56.3 81.7
+ CUSA
57.3 83.1
CLIPViT-L/14 ‚Ä° 67.1 89.4
+ CUSA
67.9 90.3

81.1
82.1
87.7
88.1

422.6
429.7
469.6
473.1

78.7
82.1
87.3
90.8

95.4
95.3
99.0
99.1

98.0
97.9
99.5
99.7

66.3
67.5
76.4
77.4

88.6
89.6
94.8
95.5

93.1
93.9
97.4
97.7

520.0
526.3
554.5
560.2

Dual Encoder + Fusion encoder reranking, pre-training
BLIPbase
81.9 95.4
97.8
64.3 85.7
91.5
OmniVL
82.1 95.9
98.1
64.8 86.1
91.6
X2VLMbase
83.5 96.3
98.5
66.2 87.1
92.2
+ CUSA
83.3 96.6
98.5
67.1 87.6
92.7
X2VLMlarge
84.4 96.5
98.5
67.7 87.5
92.5

516.6
518.6
523.8
525.8
527.1

97.3
97.3
98.5
98.5
98.8

99.9
99.9
100.0
100.0
100.0

100.0
100.0
100.0
100.0
100.0

87.3
87.9
90.4
91.3
91.8

97.6
97.8
98.2
98.8
98.6

98.9
99.1
99.3
99.5
99.5

581.0
582.0
586.4
588.1
588.7

89.4
90.3
94.7
94.7

42.8
44.2
51.6
52.4

71.2
72.7
79.1
79.8

Table 1: Experimental results of image-text retrieval on MSCOCO and Flickr30K. ‚Ä† denotes the improved results by the author
compared to the original paper, while ‚Ä° represents the CLIPViT-L/14@336px model.
t2t
as Qi2i
i and use the similar step to obtain Qi . During training, we employ KL divergence to guide the alignment loss
of the uni-modal logits using Pii2i and Pit2t , respectively. This
alignment loss facilitates the alignment between uni-modal
samples: During training, we regard Pii2i as the target distribution to guide the learnable distribution Qi2i
i for imageto-text alignment using KL divergence. Similarly, we use
Pit2t to guide the learnable distribution Qt2t
i for text-to-image
alignment. Finally, the loss function for USA is denoted as
LUSA , which can be written as


t2t
LUSA = Li2i
USA + LUSA /2

(8)

t2t
t2t
= DKL (Pii2i ‚à• Qi2i
i ) + DKL (Pi ‚à• Qi ) /2.

The USA method brings similar samples closer together in
a uni-modal manner while pushing dissimilar samples apart.
After such an intra-modal alignment operation, we can make
the model more biased towards the situation in Figure 3 (b).
Through this uni-modal constraint, we hope that the ITR
model can achieve better results in the unseen data.

Training Objective
We use the above two losses, CSA and USA together, to
adjust the original loss of the ITR model, so the overall loss
function is expressed as:
LCUSA = Loriginal + Œ± ¬∑ LCSA + Œ≤ ¬∑ LUSA .

(9)

where Œ± and Œ≤ is the loss weight, which ranged from 0.1
to 1.0. Our method is plug-and-play and does not affect the

original architecture of the ITR model. When applying our
method, we only need to add one FC layer at the image and
text ends respectively to implement the USA method, and
the rest of the model structure does not need any changes.
Therefore, it can be easily extended to existing ITR models.

Experiments
Experiment Setup
Datasets To evaluate the ability of the ITR model in
image-text retrieval and judging similar samples, we evaluated our method on several datasets for both cross-modal
and uni-modal tasks. For image-text retrieval, we evaluate
our approach on three datasets: Flickr30K (Young et al.
2014), MSCOCO (Lin et al. 2014), and ECCV Caption
(Chun et al. 2022). For image retrieval experiments, our
evaluation is conducted on the test sets of four widely used
datasets: CUB (Welinder et al. 2010), SOP (Oh Song et al.
2016), In-Shop (Liu et al. 2016), and iNaturalist (Van Horn
2018). In terms of semantic textual similarity, we evaluate
our approach on seven STS tasks: STS 2012‚Äì2016(Agirre
et al. 2012, 2013, 2014, 2015, 2016), STS Benchmark (Cer
et al. 2017) and SICK-Relatedness (Marelli et al. 2014).
Similar to image retrieval, we solely use the test sets of the
STS datasets for our semantic textual similarity evaluation.
Evaluation Metrics In the evaluation of the performance
on the Flickr30K and MSCOCO datasets, we utilize the
R@K(recall at K, K ‚àà {1, 5, 10}) metric, representing
the proportion of queries where the ground truth is ranked

Model

Image-to-Text
Text-to-Image
mAP@R R-P R@1 mAP@R R-P R@1

Faster-RCNN, ResNet-101, without pre-training
SGR‚Ä†1
26.8
38.7 70.3
42.2
+ CUSA
28.0
40.0 72.4
44.0
SAF‚Ä†1
26.6
38.5 69.6
43.1
+ CUSA
27.4
39.8 71.4
44.4
SGRAF‚Ä†1
28.1
39.8 72.3
43.7
+ CUSA
29.5
41.4 74.5
46.4

51.2
53.0
52.0
53.6
52.5
55.1

83.6
83.4
83.8
84.6
84.4
85.7

Dual-Encoder, pre-training
CLIPViT-B/32
28.5
39.4
+ CUSA
29.6
40.7
CLIPViT-L/14@336px
32.8
43.4
+ CUSA
33.6
44.1

50.8
53.6
54.2
55.8

83.0
85.7
87.2
88.2

72.5
72.0
79.7
80.9

41.7
45.2
45.5
47.6

Dual Encoder + Fusion encoder reranking, pre-training
X2VLMbase ‚Ä†2
36.6
45.2 89.7
43.8
51.2 93.5
+ CUSA
37.6
46.5 89.8
48.4
55.9 94.1

Table 2: Experimental results of image-text retrieval on
ECCV Caption. ‚Ä†1 denotes the results of reproducing the
method, and ‚Ä†2 denotes the results from the checkpoint provided by the author.
within the top K. Additionally, to comprehensively assess
the image-text retrieval performance, we summarize all the
recall values as RSUM. Inspired by (Chun et al. 2022), we
also utilize the R-P and mAP@R metrics on the ECCV Caption dataset to evaluate the ability of the model to recall incorrect negatives. Following the work of (An et al. 2023),
we adopt the R@1 metric as the standard for evaluating performance across all image retrieval datasets. In the case of
semantic textual similarity, we leverage the SentEval (Conneau and Kiela 2018) toolkit to compute Spearman‚Äôs correlation, which serves as a reliable measure of the semantic
textual similarity performance of the model.
Implementation Details To validate the improved performance of our approach in cross-modal and uni-modal tasks,
we executed a series of experiments involving three models:
SGRAF (Diao et al. 2021), CLIPViT-B/32, ViT-L/14@336 (Radford et al. 2021), and X2VLMbase (Zeng et al. 2023). These
models are all impressive models for image-text retrieval,
with SGRAF being an open-source, non-pretrained SOTA
model, CLIP being a popular pre-trained dual-encoder
model, and X2VLM being an advanced pre-trained model
with dual encoders and a fusion encoder. All CLIP model
reports are based on fine-tuned results using InfoNCE.

Main Results
Results on MSCOCO and Flickr30K Table 1 shows the
results of our comparison on various types of ITR models. It
shows that our method can achieve improvement on all models and has achieved new SOTA results in both pre-trained
and non-pretrained benchmarks. On the MSCOCO 5K test
set, our method increased the RSUM of SGRAF+CUSA by
9.5%, CLIPB/32+CUSA by 7.1%, CLIPL/14+CUSA by 3.5%, and
X2VLMbase+CUSA by 2.0%. This is a significant progress.
On the Flickr30K test set, our method increased the

Model

CUB SOP In-Shop INaturalist Avg.

Faster-RCNN, ResNet-101, without pre-training
SGR‚Ä†1
31.1 51.9 19.5
33.7
+ CUSA
34.6 60.7 31.6
41.9
SAF‚Ä†1
34.1 52.8 20.3
37.0
+ CUSA
39.9 59.6 32.2
44.6

34.1
42.2
36.0
44.1

Dual-Encoder, pre-training
CLIPViT-B/32
41.5 51.8
+ CUSA
49.6 56.5
CLIPViT-L/14@336px 58.3 61.1
+ CUSA
67.2 63.0

40.7
46.5
57.4
61.8

28.1
34.1
46.9
48.2

41.3
45.6
63.5
68.7

Dual Encoder + Fusion encoder reranking, pre-training
X2VLMbase ‚Ä†2
53.6 64.2 52.6
59.3
57.4
+ CUSA
58.9 67.0 54.2
62.2
60.6

Table 3: Performance of image retrieval on 4 datasets.
Model

STS12-16Avg. STS-B SICK-R Avg.‚Ä°

Faster-RCNN, ResNet-101, without pre-training
SGR‚Ä†1
51.8
58.1
62.7
+ CUSA
55.9
65.2
64.9
SAF‚Ä†1
53.9
64.5
63.5
+ CUSA
54.8
66.3
64.5

54.3
58.5
56.8
57.8

Dual-Encoder, pre-training
CLIPViT-B/32
67.4
+ CUSA
71.6
CLIPViT-L/14@336px
69.8
+ CUSA
73.4

69.4
73.2
71.9
74.5

76.2
78.3
78.6
79.9

72.9
75.8
75.5
74.9

Dual Encoder + Fusion encoder reranking, pre-training
X2VLMbase ‚Ä†2
26.6
22.3
50.4
29.4
+ CUSA
46.8
47.9
76.2
51.2

Table 4: Sentence embedding performance on STS tasks. ‚Ä°
represents the average result of 7 STS datasets.
RSUM of SGRAF+CUSA by 13.3%, CLIPB/32+CUSA by 6.3%,
CLIPL/14+CUSA by 5.7%, and X2VLMbase+CUSA by 1.7%.
CUSA consistently performs excellently, proving its effectiveness and robustness.
Results on ECCV Caption We also conducted a fair experiment on the ECCV Caption dataset. As shown in Table
2, with the help of our CUSA method, there is an average
improvement of 1.0% in image-to-text retrieval for 4 models on 3 metrics, while in text-to-image retrieval, the average
improvement of all metrics is 2.5%. The results show that
the introduction of our CUSA method can improve the accuracy and recall of false negatives retrieved by ITR models,
proving the effectiveness of our method.

Additional Results on Uni-Modal Retrieval
Table 3 and Table 4 show the performance of the ITR
model in uni-modal tasks. In the image modality, our CUSA
method shows an average improvement of 5.9% compared
to models without it. In the text modality, the average improvement is 6.7%. This strongly indicates that our method
can enhance the ability of the model to recognize similar in-

Model

Cross-modal
Uni-modal
RSUM‚Ä°1 RSUM‚Ä°2 Avg.‚Ä°3 Avg.IR Avg.STS

CLIPViT-B/32
+ CSA
+ USA
+ CUSA

422.6
427.7
425.7
429.7

520.0
525.4
523.4
526.3

52.6
54.5
53.6
54.5

40.7
40.4
47.8
46.5

69.4
66.5
73.1
73.2

Table 5: Ablation study on two types of tasks with four different settings. ‚Ä°1 , ‚Ä°2 , and ‚Ä°3 represent results on MSCOCO,
Flickr30K, and ECCV Caption datasets respectively.

(a). text-to-image
A young man on a
skateboard jumps a
huge tennis show in
front of an old
building.

CLIPViT-B/32 + CUSA

(b). image-to-text

put samples, thus facilitating image-text retrieval. Not only
that, the significant improvement in uni-modal tasks brings
a certain universal retrieval ability to the ITR model, which
can be very useful in low-resource scenarios.
CLIPViT-B/32
Image

CLIPViT-B/32 + USA
Text

(a)

CLIPViT-B/32

CLIPViT-B/32

CLIPViT-B/32 + CUSA

1. A black and white photo of two
birds walking in a grassy field.

1. A black and white image of
two birds in the sand.

2. A black and white image of
two birds in the sand.

2. A black and white photo of two
birds walking in a grassy field.

1. Two fighter jets flying through
a blue sky.

1. Two seagulls flying in a clear
blue sky.

2. An old military plan does
tricks against a bright blue sky.

2. Two seagulls flying in the
light blue sky.

Figure 5: Case study: the green texts or boxes represent the
same as the ground-truth, while the red ones do not.

(b)

Figure 4: Visualization of features generated from 5000 randomly selected image-text pairs from the MSCOCO test set.
(a) represents the visualization of image features, while (b)
represents the visualization of text features.

Ablation Study
To evaluate the contributions of CSA and USA, we used the
original loss and tested each method on various ITR models.
In this analysis, we fine-tuned CLIPViT-B/32 on the MSCOCO
dataset and evaluated its performance on two types of tasks,
as shown in Table 5. The results show that removing either
CSA or USA leads to a decrease in image-text retrieval performance, with the effect of CSA being more significant.
The reason is that CSA aims to overcome false negatives and
improve the cross-modal capabilities of the model. Without
the USA method for uni-modal alignment, it would significantly impair the ability of ITR models to recognize similar
input samples, thereby harming performance.
To demonstrate the impact of the USA method on the
features generated by the ITR models, we conducted a controlled experiment on the CLIPViT-B/32 model both with and
without the USA method. We fine-tuned it on MSCOCO,
randomly selected 5000 image-text pairs from its test set,
and generated features for images and texts separately. Then
we used TSNE for dimensionality reduction analysis, and
the visualization results are shown in Figure 4. It shows
that the features generated with the USA method and those

generated without the USA method are completely distinguished, indicating that the USA method has changed the
distribution of the original feature (Whether in image modality or text modality). Further observation of the visualization
illustration shows that the USA method can better cluster
similar samples together and distinguish dissimilar samples,
indicating that the USA method is effective in improving the
ability of the ITR model to recognize similar input samples.

Case Study
We have presented the comparison results between our
method and the original model in Figure 5. As shown in
Figure 5 (a), we utilized a text query to retrieve the top-3
ranked images, and the results indicate that our method can
recall more false negative cases. Furthermore, in Figure 5
(b), we employed two similar images to retrieve the top-2
similar texts, and the results demonstrate that our method
enables the model to better recognize similar input samples.

Conclusion
In this paper, we have proposed a novel method for imagetext retrieval, called Cross-modal and Uni-modal Soft-label
Alignment. Our method leverages a uni-modal pre-training
model to provide soft-label supervision signals for the ITR
model, and uses two alignment techniques, CSA and USA,
to overcome false negatives and enhance similarity recognition between uni-modal samples. Our method is plug-andplay and can be easily applied to existing ITR models without changing their original architectures. We have conducted
extensive experiments on various ITR models and datasets
and demonstrated that our method can consistently improve
the performance of image-text retrieval and achieve new
state-of-the-art results. Moreover, our method can also boost
the uni-modal retrieval performance of the ITR model, enabling it to achieve universal retrieval.

References
Agirre, E.; Banea, C.; Cardie, C.; Cer, D.; Diab, M.;
Gonzalez-Agirre, A.; Guo, W.; Lopez-Gazpio, I.; Maritxalar, M.; Mihalcea, R.; et al. 2015. Semeval-2015 task 2:
Semantic textual similarity, english, spanish and pilot on interpretability. In Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015), 252‚Äì263.
Agirre, E.; Banea, C.; Cardie, C.; Cer, D.; Diab, M.;
Gonzalez-Agirre, A.; Guo, W.; Mihalcea, R.; Rigau, G.; and
Wiebe, J. 2014. Semeval-2014 task 10: Multilingual semantic textual similarity. In Proceedings of the 8th international
workshop on semantic evaluation (SemEval 2014), 81‚Äì91.
Agirre, E.; Banea, C.; Cer, D.; Diab, M.; Gonzalez Agirre,
A.; Mihalcea, R.; Rigau Claramunt, G.; and Wiebe, J. 2016.
Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation. In SemEval-2016. 10th
International Workshop on Semantic Evaluation; 2016 Jun
16-17; San Diego, CA. Stroudsburg (PA): ACL; 2016. p.
497-511. ACL (Association for Computational Linguistics).
Agirre, E.; Cer, D.; Diab, M.; and Gonzalez-Agirre, A. 2012.
Semeval-2012 task 6: A pilot on semantic textual similarity. In * SEM 2012: The First Joint Conference on Lexical
and Computational Semantics‚ÄìVolume 1: Proceedings of the
main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic
Evaluation (SemEval 2012), 385‚Äì393.
Agirre, E.; Cer, D.; Diab, M.; Gonzalez-Agirre, A.; and Guo,
W. 2013. * SEM 2013 shared task: Semantic textual similarity. In Second joint conference on lexical and computational
semantics (* SEM), volume 1: proceedings of the Main conference and the shared task: semantic textual similarity, 32‚Äì
43.
An, X.; Deng, J.; Yang, K.; Li, J.; Feng, Z.; Guo, J.; Yang,
J.; and Liu, T. 2023. Unicom: Universal and Compact Representation Learning for Image Retrieval. In ICLR.
Cer, D.; Diab, M.; Agirre, E.; Lopez-Gazpio, I.; and Specia,
L. 2017. Semeval-2017 task 1: Semantic textual similaritymultilingual and cross-lingual focused evaluation. arXiv
preprint arXiv:1708.00055.
Chen, J.; Hu, H.; Wu, H.; Jiang, Y.; and Wang, C. 2021.
Learning the best pooling strategy for visual semantic embedding. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, 15789‚Äì15798.
Chen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020. A
Simple Framework for Contrastive Learning of Visual Representations. In III, H. D.; and Singh, A., eds., Proceedings
of the 37th International Conference on Machine Learning,
volume 119 of Proceedings of Machine Learning Research,
1597‚Äì1607. PMLR.
Chun, S.; Kim, W.; Park, S.; Chang, M.; and Oh, S. J.
2022. Eccv caption: Correcting false negatives by collecting
machine-and-human-verified image-caption associations for
ms-coco. In European Conference on Computer Vision, 1‚Äì
19. Springer.
Chun, S.; Oh, S. J.; de Rezende, R. S.; Kalantidis, Y.; and
Larlus, D. 2021. Probabilistic Embeddings for Cross-Modal
Retrieval. In Proceedings of the IEEE/CVF Conference on

Computer Vision and Pattern Recognition (CVPR), 8415‚Äì
8424.
Conneau, A.; and Kiela, D. 2018. SentEval: An Evaluation Toolkit for Universal Sentence Representations.
arXiv:1803.05449.
Diao, H.; Zhang, Y.; Ma, L.; and Lu, H. 2021. Similarity
reasoning and filtration for image-text matching. In Proceedings of the AAAI conference on artificial intelligence,
volume 35, 1218‚Äì1226.
Gao, T.; Yao, X.; and Chen, D. 2022.
SimCSE:
Simple Contrastive Learning of Sentence Embeddings.
arXiv:2104.08821.
Gao, Y.; Liu, J.; Xu, Z.; Wu, T.; Liu, W.; Yang, J.; Li, K.;
and Sun, X. 2023. SoftCLIP: Softer Cross-modal Alignment
Makes CLIP Stronger. arXiv:2303.17561.
He, K.; Fan, H.; Wu, Y.; Xie, S.; and Girshick, R. 2020.
Momentum Contrast for Unsupervised Visual Representation Learning. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR).
Jia, C.; Yang, Y.; Xia, Y.; Chen, Y.-T.; Parekh, Z.; Pham, H.;
Le, Q.; Sung, Y.-H.; Li, Z.; and Duerig, T. 2021. Scaling
up visual and vision-language representation learning with
noisy text supervision. In International conference on machine learning, 4904‚Äì4916. PMLR.
Lee, K.-H.; Chen, X.; Hua, G.; Hu, H.; and He, X. 2018.
Stacked cross attention for image-text matching. In Proceedings of the European conference on computer vision
(ECCV), 201‚Äì216.
Li, J.; Li, D.; Xiong, C.; and Hoi, S. 2022a. Blip: Bootstrapping language-image pre-training for unified visionlanguage understanding and generation. In International
Conference on Machine Learning, 12888‚Äì12900. PMLR.
Li, J.; Selvaraju, R.; Gotmare, A.; Joty, S.; Xiong, C.; and
Hoi, S. C. H. 2021. Align before fuse: Vision and language
representation learning with momentum distillation. Advances in neural information processing systems, 34: 9694‚Äì
9705.
Li, K.; Zhang, Y.; Li, K.; Li, Y.; and Fu, Y. 2022b. Imagetext embedding learning via visual and textual semantic reasoning. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 45(1): 641‚Äì656.
Li, Z.; Guo, C.; Feng, Z.; Hwang, J.-N.; and Du, Z. 2023a.
Integrating Language Guidance into Image-Text Matching
for Correcting False Negatives. IEEE Transactions on Multimedia, 1‚Äì14.
Li, Z.; Guo, C.; Feng, Z.; Hwang, J.-N.; Jin, Y.; and Zhang,
Y. 2022c. Image-Text Retrieval with Binary and Continuous
Label Supervision. arXiv:2210.11319.
Li, Z.; Guo, C.; Wang, X.; Feng, Z.; and Wang, Y. 2023b. Integrating Listwise Ranking into Pairwise-based Image-Text
Retrieval. arXiv:2305.16566.
Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; DollaÃÅr, P.; and Zitnick, C. L. 2014. Microsoft
coco: Common objects in context. In Computer Vision‚Äì
ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, 740‚Äì
755. Springer.

Liu, Z.; Luo, P.; Qiu, S.; Wang, X.; and Tang, X. 2016. Deepfashion: Powering robust clothes recognition and retrieval
with rich annotations. In Proceedings of the IEEE conference on computer vision and pattern recognition, 1096‚Äì
1104.
Marelli, M.; Bentivogli, L.; Baroni, M.; Bernardi, R.;
Menini, S.; and Zamparelli, R. 2014. Semeval-2014 task 1:
Evaluation of compositional distributional semantic models
on full sentences through semantic relatedness and textual
entailment. In Proceedings of the 8th international workshop on semantic evaluation (SemEval 2014), 1‚Äì8.
Oh Song, H.; Xiang, Y.; Jegelka, S.; and Savarese, S. 2016.
Deep metric learning via lifted structured feature embedding. In Proceedings of the IEEE conference on computer
vision and pattern recognition, 4004‚Äì4012.
Parekh, Z.; Baldridge, J.; Cer, D.; Waters, A.; and Yang,
Y. 2021. Crisscrossed Captions: Extended Intramodal and
Intermodal Semantic Similarity Judgments for MS-COCO.
arXiv:2004.15020.
Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;
Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;
et al. 2021. Learning transferable visual models from natural language supervision. In International conference on
machine learning, 8748‚Äì8763. PMLR.
Reimers, N.; and Gurevych, I. 2019. Sentence-BERT:
Sentence Embeddings using Siamese BERT-Networks.
arXiv:1908.10084.
Song, K.; Tan, X.; Qin, T.; Lu, J.; and Liu, T.-Y. 2020. Mpnet: Masked and permuted pre-training for language understanding. Advances in Neural Information Processing Systems, 33: 16857‚Äì16867.
van den Oord, A.; Li, Y.; and Vinyals, O. 2019. Representation Learning with Contrastive Predictive Coding.
arXiv:1807.03748.
Van Horn, G. 2018. Oisin Mac Aodha, Yang Song, Yin Cui,
Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and
Serge Belongie. The inaturalist species classification and detection dataset. In Proceedings of the IEEE conference on
computer vision and pattern recognition, volume 2, 5.
Wang, J.; Chen, D.; Wu, Z.; Luo, C.; Zhou, L.; Zhao, Y.; Xie,
Y.; Liu, C.; Jiang, Y.-G.; and Yuan, L. 2022. Omnivl: One
foundation model for image-language and video-language
tasks. Advances in neural information processing systems,
35: 5696‚Äì5710.
Welinder, P.; Branson, S.; Mita, T.; Wah, C.; Schroff, F.; Belongie, S.; and Perona, P. 2010. Caltech-UCSD birds 200.
Yang, J.; Duan, J.; Tran, S.; Xu, Y.; Chanda, S.; Chen, L.;
Zeng, B.; Chilimbi, T.; and Huang, J. 2022. Vision-language
pre-training with triple contrastive learning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 15671‚Äì15680.
Young, P.; Lai, A.; Hodosh, M.; and Hockenmaier, J. 2014.
From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.
Transactions of the Association for Computational Linguistics, 2: 67‚Äì78.

Zeng, Y.; Zhang, X.; Li, H.; Wang, J.; Zhang, J.; and Zhou,
W. 2023. X2 -VLM: All-In-One Pre-trained Model For
Vision-Language Tasks. arXiv:2211.12402.
Zhang, K.; Mao, Z.; Wang, Q.; and Zhang, Y. 2022.
Negative-aware attention framework for image-text matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 15661‚Äì15670.

