# ğŸ” **Diagnostics Introduced (after Exp.1)**

After seeing the poor baseline results, I introduced a diagnostic suite to understand the geometry of the task before modifying the model.

### ğŸ”§ What I added

* **Ridge Linear Transferability** â†’ measures best possible linear map.
* **Procrustes (orthogonal vs linear)** â†’ checks if mapping is isometric or anisotropic.
* **Mutual kNN alignment** â†’ measures structural/topological match.

### ğŸ’¡ What the diagnostics revealed

* The mapping is mostly linear.
* The geometry is strongly anisotropic.
* KL/VAE-like smoothing destroys useful structure.

### ğŸ” What I found out

* Ridge achieved strong alignment â†’ any nonlinear translator must beat this.
* Orthogonal Procrustes underperformed linear Procrustes â†’ the mapping is not a simple rotation.
* Mutual-kNN was extremely low â†’ the cross-modal topology is fragile.
* These confirmed that aggressive geometric constraints or priors would likely harm performance.

### ğŸ“š Papers referenced

* *Harnessing the Universal Geometry of Embeddings*
* *Revisiting Model Stitching*
* *Platonic Representation Hypothesis*
* *Relative Representations Enable Zero-Shot Communication*

---

# ğŸ§ª **Experiment 2 â€” Deterministic Contrastive Translator**

This was built as a direct response to the diagnostics pointing to: â€œavoid smoothing, keep the mapping simpleâ€.

### ğŸ”§ What I applied

* **Removed KL + sampling** â†’ avoids Gaussian smoothing.
* **Symmetric InfoNCE** â†’ CLIP-style contrastive alignment.
* **Learned temperature** â†’ stabilizes gradient scaling.
* **Memory-bank hard negatives** â†’ stronger contrastive signals.
* **Ridge distillation** â†’ pushes model toward optimal linear map.
* **Label smoothing** â†’ more stable CE gradients.

### ğŸ” What I found out

* This experiment gave a **slight improvement**.
* Removing KL was critical; contrastive alignment helped.
* However, conflict between InfoNCE and ridge teacher prevented major gains.
* Hard negatives increased instability.

### ğŸ“š Papers referenced

* **CLIP** (InfoNCE, temp scaling)
* **MoCo / SupCon** (memory bank)
* **Universal Geometry** (ridge distillation)
* **Relative Representations** (angle preservation)

---

# ğŸ§ª **Experiment 3 â€” Multi-Positive SupCon + Geometry Polishing**

This attempt tried to use multi-caption positives + geometric constraints.

### ğŸ”§ What I applied

* **Multi-positive SupCon** â†’ uses all captions for an image.
* **Grouped sampling by image-ID** â†’ ensures multi-view training.
* **Whitening + orthoreg** â†’ enforces cleaner geometry.
* **Uniformity loss** â†’ spreads embeddings uniformly.
* **Ridge-blend + dynamic clamp** â†’ stabilizes norms.
* **Optional k-reciprocal re-ranking** â†’ improves final retrieval.

### ğŸ” What I found out

* Performance **worsened**.
* Too many geometric constraints conflicted with the natural anisotropy.
* Whitening + uniformity flattened the structure â†’ bad for retrieval.
* SupCon created tight local clusters but harmed global ranking.

### ğŸ“š Papers referenced

* **SupCon (Khosla et al.)**
* **Revisiting Model Stitching / Procrustes**
* **Platonic Representation Hypothesis**
* **SimCLR** (uniformity)
* **Retrieval re-ranking literature**

---

# ğŸ§ª **Experiment 4**

This phase focused on stability and avoiding harmful geometry modifications.

### ğŸ”§ What I applied

* **Mixup warmup** â†’ smoother early gradients.
* **Small Gaussian noise conditioning** â†’ robustness (borrowed from diffusion models).
* **Fixed-temperature CE** â†’ prevents temperature collapse.
* **Hard triplet (margin=0.06)** â†’ stronger local ranking.
* **DynamicThresholdingClamp** â†’ inspired by Imagen; prevents norm blow-ups.
* **Ridge-blend inference** â†’ improves ranking quality.
* **Removed EMA, KL** â†’ avoid instability.

### ğŸ” What I found out

* Performance **decreased**.
* Triplet + CE + geometric control created conflicting optimization signals.
* Noise + clamp altered latent scales in ways that hurt retrieval.
* The model became stable but not expressive enough.

### ğŸ“š Papers referenced

* **CLIP stability heuristics**
* **Imagen diffusion model** (dynamic thresholding)
* **Diffusion noise conditioning**
* **Triplet margin loss literature**